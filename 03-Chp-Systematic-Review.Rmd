---
bibliography: bibliography/references.bib
csl: bibliography/nature.csl
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
  bookdown::word_document2: 
      toc: false
      toc_depth: 3
      reference_docx: templates/word-styles-reference-01.docx
      number_sections: false 
  bookdown::html_document2: default
documentclass: book
---
```{block type='savequote', include=knitr::is_latex_output(),quote_author='(ref:sys-rev-quote)', echo = FALSE}
"It is surely a great criticism of our profession that we have not organised a critical summary by speciality or sub-speciality, up-dated periodically, of all relevant RCTS."  
```

(ref:sys-rev-quote) --- Archibald Cochrane, 2000 [@cochrane1979]

# Systematic review of all evidence available on the association between blood lipids (and treatment) and dementia outcomes {#sys-rev-heading}

\minitoc <!-- this will include a mini table of contents-->

```{r, echo = FALSE, warning=FALSE, message=FALSE}
source("R/doc_options.R")
source("R/helper.R")
knitr::read_chunk("R/03-Code-Systematic-Review.R")
doc_type <- knitr::opts_knit$get('rmarkdown.pandoc.to') # Info on knitting format
```

<!-- ## To Do -->

<!-- TODO Extra studies to include: -->
<!-- * Larsson - 10.1136/bmj.j5375 [@larsson2017b] -->
<!-- * Statins and the risk of dementia Jick -->

## Lay summary

Systematic reviews are a type of research that aim to use all existing evidence to provide the best answer to an important research question. They do this by finding and combining the results from many related primary research studies. Reviews involve multiple steps including: searching of existing studies; assessment of the studies against predefined inclusion criteria; collection of data from each study; assessment of each study's methods. 

This chapter presents a systematic review of primary studies that have examined the relationship between the levels of blood lipids (such as cholesterol and triglycerides), and treatments that change these levels, and dementia.

There were XXX primary studies <!-- TODO check this -->that contained information on this relationship. I found that statins reduce the risk of Alzheimer's disease, but had no effect of vascular dementia. Lipids were not associated with any outcome. The methods used in some of the primary studies meant that I was less confident in the accuracy of their results.

The use of the results of this review in subsequent chapters is dicussed.

<!-- TODO Search for MCI -->

<!-- TODO text -->


&nbsp;

<!----------------------------------------------------------------------------->
## Introduction {#sys-rev-intro}

In this chapter, I describe a comprehensive systematic review of the relationship between blood lipid levels, and treatments that modify them, and the subsequent risk of dementia and related outcomes. This analysis sought to address two specific aims.

Firstly, as discussed in the Introduction to this thesis (Section \@ref(evidence-association)), several diverse forms of evidence on the relationship of lipids and dementia exist. These include randomised controlled trials, observational studies of different analytical design, and Mendelian randomisation studies. However, based on a scoping review of existing literature, no previous evidence synthesis exercise has attempted to examine the association of lipids/statins with dementia outcomes across these distinct evidence types. Collating these diverse evidence sources is important, as if the observed association between lipids and dementia is constant across them, it increases our confidence in the association. As such, the primary aim of this analysis was to systematically review all available literature, regardless of study design.

Secondly, I explicitly sought to include health-related preprint servers as a potential evidence source in this review, as they are infrequently considered by evidence synthesists but may contain important unpublished studies. As a sensitivity analysis to the systematic review presented in this chapter, I sought to quantify the additional evidential value of including preprints. This inclusion of preprint serves makes use of the preprint search tool presented in Chapter \@ref(sys-rev-tools-heading).

The results of this review are used to guide the primary analysis presented in Chapter \@ref(cprd-analysis-heading), in addition to forming a key evidence source used in the triangulation exercise presented in Chapter \@ref(discussion-heading).

&nbsp;
<!----------------------------------------------------------------------------->

## Methods

### Protocol

A pre-specified protocol for this analysis was registered using the Open Science Framework, and is available for inspection.[@mcguinnessluke2020] Deviations from this protocol are detailed in the relevant sections.

<!----------------------------------------------------------------------------->
&nbsp;

### Contributions

In line with best-practice guidance, secondary reviewers were used to check the accuracy of screening, data extraction and risk-of-bias assessment processes. Due to the scale of the project, this systematic review was performed in conjunction with a team of secondary reviewers and an information specialist (see Acknowledgments and Author declaration). <!-- TODO add the following to acknowledgements These included Alexandra MacAleenan, Athena Sheppard, and Matthew Lee.  In addition, Sarah Dawson, an information specialist, provided input to the design of the search strategy.-->

<!-- TODO Double check this list at the end of the project. -->

<!----------------------------------------------------------------------------->
&nbsp;

### Search strategy

I systematically searched several electronic bibliographic databases to identify potentially relevant entries (hereafter referred to as "records"). The following databases were searched from inception onwards: Medline, EMBASE, Psychinfo, Cochrane Central Register of Controlled Trials (CENTRAL), and Web of Science Core Collection. As the contents of the Web of Science Core Collection can vary by institution,[@gusenbauer2020a] the specific databases searched via this platform are listed in Appendix \@ref(appendix-wos-databases). The search strategy used in each database was developed in an iterative manner using a combination of free text and controlled vocabulary (MeSH/EMTREE)[@lefebvre2019searching] terms to identify studies which have examined the relationship between blood lipids levels and dementia, incorporating input from an information specialist. The strategy included terms related to lipids, lipid modifying treatments, and dementia, and was designed for MEDLINE before being adapted for use in the other bibliography databases listed. An outline of the general strategy is presented in the Table \@ref(tab:searchOverview-table) below and the full search strategies for each database are presented in Appendix \@ref(appendix-search-strategy). <!-- TODO Need to actually attach each search strategy. Should be able to loop through search strategy results. -->

&nbsp;

<!----------------------------------------------------------------------------->
(ref:searchOverview-caption) Summary of systematic search by topic. The full search strategy including all terms and the number of hits per term is included in Appendix \@ref(appendix-search-strategy).

(ref:searchOverview-scaption) searchOverview

```{r searchOverview-table, message=FALSE, results="asis", echo = FALSE}
```
<!----------------------------------------------------------------------------->

&nbsp;

When searching the bibliographic databases, study design filters were employed to try and reduce the screening load. To ensure that the study design filters are not excluding potentially relevant records, a random sample of 500 records identified by the main search but excluded by the filters (defined as "8 NOT 12" in Table \@ref(tab:searchOverview-table)) was screened.

<!-- TODO need to comment on this feed-back process, or remove -->



I also searched clinical trial registries, for example ClinicalTrials.gov, to identify relevant randomized controlled trials. In addition, I searched the bioRxiv and medRxiv preprint repositories using the tool developed in Chapter \@ref(sys-rev-tools-heading) to identify potentially relevant preprinted studies.

Grey literature was searched via ProQuest, OpenGrey and Web of Science Conference Proceedings Citation Index, while theses were accessed using the Open Access Theses and Dissertations portal. In addition, the abstracts list of relevant conferences (e.g. the proceedings of the Alzheimer's Association International Conference, published in the journal Alzheimer's & Dementia) were searched by hand. <!-- TODO how were these searched? Date and other range --> Finally, the reference lists of included studies were searched by hand while studies citing included studies was examined using Google Scholar (forward and reverse citation searching or "snowballing"<!-- TODO CITATION NEEDED -->).

&nbsp;
<!----------------------------------------------------------------------------->

### Study selection

Records were imported into Endnote and de-duplicated using the method outlined in Bramer et al. (2016).[@bramer2016] In summary, this method uses multiple stages to identify potential duplicates, beginning with automatic deletion of records matching on multiple fields ("Author" + "Year" + "Title" + "Journal"), followed by manual review of less similar articles (e.g. those matched based on the "Title" field alone).

Following deduplication of records, screening (both title/abstract and full-text) was performed using a combination of Endnote, a citation management tool,[@hupe2019] and Rayyan, a web-based screening application.[@ouzzani2016] Title and abstract screening to remove obviously irrelevant records was performed primarily by me, with a random ~10% sample of excluded records being screened in duplicate to ensure consistency with the inclusion criteria. Additionally, records were rescreened by me with a 1 month lag to intra-rater consistency.

Similarly, I completed all full-text screening, with a random ~10% being screened in duplicate by a second reviewer. In addition, any records identified I identified as being difficult to assess against the inclusion criteria were screened in duplicate. Reasons for exclusion at this stage were recorded. Disagreements occurring during either stage of the screening process were resolved through discussion with a senior colleague. A PRIMSA flow diagram was produced to document how records moved through the review.[@page2021]

The criteria used to assess eligibility are presented in the subsequent sections.

<!----------------------------------------------------------------------------->
&nbsp;

#### Inclusion criteria

I sought to include studies that examine the relationship between blood lipid levels (or any specific lipid fraction, including total cholesterol, HDL, LDL, and triglycerides) and risk of incident dementia and its subtypes. Eligible study designs included randomized controlled trials and non-randomized observational studies of lipid modifying treatments, longitudinal studies examining the effect of increased/decreased blood lipid levels, and genetic instrumental variable (Mendelian randomization) studies examining the effect of genetically increased/decreased blood lipid levels.

Participants were screened for dementia at baseline and prevalent cases excluded. Alternatively, where no baseline screening was employed, participants were assumed to be dementia free if less than <50 years of age at baseline. Studies of any duration were included to allow for exploration of the effect of length of follow-up on the effect estimate using meta-regression. No limits were placed on the sample size of included studies.

Eligible studies defined dementia according to recognised criteria, for example the International Classification of Diseases (ICD),[@organizationwho1993] National Institute of Neurological Disorders and Stroke Association-Internationale pour la Recherche en l'Enseignement en Neurosciences (NINDS-AIREN),[@roman1993] or Diagnostic and Statistical Manual of Mental Disorders (DSM) criteria.[@edition2013] Studies utilising electronic health records were the exception to this, as it was assumed that these criteria were used when entering the outcome into the EHR.

<!-- TODO Cite Peter Tennant's piece here on the problems with causal inference from analysis of change scores -->

<!-- TODO Any study using EHR - go back and extract codelists -->

No limitations were imposed on publication status, publication date, venue or language, although sufficiently detailed reports of the studies to be able to examine their methods were required for inclusion.

<!----------------------------------------------------------------------------->
&nbsp;

#### Exclusion criteria

Due to the significant impact of a memory-related outcome such as dementia on exposure recall, case-control studies were excluded, though nested case-control studies, where historical records are used to determine the exposure status, were eligible for inclusion. Cross-sectional studies, qualitative studies, case reports/series and narrative reviews were also excluded. Studies which presented no evidence of attempting to exclude prevalent cases from their analyses were also excluded. Studies that measure change in continuous cognitive measures (e.g. MoCA score) without attempt to map these scores to ordinal groups (e.g. no dementia/dementia) were excluded. Conference abstracts with no corresponding full-text publication were examined, and where required, I contacted authors to obtain information on the study's status. Previous systematic reviews were not eligible for inclusion, but their reference lists were screened to identify any potentially relevant articles. 

Studies with outcomes not directly related to the clinical syndrome of dementia (e.g., neuroimaging), studies implementing a "multi-domain intervention" where a lipid-regulating agent is included in each arms (e.g. for example, a study examining exercise + statins vs statins alone, but a study examining exercise + statins vs exercise alone would be included), and studies where there was no screening for dementia at baseline except if the sample was initially assessed in mid-life (i.e. below the age of 50) were excluded. Finally, studies using a dietary intervention, for example omega-3 fatty acid enriched diet, were excluded as it is difficult to disentangle the effect of other elements contained within the diet. Note, this is distinct from studies which delivered a simple tablet-based omega-3 intervention, which would have been eligible for inclusion.

<!----------------------------------------------------------------------------->
&nbsp;

### Validation of screening process

Inter- and intra-rater reliability during the screening stages were assessed for a 10% sub-sample of records. Intra-rater reliability involved a single reviewer applying the inclusion criteria to the same set of records while blinded to their previous decisions (i.e. assessment of conistency), while inter-rater reliability involved two reviewers independently screening the same set of records (i.e. assessment of accuracy).

Rater reliability was assessed using Gwet's agreement coefficient (AC1).[@gwet2008] This measure was chosen over other methods such as percent agreement (number of agreements divided by total number of assessments), as it accounts for chance agreement between reviewers but does not suffer from bias due to severely imbalanced marginal totals in the same way that Cohen's $kappa$ value does. [@cohen1960: @gwet2008; @wongpakaran2013] Given the small number of included studies in this review as a proportion of the total number screened, this is an important characteristic.

Gwet's AC1 is defined as:

$$AC1 = \frac{observed\;agreement-chance\;agreement}{1-chance\;agreement}$$ 

In reference to a two-by-two table with cells A, B, C and D, it is calculated using the following:

\begin{equation}
  AC1 = \frac{\frac{A+D}{N}-e(\gamma)}{1-e(\gamma)}
  (\#eq:AC1-main)
\end{equation}

where $e(\gamma)$ is the chance agreement between raters, given as $2q(1-q)$, where 

\begin{equation}
  q = \frac{(A+C)+(A+B)}{2N}
  (\#eq:AC1-supp)
\end{equation}

<!-- TODO Need to be sure of how to calculate. -->

How to interpret agreement co-efficients is widely debated, and while arbitary cut-off values may mislead readers,[@brennan1992] they provide a useful rubric by which to assess inter-rater agreement. Here, I used guidelines based on a stricter interpretation of the Cohen's $kappa$ coefficient,[@mchugh2012] presented in Table \@ref(tab:gwet-table).

&nbsp;

<!----------------------------------------------------------------------------->
(ref:gwet-caption) Suggested ranges to aid in interpretation of Gwet's AC1 inter-rater reliability metric

(ref:gwet-scaption) Ranges for Gwet's AC1

```{r gwet-table, message=FALSE, results="asis", echo = FALSE}
```
<!----------------------------------------------------------------------------->

&nbsp;

Intra- and inter-rater reliability was assessed against these cut-offs. If this assessment demonstrated issues with the screening process (defined as an AC1 of <.9), a larger proportion of records would have been dual-screened. 

&nbsp;
<!----------------------------------------------------------------------------->

### Data extraction

Data extraction was performed using a piloted data extraction form. Extracted items included: article metadata (year of publication, author list, journal), study characteristics (study location, data source, exposure, outcomes, outcome criteria used), patient characteristics (age, sex, baseline cognition scores, baseline education scores), and results (exposure-outcome pairing, effect measure, effect estimate, error estimate, p-value). I extracted all data in the first instance, which was subsequently checked for accuracy by a second member of the review team.

#### Study-fication

As part of the data extraction process, multiple resords resulting from the analysis of the same data were included and grouped into single units, hereafter called studies. This is likely in the advent of multiple papers reporting results on the same cohort, but say, at different time points. Study-fication builds out the most comprehensive accounts of the studies and results from as many published articles were applicable. 

This was particularly relevant to preprints and published papers reporting the same study, which were not considered to be duplicate records but instead different reports of the same study. This is due to the potential for the published version to offer some information that the preprint did not, and vice versa.

<!----------------------------------------------------------------------------->
&nbsp;

#### Combining across groups

Following best practice, where summary data was presented across two groups (e.g. age at baseline stratified by hypercholesterolemia status), the following approach was used to combine the groups. [@higgins2019]

\begin{equation}
N = N_1 + N_2
  (\#eq:combiningGroups1)
\end{equation}


\begin{equation}
Mean = \frac{(N_1M_1 + N_2N_2)}{(N_1 + N_2)}
  (\#eq:combiningGroups2)
\end{equation}


\begin{equation}
SD = \sqrt{\frac{(N_1-1)SD_1^2 + (N_2-1)SD_2^2 + \frac{N_1N_2}{N_1 + N_2}(M_1^2 + M_2^2 - 2M_1M_2)}{N1 + N2 -1}}
  (\#eq:combiningGroups3)
\end{equation}

This was implemented in a systematic manner, with the raw group data being extracted and a cleaning script written to combine the groups for analysis.

<!----------------------------------------------------------------------------->
&nbsp;


#### Harmonisation of cholesterol measures

<!-- TODO This might not be as accurate now given the focus on dose response --> <!-- TODO Cross reference with the data extraction issues here - i.e. not providing qratile cut-off values, not poviding numbers per group, etc. --> Where necessary, lipid levels reported in _mmol/L_ were converted in _mg/dL_ using the following formula:

\begin{equation} 
  mg/dL = mmol/L \times{} Z
  (\#eq:lipidConversion)
\end{equation} 

where $Z = 38.67$ for total cholesterol, LDL-c and HDL-c, and $Z = 88.57$ for triglycerides. For widely-used categorises of lipids levels on the _mg/dL_ scale, see Table \@ref(tab:lipidLevels-table) in Section \@ref(intro-lipid-fractions). 

<!----------------------------------------------------------------------------->
&nbsp;


#### Following up with authors {#contacting-authors}

Where additional data points not included in the report of an analysis were required either for the analysis or risk-of-bias assessment, the corresponding author of the study was contacted. This approach was taken due to the potentially large impact of following up with authors on the results of the review.[@reynders2019] 



<!----------------------------------------------------------------------------->
&nbsp;

#### Analysis of varying effect measures

<!-- TODO Include formulae and informed assumptions  -->

The range of effect measures presented by studies (odds ratios, risk ratios, hazard ratios, etc) are not directly interchangeable in the context of systematic review. If the outcome is rare, as is the case for dementia outcomes at and estimated prevalence of  odds and risk ratios approximate each other.<!-- TODO CITATION NEEDED --> However, hazard ratios provide a very different interpretation, taking into account person-timea-at-risk in each treatment group. As such, different effect estimates can be one potential problem that precludes a meta-analysis of all studies.[@mckenzie2019]

Several existing reviews do not distinguish between the types of effect measures and include all existing studies in a single meta-analysis to produce an overall effect. In addition, there is some evidence of manipulation of effect estimates in previous reviews,(e.g. Chou, Sci Reports - at least one study disagrees with) but this is not accurately documented in the review text.
<!-- TODO Need to really fix this -->

In this review, studies reporting hazard ratios were synthesised separately to those reporting odds/risk ratios.

<!----------------------------------------------------------------------------->
&nbsp;

### Risk-of-bias assessment {#risk-of-bias}

A key use of the review presented in this chapter is to identify different sources of evidence at risk of a diverse range of biases, and to contrast and compare findings across them (see Section \@ref(triangulation-overview) for an overview of triangulation and Section \@ref(intro-triangulation) for the results of this qualitative analysis). To enable this triangulation exercise, a detailed and structured risk-of-bias assessment formed an important part of this review.

There has been a recent movement within the evidence synthesis community from examining _methodological quality_ to assessing _risk of bias_,[@mcguinness2018; @sterne2016] and thus directly evaluating the internal validity of a study. Internal validity is defined here as the absence of systematic error (or bias) in a study, which may influence its results.[@campbell1957; @juni2001] 

This move was prompted by a unclear definition of "methodological quality" which could include facets such as unclear reporting, and challenges in the comparison of results from different tools. <!-- TODO CITATION NEEDED --> As part of this shift, the community also moved from checklist or score based tools towards domain-based methods, in which different potential sources of bias in a study are assessed in order. Additionally, bias should be assessed at the result (defined as a a specific outcome at a specific timepoint) rather than the study level. For example, a study may report on the efficacy of an intervention at six months and two years follow-up. In this case, missing outcome data that is not an issue at six months may introduce bias at 2 year follow-up, and assigning a bias judgement to the study as a whole masks the different level of bias for each unique result.

In this review, domain-based tools were used to assess the risk of bias for each result in each included study. The study design-specific tools are introduced and discussed in more detail in the following sections. 

<!----------------------------------------------------------------------------->
&nbsp;

#### Randomised controlled trials

Randomized controlled trials were assessed using the RoB2 tool.[@sterne2019] The tool assess the risk of bias across five domains: Bias arising from the randomization process, Bias due to deviations from intended intervention, Bias due to missing outcome data, Bias in measurement of the outcome, Bias in selection of the reported result. Acceptable judgements include: low risk of bias, some concerns, high risk of bias. Each of the 5 domains contains a series of signalling questions or prompts, which guide the user through the tool. Once a domain-level judgement for each domain has been assigned, an overall judgement, using the same three levels of risk of bias, is assigned to the result.

<!----------------------------------------------------------------------------->
&nbsp;

#### Non-randomised studies of interventions/exposures

For non-randomised studies of interventions (NRSI), I used the ROBINS-I (Risk Of Bias In Non-randomised Studies - of Interventions) tool.[@sterne2016] This tool assess the risk of bias across seven domains: Bias due to confounding, Bias due to selection of participants, Bias in classification of interventions, Bias due to deviations from intended interventions, Bias due to missing data, Bias in measurement of outcomes, and Bias in selection of the reported result. Similar to RoB 2, it has a number of prompting questions per domain, with acceptable judgements including “Low risk”, “Moderate risk”, “Serious risk” and “Critical risk”. Ideally, observational studies should be assessed in reference to an idealised randomised controlled trial. Under this approach, the (rare) overall judgement of "Low" indicates that the results should be considered equivalent to produced by a randomised controlled trial.

While a risk-of-bias tool for non-randomised studies of exposures (NRSE) is currently under development,[@morganr2020] but was insufficiently developed at the time the risk-of-bias assessments for this review were performed. Instead, I used a version of the ROBINS-I tool informed by the preliminary ROBINS-E tool (“Risk of Bias In Non-randomised Studies – of Exposure”), which I had applied in a published review.[@french2019] The version had no signalling questions and so judgements were made at the domain level. The motivation for this using this tool above other established tools such as the Newcastle-Ottowa scale (NOS).[@wells2000] was two-fold. In the first instance, as mentioned in the introduction to this section, using a domain-based tool has distinct advantages over better-developed checklist-type tools including the NOS. Additionally, using a domain-based tool for non-randomised studies of exposures enabled better comparison with risk-of-bias assessments performed for the other study designs.

<!----------------------------------------------------------------------------->
&nbsp;

#### Mendelian randomisation studies

At present, no formalised risk-of-bias assessment tool for Mendelian randomization studies is available. Assessment of the risk of bias in Mendelian randomisation studies was informed by the approach used in a previous systematic review of Mendelian randomisation,[@mamluk2020] as identified by a review of risk-of-bias assessments in systematic reviews of Mendelian randomisation studies (advance results from this review were obtained from contact with the authors.). <!-- TODO CITATION NEEDED - add once published as a preprint --> A copy of this tool is available in Appendix \@ref(appendix-mr-rob), but in summary, results were assessed for bias arising from weak instruments, genetic and other confounding, pleiotropy, and selection of participants.

<!----------------------------------------------------------------------------->
&nbsp;

#### Risk of bias due to missing evidence

A recent shift towards the assessment of missing evidence due to selective non-reporting - as distinct from the selective reporting of a single result from multiple planned - is demonstrated via the forthcoming RoB-ME (Risk of Bias due to Missing Evidence in a synthesis) tool.[@zotero-15123] <!-- TODO Need more here describing missing evidence --> The tool is in development stages, and as part of this review, I piloted the tool, and provided feedback to the developers. <!-- TODO Cite Ben's work on effect of missing study versus choice of analysis method -->

This additional appraisal marks a departure from the registered protocol, as there was initially no intention to try and examine the risk of bias due to missing evidence. This is largely because the tool did not exist when the protocol was originally registered.

<!----------------------------------------------------------------------------->
&nbsp;

### Analysis methods

An inital qualiatative synthesis of evidence was performed, summarising the data extracted from studies stratified by study design

Where individual studies were deemed comparable, they were incorporated into a quantitative analysis or "meta-analysis". <!-- TODO CITATION NEEDED --> Meta-analysis provides a summary or pooled effect estimate across studies.


Of note, studies were not combined across different study designs (i.e. RCTs were not combined in a meta-analysis with results from observational studies). The results from each individual analytical approach were summarised, but are compare and contrasted more fully in the triangulation exercise presented in Chapter \@ref(discussion-heading)


&nbsp;
<!----------------------------------------------------------------------------->

#### Standard meta-analysis

Both a fixed-effect and random-effects meta-analysis model was employed to combine the different included studies. <!-- TODO Need more information on the difference between the two approaches, and their formula here, and description of different methods and quarrels regarding distinction fixed effect vs random effects meta-analyses. --> The fixed-effect method was implemented as:

<!-- TODO Check these against those in Ben's thesis paper -->


\begin{equation}
  \theta_i = \mu + u_i
  (\#eq:meta-analysis-random)
\end{equation}

\begin{equation}
  weighted\ average = \frac{\sum Y_i (1/SE_i^2)}{\sum(1/SE_i^2)}
  (\#eq:meta-analysis-fixed)
\end{equation}

<!-- where ... -->

<!-- TODO Need equation for random effects too. See Sean's thesis here. -->

#### Dose-response analyses

<!-- TODO Explore how to map from simple diagnosis of hyperchol to dose response. Seems that does response needs >3 categories -->

<!-- Will also need description of dose-response meta-analysis here, with reference to forthcoming book Julian sent on. -->

This was particularly important for the dose response meta-analysis, where the number of participants and the cut-offs per category were often not reported.

<!-- TODO Use the example of getting the cut-off points from the authors as an illustration of how reporting sucks  -->

Several of the included studies presented data on multiple categories of lipid levels, but provided an overall effect estimate based on a comparison of only two of these categories (e.g. for example, highest vs lowest quartile). <!-- TODO Question: if I look at highest vs lowest, and also at top dose vs lowest dose, am I double counting results? --> While this allows for easy interpretation of the resulting effect estimate, it ignores any potential non-linear relationships between the exposure and outcome, in addition to discarding useful information contain in the interim groups. In order to address this limitation, I performed a dose-response meta-analysis in those studies reporting more than two categories for lipid levels.

Studies were excluded from this analysis if the number of categories was less than three, if the exposure cut-off points for for each category were not reported (e.g. if the study reports splitting participants into quartiles and comparing the highest vs lowest without giving the quartile bands).

A restricted cubic spline model was fitted to allow for a non-linear relationship, for example a U or J-shaped relationship, where low and high levels of the exposure can have different effects versus a "normal" reference dose. The locations of the knots in the model wer identified using fixed percentiles (25th, 50th, 75th) of the exposure data. Reference doses were defined _a priori_ as the cut-off of the "Normal"/"Optimal" categories for each fractions, as detailed in Table \@ref(tab:lipidLevels-table). Under this approach, the reference dose was defined as 200 mg/dL for total cholesterol, 100 mg/dL for LDL-c, 40 mg/dL for HDL-c, and 150 mg/dL for triglycerides. 

Due to the requirements for the dose response analysis, studies were excluded from this secondary analysis if they did not provide the require information: cut-off points. 


Where this was not reported in the study, I contacted the corresponding author to attempt to obtain the required information (see Section \@ref(contacting-authors)). 

When the highest category was open ended (e.g LDL-c $\geqslant$ 200 mg/dL), I calculated category midpoint by assuming the width of the highest category was the same as the one immediately below it. Similarly, when, the lowest category was open-ended (e.g LDL-c $\leqslant$ 100 mg/dL), I set the lower boundary for this category to zero (though this is unlikely to occur natually, it was difficult to define).

&nbsp;
<!----------------------------------------------------------------------------->

#### Sensitivity analyses

I conducted a leave-one-out analysis in order to explore the impact of any results on the summary effect estimate. <!-- TODO CITATION NEEDED --> In addition, where there was evidence of heterogeneity between results included in a meta-analysis, I investigated this further using meta-regression against reported characteristics. _A priori_, I was interested in the effect that the age at baseline, sex and risk-of-bias judgement had on the results. <!-- TODO Need more here -->

Finally, I investigated the potential for small study effects, which may be caused by publication bias, both visually using funnel plots and statistically using Egger's regression test.  <!-- TODO CITATION NEEDED - cite Jonathon on interpreting small study effects, and whatever citation is needed for  -->


&nbsp;
<!----------------------------------------------------------------------------->

#### Visualisation of results

Evidence maps are useful way to explore the distribution of research cohorts included in a systematic review.[@saran2018] As such, the location of each individual study contributing to the evidence base was quantified and visualised on a world map.

One of the limitations of current risk-of-bias assessments in systematic reviews is that they are often divorced from the results to which they refer, and are infrequently incorporated into the analysis.

In response to this criticism, I developed a new visualisation tool was designed to allow for "paired" forest plots, as recommended by the ROB2 publication, where the risk-of-bias assessment is presented alongside the results.[@sterne2019] This tool was developed as an adjunct to this thesis to aid in creating standardised risk-of-bias figures,[@mcguinness2020robvisPaper] and the "paired" forest plot functionality grew out of a collaboration with other researchers to design a modular method for creating custom forest plots.[@zotero-14999] <!-- Apparently collaboration is a big thing they want to see --> A summary of this tool is contained in Appendix \@ref(appendix-robvis), and all forest plots presented in this Chapter were created using this tool.

&nbsp;
<!----------------------------------------------------------------------------->

#### Assessment of added value of including preprints

__[Note: Julian, I am particularly interested in your feedback on this section, and the corresponding results section (Section \@ref(sys-rev-including-preprints)), as I am not convinced on the language I am using]__

Preprints are considered a valuable evidence source within this thesis (see Introduction, Section \@ref(diverse-sources-preprints)) but their inclusion in a systematic review. 

As a sensitivity analysis, I explored the additional evidential value of including preprints in each meta-analysis performed, assessed using the fixed effect weight from a standard meta-analysis.

Additionally, I followed preprints up over time to investigate whether all identified preprints included in the review were subsequently published (in which case preprints provide a snapshot into the future, and a systematic review update would capture these reports) or alternatively, if some preprints were not published, then preprints provide a distinct evidence source.

&nbsp;
<!----------------------------------------------------------------------------->

## Results 

<!-- TODO Compare findings for two papers from Rantanen - not sure why? -->

### Initial search and validation of search filters

The database search identified `r num_to_text(23447)` records, of which 

<!-- TODO Cross check this with the actual numbers included -->

Of the random sample of 500 records screened to ensure the accuracy of the study design filters, no eligible records were identified. Many of those excluded by the filters were basic science studies, commentaries or educational articles, as expected. <!-- TODO include some example citations here -->


&nbsp;
<!----------------------------------------------------------------------------->

### Screening results

Following de-duplication, the titles and abstracts of `r num_to_text(16109)` records were assessed for eligibility. `r num_to_text(387)` were deemed potentially eligible, and the full text records for these were accessed and screened. <!-- TODO Check these figures & Finish PRISMA -->

A PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-analyses) flow diagram,[@page2021] <!-- TODO CITATION NEEDED - cite PRIMSA flow paper here, if published --> presented in Figure \@ref(fig:prisma-flow-fig), illustrates the movement of articles through the review. To highlight the contribution of preprint archives to the review, the flow diagram delineates between those records captured through databases searches (presented on the right of the diagram) and those captured by the search tool described in the previous chapter (presented in grey on the left of the diagram).

Common reasons for exclusion at the full text-stage included studies that reported on the wrong exposures (n = XXX; most commonly a inelgible lipid fraction), used the wrong study design (n= XXX), or reported on a wrong outcome (n=24; e.g. change in cognitive scores).

### Validation of screening

<!----------------------------------------------------------------------------->

<!-- TODO PRISMA must include reasons for exclusion - extract from Rayyan -->

```{r prisma-flow-setup, include = FALSE}
```

\blandscape{}

&nbsp;

(ref:prisma-flow-cap) PRISMA flow diagram illustrating how records moved through the systematic review process. The different contributions of standard bibliographic databases and preprint servers to the review are indicated.

(ref:prisma-flow-scap) PRISMA flow diagram

```{r prisma-flow-fig, echo = FALSE, results="asis", fig.pos="H", fig.cap='(ref:prisma-flow-cap)', out.width='100%', fig.scap='(ref:prisma-flow-scap)'}
knitr::include_graphics(file.path("figures/sys-rev/prismaflow.png"))
```

\elandscape{}

<!----------------------------------------------------------------------------->


```{r agree-setup, include = FALSE}
```

For the assessment of the intra-/inter-rater reliability, the estimated values of $AC1$ were interpreted against the categories presented in Table \@ref(tab:gwet-table). For the inter-rater reliability, agreement was "almost perfect" ($AC1$ = `r agreeInter_coeff[1]`, $kappa$ = `r agreeInter_coeff[2]`, Table \@ref(tab:agreeInter-table)). Similarly for intra-rater reliability, agreement was "almost perfect" ($AC1$ = `r agreeIntra_coeff[1]`, $kappa$ = `r agreeIntra_coeff[2]`, Table \@ref(tab:agreeIntra-table)). The discrepancy between the $AC1$ and $kappa$ coefficients illustrates the sensitivity of $kappa$ to imbalanced marginals, caused in this sample by a large imbalance  towards exclusion.[@feinstein1990]

&nbsp;

<!----------------------------------------------------------------------------->
(ref:agreeInter-caption) Inter-rater agreement on a subset of records, indicating high accuracy. 

<!-- TODO Include AC1 result in caption above -->

(ref:agreeInter-scaption) Inter-rater agreement

```{r agreeInter-table, message=FALSE, results="asis", echo = FALSE}
```
<!----------------------------------------------------------------------------->

<!----------------------------------------------------------------------------->
(ref:agreeIntra-caption) Intra-rater agreement on subset of records, indicating high consistency.

(ref:agreeIntra-scaption) Inter-rater agreement

```{r agreeIntra-table, message=FALSE, results="asis", echo = FALSE}
```
<!----------------------------------------------------------------------------->

&nbsp;

Those records which were excluded in the initial screening, but were included by the second reviewer (n=`r discrepancy_Inter`, Table \@ref(tab:agreeInter-table)) were investigated. This discrepancy between the two reviewers was explained in all cases by differing interpretations of the inclusion criteria, specifically around the definition of cognitive decline/MCI versus dementia, and the definition of eligible lipids fractions. 

&nbsp;
<!----------------------------------------------------------------------------->

### Characteristics of included studies

```{r characteristicsSetup, message=FALSE, results="asis", echo = FALSE}
```

Following full-text screening, 87 reports describing `r n_included` unique studies met the criteria for inclusion in the review.[@ancelin2012a; @ancelin2013; @andrews2019a; @arvanitakis2008; @batty2014; @benn2017b; @bettermann2012a; @beydoun2011; @bruce2017; @burgess2017; @chao2015a; @chen2014; @chiang2007; @chitnis2015; @chou2014a; @chuang2015; @cramer2008; @dodge2011; @forti2010; @gnjidic2016; @gottesman2017; @gustafson2012; @haag2009; @hayden2006; @heartprotectionstudycollaborativegroup2002; @hendrie2015; @hippisley-cox2010a; @jick2000; @kimm2011; @kivipelto2001; @kivipelto2005a; @kuo2015; @li2004; @li2005a; @li2010; @liao2013; @liu2019a; @mainousa.g.2005; @mielke2005a; @mielke2010a; @mielke2011; @mukherjee2013; @muller2007; @noale2013; @notkola1998; @ostergaard2017; @pan2018; @parikh2011; @peters2009; @raffaitin2009; @rantanen2017; @rea2005a; @redelmeier2019; @reitz2004a; @reitz2010; @ridker2008; @ronnemaa2011; @schilling2017a; @smeeth2009a; @so2017; @solomon2007; @solomon2009a; @solomon2010; @sparks2008; @strand2013; @svensson2019; @szwast2007; @tan2003a; @tynkkynen2016; @tynkkynen2018; @wang2012; @whitmer2005a; @yang2015; @yoshitake1995; @zamrini2004; @zandi2005; @zhu2018a; @zimetbaum1992] Table \@ref(tab:studyCharacteristics-table) presents a summary of the characteristics of each study.

The majority of studies were non-randomised studies with the sole two included randomised controlled trials both examining all-cause dementia. Similarly, a relatively small number of Mendelian randomisation studies were identified, and several performed a two-sample MR analysis using the same summary statistics from published GWAS, leading to complications in synthesis (see Section\@ref()). 

Of the non-randomised studies examining treatments that modify lipid levels, the overwhelming majority examined statin use. For exposue studies, total cholesterol and LDL-cw were the most frequently reported lipid elements.

In terms of outcomes, the vast majority of studies examined either all-cause dementia or Alzheimer's disease, with only a small proportion examining vascular dementia (n=XXX, %). Some rarer outcome classifications such as vascular-component or mixed dementia were also investigated.

Of note, several studies provided evidence on a lipid fraction as part of a wider study on Mets. Raffatin/Ng

Many studies using electronic health records as their data source did not accurately report the diagnostic codes used to identify cases.

Midlife cholesterol levels were of particular interest, with several studies examining this age-range. <!-- TODO Talk a little bit more about age here -->

Three included reports were preprints (denoted in the Table \@ref(tab:studyCharacteristics-table) using an asterisk), one of which had subsequently been published and was captured by the primary literature search. All three included preprints were obtained from the bioRxiv preprint server and described Mendelian randomisation analyses.

Several Mendelian random studies made use of summary level data from the same published GWAS (the Global Lipids Genetics Consortium and the IGAP consortium <!-- TODO CITATION NEEDED -->). As this could incur double counting of participants, and so produce artifically precise estimates if meta-analysed, only results from one of these studies were included <!-- TODO How chosen?? -->. As discussed in Section \@ref(rev-discussion-MR), the choice was not material, as the results from all studies using these data sources were comparable.

<!--- TODO Figures and tables to include here: 

-   Summary of risk of bias (not done)

--->

<!-- TODO Also need to extract the codes used in each study -->

<!-- TODO Need to add abbreviations to table -->

\blandscape{}
<!----------------------------------------------------------------------------->
(ref:studyCharacteristics-caption) Characteristics of included studies, stratified by study design. Note that three studies reported on multiple study designs, and these have been duplicated across the relevant sub-sections.

(ref:studyCharacteristics-scaption) Characteristics of included studies

```{r studyCharacteristics-table, message=FALSE, results="asis", echo = FALSE}
```
<!----------------------------------------------------------------------------->
\elandscape{}

As illustrated in Figure \@ref(fig:cohortLocations), the majority of reports described studies conducted in high-income countries.

<!----------------------------------------------------------------------------->
```{r cohortLocationsSetup, include = FALSE}
```

(ref:cohortLocations-cap) Geographical distribution of study cohorts

(ref:cohortLocations-scap) Geographical distribution of study cohorts

```{r cohortLocations, echo = FALSE, results="asis", fig.pos = "H", fig.cap='(ref:cohortLocations-cap)', out.width='100%', fig.scap='(ref:cohortLocations-scap)'}
knitr::include_graphics(file.path("figures/sys-rev/cohortLocations.png"))
```
<!----------------------------------------------------------------------------->
&nbsp;

of interest, many of the included studies were peformed in Taiwan (`r n_taiwan`), all but one of which made use of the Taiwan National Health Insurance.


### Risk of bias {#risk-of-bias-res}

As discussed, the results of the risk of bias assessments are presented alongside their corresponding result. 

However, as a brief summary of the biases at play:

Both randomised controlled trials were considered to be at low risk of bias. 

For NRSI
For NRSE 

For MR studies

Additionally, Several synthesis were also assessed to be at risk of bias due to missing evidence. A key issue identified relating to bias due to missing data was the potential for the search to miss relevant Mendelian randomisation studies which had examined the association between Alzhiemer's disease and multiple risk factors. This is discussed further in Section \@ref(rev-discussion-MR).

<!-- TODO Discuss Taiwan health database as good evidence on non-Caucasian idenitiies -->

Haag is a good example of potentially missing results - number of cases of other outcoems reported, but no analysis performed.

&nbsp;

<!----------------------------------------------------------------------------->

### All-cause dementia

__Statins__

```{r rctStatinDementia, include = FALSE, message=FALSE, warning=FALSE}
```

The two randomised controlled trials provided very weak evidence (`r rct_statin_acd`) of an effect on statin use on all-cause dementia risk (Figure \@ref(fig:rctStatinDementiaFig)).

<!-----------------------------------------------------------------------------> 

(ref:statinsRCT-cap) Random effects meta-analysis of randomised controlled trials examining statin statins on all-cause dementia

(ref:statinsRCT-scap) Random effects meta-analysis of statins on all-cause dementia

```{r rctStatinDementiaFig, echo = FALSE, results="asis", fig.pos = "H", fig.cap='(ref:statinsRCT-cap)', out.width='100%', fig.scap='(ref:statinsRCT-scap)'}
knitr::include_graphics(file.path("figures/sys-rev/fp_rct_statins_Dementia.png"))
```



```{r obsStatins, include = FALSE, message=FALSE, warning=FALSE}
```

In contrast, a meta-analysis of prospective observational studies provided evidence of a protective effect of statins use on all-cause dementia risk (`r obsStatins$Dementia`, Figure \@ref(fig:obsStatinDementiaFig)).

<!----------------------------------------------------------------------------->
(ref:obsStatinDementia-cap) Random effects meta-analysis of non-randomised studies examining the effect of statin use on all-cause dementia

(ref:obsStatinDementia-scap) Random effects meta-analysis of statins on all-cause dementia

```{r obsStatinDementiaFig, echo = FALSE, results="asis", fig.pos = "H", fig.cap='(ref:obsStatinDementia-cap)', out.width='100%', fig.scap='(ref:obsStatinDementia-scap)'}
knitr::include_graphics(file.path("figures/sys-rev/fp_obs_Statin-Ever_Dementia.png"))
```
<!----------------------------------------------------------------------------->

```{r mrStatins, include = FALSE, message=FALSE, warning=FALSE}
```

A single study examing the effect of HMGCR inhibition on any dementia, via a , and found weak evidence for an effect (`r mrStatin$Dementia`).

__Lipids__

* NRSE

* MR studies per 1-SD

In NRSI, statin use was associated with a reduce risk of all-cause dementia (`r obsStatins$Dementia`, Figure \@ref(fig:statinsNRSI)).

&nbsp;

Several studies presented results per SD change in a lipid fraction.

In Mendelian randomisation studies, a similar

<!----------------------------------------------------------------------------->
&nbsp;

### Alzheimer's disease

There were no randomised trials of statin, or any other lipid regulating agent, use on Alzheimer's disease, though several observational studies reported on this outcome, providing evidence for a protective effect (`r obsStatins$AD`; Figure \@ref(fig:obsStatinADFig)) 

<!----------------------------------------------------------------------------->
(ref:obsStatinAD-cap) Random effects meta-analysis of non-randomised studies examining the effect of statin use on Alzheimer's disease

(ref:obsStatinAD-scap) Random effects meta-analysis of statins on Alzheimer's disease

```{r obsStatinADFig, echo = FALSE, results="asis", fig.pos = "H", fig.cap='(ref:obsStatinAD-cap)', out.width='100%', fig.scap='(ref:obsStatinAD-scap)'}
knitr::include_graphics(file.path("figures/sys-rev/fp_obs_Statin-Ever_AD.png"))
```
<!----------------------------------------------------------------------------->

Two Mendelian randomisation studies looked at specifically as a result of by HMGCR inhibition (mediated by a single SNP; rs17238484). The first used a one sample approach (SNP-exposure and SNP-outcome associations are estimated using the same dataset) in a large Copenhagen-based cohort, while the second made use of summary level data obtained from the Global Lipids Genetic Consortium (SNP-exposure) and the International Genomics of Alzheimer's Project (SNP-outcome). 

<!----------------------------------------------------------------------------->
(ref:mrStatinDementia-cap) Random effects meta-analysis of LDL-c lowering via genetic HMGCR inhibition on Alzheimer's disease

(ref:mrStatinDementia-scap) Random effects meta-analysis of LDL-c lowering via genetic HMGCR inhibition on Alzheimer's disease

```{r mrStatinDementiaFig, echo = FALSE, results="asis", fig.pos = "H", fig.cap='(ref:mrStatinDementia-cap)', out.width='100%', fig.scap='(ref:mrStatinDementia-scap)'}
knitr::include_graphics(file.path("figures/sys-rev/fp_MR_HMGCR_AD.png"))
```
<!----------------------------------------------------------------------------->

__Lipids__




<!----------------------------------------------------------------------------->
&nbsp;

### Vascular dementia 

__Statins__

<!----------------------------------------------------------------------------->
(ref:obsStatinVaD-cap) Random effects meta-analysis of non-randomised studies examining effect of statin use on vascular dementia

(ref:obsStatinVaD-scap) Random effects meta-analysis of statins on vascular dementia

```{r obsStatinDVaDFig, echo = FALSE, results="asis", fig.pos = "H", fig.cap='(ref:obsStatinVaD-cap)', out.width='100%', fig.scap='(ref:obsStatinDVaD-scap)'}
knitr::include_graphics(file.path("figures/sys-rev/fp_obs_Statin-Ever_VaD.png"))
```
<!----------------------------------------------------------------------------->


__Lipids__


### Dose response meta-analysis of lipid levels {#dose-response-results}

<!-- TODO Cite excluded studies and list exclusion reasons -->

Several studies were excluded from the dose-response meta-analysis, as the number of cases/controls per dose group could not be calculated and the corresponding author for the study did not respond to clarification requests. The results from the dose response analysis can be seen in Figure \@ref(fig:lipidsDoseResponse).

&nbsp;

### Sources of heterogeneity 

Data on another potentially important source of heterogeneity
Plus other such as education level and baseline cognitive scores, but data were either not reported for most studies or when reported, were too diverse to synthesize.

<!-- Detail that some of these are exploratory, in particular the effect of different scales on the association between the groups. **[CROSS]** -->

<!-- Important here to examine effect of risk of bias (due to immortal time in observational studies). -->

<!-- Also important to examine the effect of: -->

<!-- * age at baseline -->
<!-- * sex (percentage) -->
<!-- * education -->
<!-- * baseline cognitive scores (how to group) -->

<!-- ### Sensitivity analyses -->

&nbsp;
<!----------------------------------------------------------------------------->

### Sensitivity analysis

Given that the majority of the studies included in the review were of 

### Small study effects {#sys-rev-pub-bias}

<!-- -   Check if there are protocols available for any of the published reports (unlikely for non-randomised controlled trials), and whether there were -->
<!-- -   Vascular dementia has substantially less published reports. Many (reference Smeeth et al 2010 here) simply group into AD and non-AD making comparison between published studies difficult -->

There was little evidence of publication bias across the evidence base (Figure \@ref(fig:funnelStatinsAny)).

<!-- One particularly interesting meta-bias potentially applicable in this review is the definition of code lists across -->

<!-- Many studies using large observational electronic health records (n=?) did not report the code-lists used define the outcome events in their analysis. Depsite attempts to obtain this additional information through contact with a -->

&nbsp;

<!----------------------------------------------------------------------------->
(ref:funnelStatinsAny-cap) Funnel plot of results examining the relationship between statins and any dementia

(ref:funnelStatinsAny-scap) Funnel plot of results examining the relationship between statins and any dementia

```{r funnelStatinsAny, echo = FALSE, results="asis", fig.pos = "H", fig.cap='(ref:funnelStatinsAny-cap)', out.width='100%', fig.scap='(ref:funnelStatinsAny-scap)'}
knitr::include_graphics(file.path("figures/sys-rev/fp_rct_statins_Dementia.png"))
```

<!----------------------------------------------------------------------------->
&nbsp;

### Added evidental value of including preprints {#sys-rev-including-preprints-res}

<!-- TODO Will have to get actual numbers for this section -->

As show in Figure \@ref(prisma-flow-fig), the number of hits returned by the preprint searching was not substantial (bioRxiv = 256, medRxiv = 0). From these hits, three preprints were included in the review as reports of eligible studies, of which two were unique reports not captured by the main search.

For the analysis of XX on YYY, preprints added ...

Investigation of the publication status of the two unique preprints indicated that one has since been published (in late 2019 date).<!-- TODO check publication date -->

<!-- QUESTION Can I explore the amount of information added by the preprints quantitatively using the weights in the meta-analysis -->

&nbsp;
<!----------------------------------------------------------------------------->

## Discussion

<!-- TODO Check inclusion of Rockwood 2002:  -->
This review has presented a summary of the available evidence on the association between lipids, and treatments that modify lipids such as statins, and the subsequent risk of dementia. 

The discussion seeks to  to summarise the key findings in terms of literature sources and results as reported. A detailed comparison across the evidence sources, exposure measures and sources of bias reported here is presented as part of the triangulation exercise (Chapter \@ref(tri-heading)).

<!----------------------------------------------------------------------------->
&nbsp;

### Summary of findings

There was some evidence of protective effect of statins on all-cause and Alzheimer's disease dementia when looking at solely at observational studies. This finding was not supported by evidence from the two available RCTs, or by studies that emulated statin treatment using a genetic proxy, suggesting that these findings may be a result of heterogenity in exposure (e.g. mid-life in studies of lipids with late-life lipid reduction in RCTs) or alternatively due to biases within the non-randomised studies. 

Some evidence that age has impact on observed lipid-dementia relationship - use to link 

The majority of studies were non-randomised studies of lipids, or treatments that affect lipid levels such as statins. This distribution of evidence between analytical designs is to be expected. Randomised controlled trials of dementia are particularly challenging, as the long follow-up, necessary due to the long latent period of the condition, makes trials logistically challenging and financial expensive. Similarly, Mendelian randomisation is a comparatively new study design (as illustrated in Figure \@ref(fig:typeByYear)), and so only appears in the literature in recent years, driven by the availability of Alzheimer's disease summary genome wide association studies (GWAS) that form the basis of two-sample Mendelian randomisation approach.

<!----------------------------------------------------------------------------->
(ref:typeByYear-cap) __Study designs by year of publication__ - 

(ref:typeByYear-scap) Study designs by year of publication

```{r typeByYear, echo = FALSE, results="asis", fig.pos = "H", fig.cap='(ref:typeByYear-cap)', out.width='100%', fig.scap='(ref:typeByYear-scap)'}
knitr::include_graphics(file.path("figures/sys-rev/type_by_year.png"))
```
<!----------------------------------------------------------------------------->

A common theme across the evidence base was a lack of data on the association of vascular dementia. This is particularly interesting given that lipids and statins are primarily related to vascular disease. __Can't look forward to other chapter__ There is the potential that studies encountered similar difficulties in address the unexpected results observed in the CPRD analysis in Chapter \@ref(cprd-analysis-heading), likely due to confounding by indication, and so may suffer from the "file-drawer effect".[@rosenthal1979] For vascular dementia, few Mendelian randomisation studies examined this outcome, primarily because of the absence (until recently) of GWAS of this outcome.

<!--- 
QUESTION
This does introduce some conflicts with other studies where a change score has been dichotomised in order to have a binary dementia or not dementia outcome. - No these studies should not be included, as not recognised criteria.
-->

<!-- __Exclusion of the PROSPER trial__ Will need to have a good bit here on the exclusion of this RCT, particularly as it is included in the Cochrane review on the topic. Cross-reference with the meta-bias section in the early section, on the impact of only included studies that sought to make a definitive diagnosis, rather than using change in cognitive assessment scales. -->

Of note, this review did not include the commonly cited PROSPER study, which examined the effect of pravastatin on CVD risk,[@shepherd2002a] reporting on cognitive outcomes as one of several secondary outcomes. While widely cited in relation to the effect of statins on dementia and included in the Cochrane review of RCTs on this topic,[@mcguinness2016] the trial only reported on the change in a range of cognitive measures (MMSE, Stroop test, Picture-Word Learning test and others) over follow-up. Though an useful indicator of general cognitive decline, it is not equivalent to a dementia diagnosis using recognised criteria, as cognitive tests should feed into a broader diagnostic pathway (see Section \@ref(diagnostic-criteria)). As such, this trial did not met the inclusion criteria for this review.

<!-- The findings on high TC in midlife being associated with increased risk of late-life AD are still limited to the few studies that report relevant data from Scandinavia and the United States. The wider literature shows that level of TC in populations varies according to diet, urbanization, ethnicity, and income [56, 57]. from Anstey 2017 -->

Questions over missing results - evidence from one of the conference abstract analysis pairs that a non-significant results are being suppressed.[@yamada2009; @yamada2009a] In addition, there were some concerns over the potential for estimates to be missing from the meta-analysis of observational studies not at random, given the preferential reporting of significant results observed in a number of analysis (see Section \@ref(risk-of-bias-res)).


<!----------------------------------------------------------------------------->
&nbsp;

### Comparison with previous reviews {#rev-previous-reviews}

<!-- TODO This section will be informed by the review published as a preprint -->

__This section will be completed once Georgia has completed her analysis, and can also be cross-references with the meta-meta analysis published is Brain Sciences recently.__

While conducting this review, I identified several previous systematic reviews of this topic.[@chu2018; @yang2020; @muangpaisan2010; @poly2020;@kuzma2018a; @kuzma2018a] However, this review is the first to use established domain based assessments tools (for example, the RoB 2 tool for randomized controlled trials)[@sterne2019] to assess the risk of bias in included studies, and explore the heterogeneity of results across different levels of risk of bias levels. Some previous reviews did assess risk of bias, but used non-domain based assessment tools, such as the Newcastle-Ottowa scale.[@anstey2015; @poly2020] 

Similar the on previous review of Mendelian randomisation studies examining risk factors for Alzhiemer's disease was conducted prior to the majority of MR studies included in this review being published, and extracted results including SNPs in APoE4 (see the following section for a discussion of the bias this introduces).

However, despite these differences in timescales and methodology, the duplication of work across reviews (including this review) is substantial. In retrospect, an alternative approach to conducting a further systematic review from scratch could have been employed. Known as an umbrella review, or review-of-reviews, these studies use other systematic reviews rather than primary studies as the unit of analysis.[@aromataris2015; @smith2011] This approach would have enabled more efficient identification of relevant primary studies to which the methods which sets this review apart from other published reviews could have been applied.

<!----------------------------------------------------------------------------->
&nbsp;

### Inclusion of Mendelian randomisations studies {#rev-discussion-MR}

One of the strengths of this review is it's inclusion of Mendelian randomisation studies as a source of evidence.

Mendelian randomisation is a powerful analytical technique, using natural variation in participants genomes to (assuming the assumptions of the method are valid), though it's inclusion as an acceptable study design in this review was complicated by a number of factors. 

Firstly, this study design is relatively new, particularly when compared to randomised trials or cohort studies. Figure \@ref(fig:typeByYear) demonstrates that Mendelian randomisation studies only begin to appear in the evidence base much later than NRSE/NRSI. As such, the process and tools for systematically assessing them are not as well developed, likely due to the limited availabilt of large scale GWAS datasets needed for two sample MR. A key example of this is in the absence of validated search filters for Mendelian randomisations studies. This limitation is further complicated by the varying terminology used to describe the method, particularly in the early years of it's application.

Additionally, there is currently no widely used risk-of-bias assessment tool for Mendelian randomisation studies. A recent commentary provided a checklist  interpreting Mendelian randomisation studies, this guide includes reporting items in their quality checklist. While reporting quality is important, it is a separate consideration to internal validity, as discussed in Section \@ref(). Similarly, a previous review of Mendelian randomisation studies used the Q-Genie tool, which was validated to assess the quality of genetic association studies in meta-analysis.[@sohani2015] While this tool addresses the studies used, it does not access the additional methodological considerations of the analysis of the Mendelian randomisation analysis itself. <!-- TODO Read this tool. --> For this review, I utilised the best available author-devised tool, sourced on a recent review of systematic reviews of Mendelian randomisation studies. 

<!-- TODO Need to add some text to the ROB section of this chapter on distinction between reporting and quality -->

<!-- Problems with overlapping samples in reviews of Mendelian randomisation studies in that may be double counting participants -->
<!-- TODO Check discussion on this point from Matt Lee thesis, in email -->
<!-- TODO Email George with query on dual counting, following review of email with Julian -->

As a further stumbling block, Mendelian randomisation, particularly when using a two-sample summary data design, is a form of analysis that lends itself to multiple exposure-outcome comparisons. This is particularly relevant to the consideration of bias due to missing evidence. As an example, through snowballing and other measures, I identified at least one relevant Mendelian randomisation study that had not been identified by the search strategy.[@larsson2017b] On review of this paper, the search would not have been expected to find it given the absence of any lipid-related keywords in the title and abstract. The study examined the association between lipid fractions with Alzheimer's disease as one of many risk factors for the condition. Studies such as this can introduce bias into a systematic review, as it is commonly only those risk factors that show a statistically significant result that are reported in the abstract and so are captured by the search. This may bias systematic reviews, including this one, as the analysis of multiple risk factors against a single outcome within a single publication becomes more common. These studies are described as "unknown unknown's" in the context of the RoB-ME tool, and are particularly challenging (as opposed to an analysis that was insufficiently reported to be included in the statistical analysis, or the "known unknown's").

Useful future work to improve the methodology for inclusion of Mendelian randomisation studies in systematic reviews should involve the development of a validated search filter for this study design.[@waffenschmidt2020; @wagner2020] Alternatively, in better-resourced reviews, a dedicated search for "risk factors" and "dementia" and "Mendelian randomisation", followed by manual review of studies that look across multiple risk factors, would be advisable. This was not feasible in the context of this review, given the large number of records to be screened, even when using study design filters (n=`r comma(16109)`). Additionally, the value of methods that supporting the traditional bibliographic database search, such as snowballing (forwards and backwards citation chasing) and communication with relevant topic experts should not be underestimated. Finally, development of a risk-of-bias assessment tool by a panel of methodologists and analysts would be of substantial benefit. 

<!-- TODO Ask Matt Page whether he has done any work on this? Studies looking at many risk factors don't report null results in abstract and so do no -->

Talk about problem with studies sharing underlying datasets in two sample Mendelian randomisation frameworks - c.f. EHR databases, which contain the same underlying sample but use different sub-samples on account of the distinct codes/conditions and timepoints used to defined the study cohorts, Mendelian randomisation analysis can be multiple studies using the exact same summary statistics from the same cohorts

One item of particular interest is the attenuation of any effects observed by Mendelian randomisation studies following the adjustment for/exclusion of genetic variation in the Apoe4 gene region. As covered in the introduction (see Section \@ref()), increasing number of ApoE4 alleles is a major independent risk factor for Alzheimer's disease, and so violates the exclusion restriction criteria of .

Introduce assumptions underlying Mendelian randomisation (with image). Higlight that most if not all included analysis initially describing a protective effect of LDL-c, which attenuates to the NULL once ApoE4 gene regions are removed. Note the region must be quite wide - talk about controversy surrounding Benn paper

Additionally many of the studies suggested a link between, but then in the sensitivity analyses or in the Discussion disclaim that when adjusted for ApoE4, any association was attenuated to the null. This point is important given one of the core assumptions of 

<!-- TODO Diagram showing three assumptions of MR, comparing it to a graph showing the invalidation of no pleiotopy assumption -->

MR are limited by the need for SNP to be present in both datasets (for TSMR).

<!----------------------------------------------------------------------------->
(ref:mrDuplication-cap) Summary of duplication of Mendelian randomisation studies which used summary statistics from the Global Lipid Genetics Consortium (GLGC) and the International Genomics of Alzheimer's Project (IGAP). Note that the Alzheimer’s Disease Genetics Consortium (ADGC) is one of the component consortia of IGAP.

(ref:mrDuplication-scap) Summary of duplication of Mendelian randomisation studies

```{r mrDuplicationSetup, include=FALSE}
```

```{r mrDuplication, echo = FALSE, results="asis", fig.pos = "H", fig.cap='(ref:mrDuplication-cap)', out.width='100%', fig.scap='(ref:mrDuplication-scap)'}
knitr::include_graphics(file.path("figures","sys-rev","mrDuplication.png"))
```
<!----------------------------------------------------------------------------->

<!-- TODO Talk about discrepancies between results of different methods, but highlight that it is a problem as the information is being contributed by the same people. -->


Recommend future large scale GWAS of other dementia outcomes, notably vascular dementia, or the use of existing GWAS to  <!-- TODO At least on vascular dementia GWAS exists - find it and cite it here. -->

A key example of the importance of . In almost all cases, Mendelian randomisation studies examine. A clear exampe of this is Ben et al, 2017, where the ApoE variants were not sufficient identified and excluded, and the published paper detailed evidence for a protective effect of LDL-c was identified (`estimate(0.83, .75,.92,"RR")`).[@benn2017] Following several rapid responses, the data was re-analysed excluding a larger area around ApoE4 which attenuated this finding towards the null.[@benn2017a]


<!----------------------------------------------------------------------------->
&nbsp;

### Inclusion of preprints

```{r preprintGrowthSetup, include=FALSE}
```

As highlighted in Section \@ref(diverse-sources-preprints), this review explicitly sought to synthesize evidence across different publication statuses (preprinted vs. published). Using the tool described in Chapter \@ref(sys-rev-tools-intro), two preprint serves related to health and biomedical sciences were search as part of this review (see Appendix \@ref(appendix-medrxivr-code) for the code used to search the repositories). There were several relevant preprints captured by the search. The added evidential value of including these preprints was highlighted in Section \ref(sys-rev-including-preprints-res). 

Three relevant preprints were identified 

medRxiv grew out of the Epidemiology and Clinical Trials categories of the bioRxiv preprint server. The small number of studies return by the searches (or the absence of any hits in the medRxiv database - see Figure \@ref(fig:prisma-flow-fig)) is due to the timing of the preprint searches. The searches for this review were performed in mid-July 2019, but the first medRxiv preprint was registered on 25th June 2019. As such, at the point it was searched, the medRxiv database contained only a small number of records (n=`r n_at_search["med"]`).

Of the three identified preprints, two were subsequently published as of September 2020. This fits well with the analysis presented in Section \@ref(medrxivr-limitations) that, allowing for a two-year lag, approximately two-thirds of preprints are published.

While none of the preprints contributed uniquely to the review, this is again. Consider the example of one of the Mendelian randomisation analyses of the effect of LDl-c on Alzheimer's disease, which found no impact of LDL-c on AD following removal of APOE4. The manuscript was initially published in bioRxiv in July 2017[@zhu2017] which was subsequently published in Nature Communications in January 2018, following peer-review.[@zhu2018] While this study was captured by both the preprint and published searches in this review, had the searches been run within this window, the preprint would have contributed unique data to the review. This illustrates that while it may not have aided this review, if the aim is to find the current state of the art in the topic area at the time of searching, inclusion of preprints is a necessity.

Of note, since the start of my review, inclusion of preprints in systematic reviews has now become widespread due to the role of preprint servers, in particular medRxiv, as a key evidence dissemination venue during the early stages of the COVID-19 pandemic. However, how well this adoption of preprints will transfer to other topics, where the speed of research does not put the same focus on preprinted articles, is currently unknown.

<!-- TODO Discuss information here that I responded to Julian with re: % of preprints that published -->

<!----------------------------------------------------------------------------->
(ref:preprintGrowth-cap) __Growth of preprint repositories over time__ - Given the relative sizes of the preprint repositories at the time the searches for this review were conducted (bioRxiv n= `r n_at_search["bio"]`, medRxiv n = `r n_at_search["med"]`), the number of hits returned by each is expected.

(ref:preprintGrowth-scap) Growth of preprint repositories over time

```{r preprintGrowth, echo = FALSE, results="asis", fig.pos = "H", fig.cap='(ref:preprintGrowth-cap)', out.width='100%', fig.scap='(ref:preprintGrowth-scap)'}
knitr::include_graphics(file.path("figures/sys-rev/preprint_growth.png"))
```
<!----------------------------------------------------------------------------->
<!-- TODO Cite studies detailing increasing acceptance as a valid form of evidence.  -->

<!-- This does speak to the over-representation of certain study designs in the preprint literature  -->

<!-- Regardless, some studies were identified -->

&nbsp;
<!----------------------------------------------------------------------------->

<!-- ### Comments on the process -->

<!-- As part of the reflective element of this thesis, I collated my experiences on performing a systematic review. -->

<!-- &nbsp; -->

<!-- #### Protocol registration -->

<!-- While the protocol was registered on the Open Science Framework largely to allow for the sharing of associated documents such as the proposed search strategy, anecdotal evidence from colleagues and collaborators suggested that the findability of the protocol was limited. In hindsight, I should also have cross-posted the protocol on PROSPERO, the international prospective register of systematic reviews,[@booth2011] (which does not allow for the uploading of related files). -->

<!-- &nbsp; -->

<!-- #### Workload -->

<!-- Systematic reviews should not be performed as part of a thesis, without suitable support and resourcing guaranteed. Assumption that everyone does a systematic review (without risk-of-bias assessments, inclusion of all literature, searching for other reviews) is foolish. -->

<!-- Average time to complete a s -->

<!-- However, new developments such as automated screening would allow for a reduced need for personnel to work on these things. [@pham2021] -->

<!-- &nbsp; -->

<!-- #### People management -->

<!-- This review provide an excellent opportunity to gain experience in managing a team of researchers. However, due to the need for dual screening and data extraction over a long period of time, a number of external researchers became involved in this review. I found the people-management aspect particularly challenging, and in retrospect could definitely have improved the process through better communication of expectations and deadlines. -->

<!-- &nbsp; -->
<!----------------------------------------------------------------------------->

<!-- ### Validation against recent umbrella review -->

<!-- In summer 2020, an umbrella review (also known as a review-of-reviews) of the impact of lipids levels of Alzheimer's disease risk -->


<!-- The study included XXX reviews and YYY primary studies, and I used this list of primary studies as an extra validation step to assess the accuracy of my work. -->

<!-- Umbrella reviews follow a systematic review approach, but their unit of interest is other systematic reviews rather than primary studies. In fields where there is a lot of existing reviews, they -->

<!-- All studies identified by reviews included in te -->

<!-- In hindsight, this approach ma -->


&nbsp;

<!----------------------------------------------------------------------------->

### Open data sharing {#sys-rev-open-data}

As discussed in Section \@ref(dose-response-results), many primary studies did not report important elements, and so these could not be extracted. This limiation was compounded by the expected low response rate to requests for further information from primary authors (although, in hindsight, the form of contact used (email) has been shown to be less successful in eliciting responses from authors when compared with telephoning[@danko2019]).

While contacting authors is worthwhile, as it can substantially change the conclusion of a systematic review[@meursingereynders2019] and is not too costly to systematic reviewers,[@cooper2019] a far preferable option is that the authors of primary studies readily deposit all relevant study data at the point of publication. 

Based on my experience of extracting data for this review, I co-wrote an guidance article to aid primary prevention scientists in preparing and sharing their data so that it can easily be incorporated into a evidence synthesis exercise, using a trial of mindfulness interventions as an case study.[@hennessy2021]

Similarly, a substantial amount of time and effort has gone into making the data obtained by this review openly available to other researchers. <!-- TODO Cite the Zenodo repo here! -->

&nbsp;
<!----------------------------------------------------------------------------->

### Strengths and limitations

<!-- TODO Cross reference with other discussion sections to ensure material isn't being repeated. -->

#### Strengths

I believe there are four aspects where this review is distinct from those reviews already available in the published literature (as identified by <!-- TODO CITATION NEEDED -->):

-   *Comprehensiveness:* While several reviews of this research topic exist,[@chu2018; @yang2020; @muangpaisan2010; @poly2020] the overlap between the list of studies included in each is not 100%. As part of this review, I have not only performed a original search of primary literature databases, but have also screened the reference lists of comparable reviews to ensure no study has been omitted. <!-- TODO Though this could --> In addition

-   *Structured risk-of-bias assessment:* The majority of the highly cited reviews on this topic either do not formally consider the risk of bias in the observational studies they include or do not use an appropriate domain-based assessment tool (e.g. ROBINS-I/E). This is important area in which this thesis can add value, as based on the risk-of-bias assessments I have performed to date, several primary studies are at high risk of bias and this should be reflected in the findings of any review on this topic.

-   *Inclusion of preprints:* Unlike other available reviews and enabled by the tool described in Chapter \@ref(sys-rev-tools-heading), this review systematically searched preprinted health-related manuscripts as a source of grey literature. As part of this chapter, I plan to examine the extent of the additional information provided to the review by the inclusion of preprints.

<!-- TODO Comment on the fact that several of the studies identified through the search of preprints, were subsequently published, but much later than the cut-off point for the search (e.g. they would not have been captured by the search for published papers) -->

- *Contribution to methods work:* A large part of this review was the associated work on improving research synthesis methods. This work is detailed as relevant throughout the Chapter, often referring to additional work detailed in the . In addition this review was used to pilot an upcoming risk of bias tool

<!----------------------------------------------------------------------------->
&nbsp;

#### Limitations

The primary limitation of this review is that several included studies used data from EHR databases, which come with serious concerns regarding validity [@hsieh2019] [@mcguinness2019validity; @wilkinson2018] Relatedly, several  studies which made use of electronic health record database did not report the specific code lists used, potentially introducing substantial heterogeneity between effect estimates. An empirical example of the effect of differing EHR code list is presented as part of the analysis in Chapter 4 (see Section \@ref(comparing-codelists)).

In addition, the fact that only a sample of records were dual screened at the title/abstract and full-text stages is a potential limitation, as there is a chance that some eligible records could have been excluded. However,  evidence from assessments of inter- and intra-rater reliability indicate that is is not a major concern.

One particular limitiation with regards to the risk-of-bias assessment is the fact that the ROBINS-E assessments were performed without the tool being finalised. This meant that there were no signalling questions to guide the domain-level risk-of-bias assessment, which may have influenced the accuracy with which domain-level judgements were assigned. However, there is no published empirical evidence supporting the need for signalling questions, and assessment of inter-rater reliability across the different tools did not indicate a specific problem with the ROBINS-E assessments. In fact, low agreement was common across the tools, though this is expected based on the available literature.[@jeyaraman2020]

One further limitation is the fact that the risk of bias due to missing evidence assessment, combined with some empirical evidence that some studies were missed by the search but contained relevant studies is a definite limitation of this review (see Section \@Ref(rev-discussion-MR) above for a fuller discussion of this issue with respect to Mendelian randomisations studies). Unfortunately, this is probably a common limitation across all reviews, based on the way in which increased sensitivity must be balanced with a reasonable workload.

<!----------------------------------------------------------------------------->
&nbsp;

## Conclusions

In this chapter I have presented a comprehensive systematic review of the different sources of evidence available which examined the relationship between lipid levels and dementia use. 

This work built on the tool introduced in the preceeding chapter (Chapter \ref(sys-rev-tools-heading)), and findings from this review are used though out the subsequent chapters: in Chapter \@ref(cprd-analysis-heading),  summary of the evidence guided the choice of analysis approach, ensuring that the new analysis was at risk of a different source of bias; while in Chapter \@ref(ipd-heading), prospective cohorts identified by the review were contacted in an attempt to obtain individual participant data; finally, the cumulative effect measures calculated here are used as a key source of evidence for the triangulation exercise presented in Chapter \ref(discussion-heading).

<!-- IDEA Link here to IPD, talking about how the meta-regression on key variables suggested the use of sex and age as key variables-->

\newpage 

## References

