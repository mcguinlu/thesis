---
bibliography: bibliography/references.bib
csl: bibliography/nature.csl
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
  bookdown::word_document2: 
      toc: false
      toc_depth: 3
      reference_docx: templates/word-styles-reference-01.docx
      number_sections: false 
  bookdown::html_document2: default
documentclass: book
---

```{block type='savequote', include=knitr::is_latex_output(),quote_author='(ref:sys-rev-quote)', echo = FALSE}
"It is surely a great criticism of our profession that we have not organised a critical summary by speciality or sub-speciality, up-dated periodically, of all relevant RCTS."  
```

(ref:sys-rev-quote) --- Archie Cochrane, 2000 [@cochrane20001931]

# Systematic review of all available evidence {#sys-rev-heading}

\minitoc <!-- this will include a mini table of contents-->

```{r, echo = FALSE, warning=FALSE, message=FALSE}
source("R/doc_options.R")
source("R/helper.R")
knitr::read_chunk("R/03-Code-Systematic-Review.R")
doc_type <- knitr::opts_knit$get('rmarkdown.pandoc.to') # Info on knitting format
```

<!-- ## To Do -->

<!-- -   TODO Examine how many times a primary study was included in a review\ -->
<!-- -   TODO Look at whether searching preprints made any difference to my results -->
<!-- -  TODO Could apply ROBIS to previous reviews to see how they stack-up? (This may be done as part of Georgia's summer project) -->

<!-- TODO Extra studies to include: -->

<!-- * Smeet et al. reference in the CPRD analysis -->
<!-- * Larsson - 10.1136/bmj.j5375 [@larsson2017b] -->
<!-- * Statins and the risk of dementia Jick -->

## Lay summary

As introduced in previous chapters, 

<!-- IDEA Evidence map - show the distribution of the different studies population across the world. Distribution will have to be of participants, not primary authors. Though could use to show how data is being used globally -->

## Introduction {#sys-rev-intro}

The aim of this chapter is to systematically review all available literature on the association between blood levels of total cholesterol and it's constituent parts (HDL-c,LDL-c and triglycerides) on the subsequent risk of dementia.

Based on the review of existing literature, no previous evidence synthesis exercise attempted to examine all available literature simultaneously. 

Only by looking across all available evidence can the true, given the often limited use of one type of evidence (RCTs are hard to do, obs studies are biased).



Literature con

## Methods

### Protocol

A pre-specified protocol for this analysis was registered using the Open Science Framework, and is available for inspection.[@mcguinnessluke2020] 

### Search strategy

I systematically search electronic bibliographic databases to identify potentially relevant records. The search strategy used in each database was developed in an iterative manner using a combination of free text and controlled vocabulary (MeSH/EMTREE)[@lefebvre2019searching] terms to identify studies which have examined the relationship between blood lipids levels and dementia, incorporating input from an information specialist. The strategy will include terms related to lipids, lipid modifying treatments, and dementia and its sub-types, and was designed for MEDLINE before being adapted for use in the other bibliography databases listed. An outline of the general strategy is presented in the Table \@ref(tab:searchOverview-table) below and the full draft search strategies for each database are presented in Appendix \@ref(appendix-search-strategy). <!-- TODO Need to actually attach each search strategy, and detail the specific databases used. See search strategy results. -->

&nbsp;

<!----------------------------------------------------------------------------->
(ref:searchOverview-caption) Summary of systematic search by topic. For a full list of search terms, see Appendix \@ref()

(ref:searchOverview-scaption) searchOverview

```{r searchOverview-table, message=FALSE, results="asis", echo = FALSE}
```
<!----------------------------------------------------------------------------->

&nbsp;

To ensure that the study design filters are not excluding potentially relevant records, a random sample of 500 records identified by the main search but excluded by the filters (defined as "8 NOT 14" in Table \@ref(tab:searchOverview-table)) was screened. If any potentially relevant studies are identified, their titles and abstracts were searched for key terms that can be incorporated into the filters to improve search sensitivity.

<!-- TODO need to comment on this feed-back process, or remove -->

The following databases will be searched from inception onwards: Medline, EMBASE, Psychinfo, Cochrane Central Register of Controlled Trials (CENTRAL), and Web of Science Core Collection (for the specific databases available thi via the Univeristy ). We will also search clinical trial registries, for example ClinicalTrials.gov, to identify relevant randomized controlled trials. As the contents of the Web of Science Core COllection can vary by institution, the specific database searched via this platform are listed in Appendix \@ref(appendix-wos-databases).

The abstracts list of relevant conferences (e.g. the proceedings of the Alzheimer's Association International Conference, published in the journal Alzheimer's & Dementia) will be searched. <!-- TODO how were these searched? -->
 Grey literature will also be searched via ProQuest, OpenGrey and Web of Science Conference Proceedings Citation Index, while theses will be accessed using the Open Access Theses and Dissertations portal. We will also search bioRxiv and medRxiv, preprint repositories using a tool built as part of this thesis, to identify potentially relevant studies. Finally, the reference lists of included studies will be searched by hand while studies citing included studies will be examined using Google Scholar (forward and reverse citation searching).

<!-- TODO Look at citationchaser for this, but have a cut-off publication date of  -->

<!-- Include table with overview of search strategy here: summarise topics and how they were combined -->


### Study selection

Records weree imported into Endnote and deduplicated using the method outlined in Bramer et al. (2016).[@bramer2016] In summary, this method uses multiple stages to identify potential duplicates, beginning with automatic deletion of records matching on multiple fields ("Author" + "Year" + "Title" + "Journal"), followed by manual review of less similar articles (e.g. those matched based on the "Title" field alone).

Screening (both title/abstract and full-text) was performed using a combination of Endnote, a citation management tool,[@hupe2019] and Rayyan, a web-based screening application.[@ouzzani2016] Title and abstract screening to remove obviously irrelevant records was performed primarily by the primary author, with a random selection of excluded records being screened in duplicate to ensure consistency with the inclusion criteria.

Full-text screening will also be completed in full by the primary author. A second reviewer will screen a random sample of included and excluded records, in addition to any records identified by the first reviewer as being difficult to assess against the inclusion criteria. Reasons for exclusion at this stage will be recorded. Disagreements occurring during either stage of the screening process will be resolved through discussion with a senior colleague. A PRIMSA flow diagram will be produced to document how records moved through the review.[@zotero-766]

#### Inclusion criteria

We will seek studies that examine the relationship between blood lipid levels (or any specific lipid fraction, including total cholesterol, HDL, LDL, and triglycerides) and risk of incident dementia/MCI. Eligible study designs include randomized controlled trials and non-randomized observational studies of lipid modifying treatments, longitudinal studies examining the effect of increased/decreased blood lipid levels, and genetic instrumental variable (Mendelian randomization) studies examining the effect of genetically increased/decreased blood lipid levels.

Participants will be free (or assumed to be free) of dementia/MCI at baseline. Studies of any duration will be included to allow for exploration of the effect of length of follow-up on the effect estimate using meta-regression. No limits will be placed on the sample size of included studies.

Eligible studies will define dementia according to recognised criteria, for example the National Institute of Neurological Disorders and Stroke Association-Internationale pour la Recherche en l'Enseignement en Neurosciences (NINDS-AIREN), International Classification of Diseases (ICD), or Diagnostic and Statistical Manual of Mental Disorders (DSM) criteria. For MCI, eligible studies are those that attempted state a definition for diagnoses of MCI (e.g. an adapted version of the Petersen criteria [@petersen1999]) and create ordinal groups of patients (e.g. no dementia or dementia/MCI/dementia) based on this definition.

No limitations will be imposed on publication status, publication date, venue or language, although we will require sufficiently detailed reports of the studies to be able to examine their methods. Preprints and unpublished reports will be eligible for inclusion if relevant. Multiple publications resulting from the analysis of the same data will be included and grouped.

#### Exclusion criteria

Case-control studies, cross-sectional studies, qualitative studies, case reports/series and narrative reviews will be excluded. Studies which present no evidence of attempting to exclude prevalent cases from their analyses will also be excluded. Studies that measure change in continuous cognitive measures (e.g. MoCA score) without attempt to map these scores to ordinal groups (e.g. no dementia/MCI/dementia) will be excluded. Conference abstracts with no corresponding full-text publication will be examined, and we will contact authors to obtain information on the study's status. Studies that are reported in insufficient detail (e.g. only in conference abstracts, new, letters, editorials and opinion) will be excluded. Previous systematic reviews will not be eligible, but their reference lists will be screened to identify any potentially relevant articles. Studies with outcomes not directly related to the clinical syndrome of dementia (e.g., neuroimaging), studies implementing a "multi-domain intervention" where the lipid-regulating agent is included in each arms (e.g. for example, a study examining exercise + statins vs statins alone, but a study examining exercise + statins vs exercise alone would be included), and studies where there was no screening for dementia at baseline except if the sample was initially assessed in mid-life (i.e. below the age of 50) will be excluded.

Excluded studies performing autopsy unless it was done under accepted criteria Exclude studies using a dietary intervention, for example omega-3 fatty acid enriched diet, as it hard to disentangle the effect of other elements contained within te diet, vs simple tablet based supplements of

<!-- TODO Expand on rationale for exclusion of cross-sectional studies. Dementia is a disease severly at risk of differential recall, leading to misclassification bias. By using prospective studies, the potential for this bias is reduced. -->

<!----------------------------------------------------------------------------->
&nbsp;

#### Assessment of inter-rater reliability

Inter- and intra-rater reliability was assessed for a 10% sub-sample of records at the title and abstract screening stage. Intra-rater reliability involved a single reviewer applying the inclusion criteria to the same set of records while blinded to their previous decisions, while inter-rater reliability involved two reviewers independently screening the same set of records.

If this approach demonstrates a significant level of erroneous exclusion by the primary author a larger proportion will be dual-screened. 

Rater reliability was assessed using Gwet's agreement coefficient (AC1).[@gwet2008] This measure of inter-rater reliability was chosen over other methods of assessing inter-rater reliability such as percent agreement (number of agreements divided by total number of assessments) as i account for chance agreement between reviewers but does not suffer from severely imbalanced marginal totals in the same way that Cohen's kappa value does. [@cohen1960: @gwet2008; @wongpakaran2013]

Gwet's AC1 is defined as

$$AC1 = \frac{observed\;agreement-chance\;agreement}{1-chance\;agreement}$$ 

In reference to a two-by-two table with cells A, B, C and D, it is calculated using the following:

$$AC1 = \frac{\frac{A+D}{N}-e(\gamma)}{1-e(\gamma)}$$ 

where $e(\gamma)$ is the chance agreement between raters, given as $2q(1-q)$, where 

$$q = \frac{(A+C)+(A+B)}{2N}$$

<!-- TODO Need to be sure of how to calculate. -->

How to interpret agreement co-efficients is widely debated. Here we use guidelines based on a stricter interpretation of the Cohen's Kappa coefficient,[@mchugh2012] presented in Table \@ref(tab:gwet-table).

&nbsp;

<!----------------------------------------------------------------------------->
(ref:gwet-caption) Suggested ranges to aid in interpretation of Gwet's AC1 inter-rater reliability metric

(ref:gwet-scaption) Ranges for Gwet's AC1

```{r gwet-table, message=FALSE, results="asis", echo = FALSE}
```
<!----------------------------------------------------------------------------->

&nbsp;

### Data extraction

Harmonization of cholesterol measures across studies was performed, as different studies used different methods to quantify exposure, including comparing differing risks in the highest vs lowest quartiles of a lipid, using a binary classification of patients into a hypercholesterolaemia or not, categorizing lipid levels into high, middle, and low groups according to study-defined criteria, and simply treating the exposure as a continuous variable.


Following guidance for a dose response meta-analysis, we picked midpoint for all studies where bands were provided (extremely common in the case of lipids)

This analysis was unusual, as multiple methods of integration and analysis were required with data spanning many forms.


A piloted data extraction for was used. I extracted the data in the first instance and subset was checked by a colleague for accuracy.


<!-- TODO Cross reference with the data extraction issues here - i.e. not providing qratile cut-off values, not poviding numbers per group, etc. -->

### Risk of bias assessment {#risk-of-bias}

Given the focus on triangulating different sources of evidence at risk of a diverse range of bias, a detailed and structured risk of bias assessment formed an important part of this review.

There has been a recent moved from 

Talk about move from checklist tools to more domain based methods

Many reviews use reporting guidelines instead - this is bad![@costa2011]

Described here how tools work and how you get an overall judgement -. algorithm

<!----------------------------------------------------------------------------->

#### Randomised controlled trials

Risk of bias assessment was performed using the domain-based risk-of-bias assessment tool appropriate to the study design. Randomized controlled trials were assessed using the RoB2 tool,[@sterne2019], 
The tool assess the risk of bias across five domains: Bias arising from the randomization process, Bias due to deviations from intended intervention, Bias due to missing outcome data, Bias in measurement of the outcome, Bias in selection of the reported result. 

<!----------------------------------------------------------------------------->
#### Non-randomised studies of interventions/exposures

For non-randomised studies of interventions, I used the ROBINS-I (Risk of bias in non-randomised studies - of interventions) <!-- TODO Capitalise appropriately --> tool.[@sterne2016] This tool assess the risk of bias across seven domains: Bias due to confounding, Bias due to selection of participants, Bias in classification of interventions, Bias due to deviations from intended interventions, Bias due to missing data, Bias in measurement of outcomes, and Bias in selection of the reported result. Similar to the RoB 2, it has a number of prompting questions per domain, with judgements includeing. Ideally, obs studies assesed in reference to an idealised randomised controlled trial. Futher, the rare overall judgement of Low should be considered equivalent to a randomised controlled trial.

For non-randomised studies o
I opted to use the ROBINS-E tool, despite it still being in development,[@morganr2020] in place of other existing published tools such as the Newcastle-Ottowa scale (NOS).[@wells2000newcastle] There are two primary reasons for this: firstly, current thinking in assessing internal validity has moved away from scales.

non-randomized studies of interventions were assessed using the ROBINS-I tool, and non-randomized studies of exposures were assessed using the ROBINS-E tool.[CITE]

<!-- TODO Cite release version of this tool, and just use domain based approach -->

Secondly, as mentioned in the introduction to this section, using a domain based to assess the risk of bias in studies of exposure allowed for better comparison and consistency across the tool used for different study designs, as they are all domain based.

<!----------------------------------------------------------------------------->

#### Mendelian randomisation studies

<!-- TODO This next bit is not all true -->

At present, no risk of bias assessment tool for Mendelian randomization studies is available. Assessment of the risk of bias in Mendelian randomisation studies was informed by the approach used in a previous systematic review of Mendelian randomisation,[@mamluk2020] as identified by a review of risk of bias assessments in systematic reviews of MR studies. A copy of this tool is available in Appendix \@ref(appendix-mr-rob). Advance results from this review of bias assessment in MR studies were obtained from contact with the authors.

<!-- TODO Cite MR tool here from malmuk, see info from Francesca -->


<!----------------------------------------------------------------------------->

### Studification

In the advent of multiple papers reporting results on the same cohort, but say, at different time points, I used a process of studifiction to build out the most comprehensive accounts of the studies and results from as many published articles were applicable. 

<!----------------------------------------------------------------------------->

### Following up with authors

As part of the IPD analysis, relevant cohort studies were identified and approached. In the first instance, the corresponding author on the main man

<!-- TODO move to results, and keep just a short bit here -->

<!----------------------------------------------------------------------------->


### Statistical analysis methods

<!----------------------------------------------------------------------------->

Meta-analysis is an important method of . . . 

Need formula here, and description of different methods and quarrels re fixed effect vs random effects meta-analyses.

This anal

Will also need description of dose-response meta-analysis here, with reference to forthcoming book Julian sent on.

### Visualisation of results

Given the importance of visualising the potential biases alongside the results of any given study, a new visualisation tool was designed to allow for "paired" forest plots (as recommended by the ROB2 publication)

This tool grew out of a collaboration with other reserachers to design a modular method for creating these plots <!-- Apparently collaboration is a big thing they want to see -->

A summary of this tool is contained in Appendix \@ref(appendix-robvis), and all forest plots presented in this analysis were created using

Results from the risk-of-bias assessment were visualized using a paired forest/risk-of-bias blobbogram, created using the `robvis` tool. This tool was developed as an adjunct to this thesis to aid in creating standard risk-of-bias figures.[@mcguinness2019] A summary of the tool is provided in Appendix \@ref(appendix-robvis).

. 

### Assessment of added value of including preprint

Preprints are a valuable evidence source (see Introduction) but their inclusion in a systematic reivew

Additionally, I followed preprints up over time to investigate whether all identified preprints included in the review were subsequently published (in which case preprints give you a look into the future, and a systematic review update would capture these reports) or alternatively, if some preprints were not published, then preprints provide a different. 

I also sought to explore the additional evidental value of including preprints in the meta-analysis, assessed using the fixed effect weight from a standard meta-analysis.

<!----------------------------------------------------------------------------->

### Patient and public involvement

<!----------------------------------------------------------------------------->

## Results

<!----------------------------------------------------------------------------->

### Search results

Detail 

### Previous reviews

<!-- TODO Not sure this is the best place for this -->


**Will need to include something in the methods section on this**

As part of this analysis, we examined the overlap between different published reviews on this topic.

Several primary studies were captured in multiple systematic reviews. However, some studies were not captured by any previous review,

Put upSet plot here.

An upset plot shows the total size of each set (in this case, the set of included studies in each review) in the bottom left hand bar plot. For example, the XXXX review includes XXXX studies.

Where a line joins two points in the matrix, the main barchart shows the number of records shared by these reviews. So for example, XXXX records were captured both by the XXXX review and the XXXX review.

Where the matrix has just a single point highlighted, this shows the records unique to that review.

The reviews are ordered by date, to attempt to show the increasing overlap and total number of studies

Our review is presented as the last point on this plot, and captured XXXX records

The alternative is to have a barplot showing the total cumulative number of records over the years, using our set as the master, with the bars coloured by the number of records included in 1/2/3/4 reviews. The information below the bar axis could then show the number of reviews published in that year, or alternative, you could have an upsidedown bar to show cumulative number of reviews on this topic.

<!--- Take a stand here! --->

Of note, this review did not include the commonly cited PROSPER study, which examined the effect of pravastatin on CVD risk and putatively provides no evidence for an effect of the statin on dementia outcomes. While widely cited and included in the Cochrane review on this topic, it has not been included here, for the reason that it only reported the change from cognitive score (as measured using the [CHECK] scale) over a mean follow-up. While this is a useful indicator of general cognitive decline, it is not equivalent to a dementia outcome, as cogntive scales should feed into a broader diagnostic pathway - See section \@ref(diagnostic-criteria)

This does introduce some conflicts with other studies where a change score has been dichotomised in order to have a binary dementia or not dementia outcome.

Does it also cause issues in terms of self-report?

<!----------------------------------------------------------------------------->

### Screening results



Following screening, XXX studies were included.

The distribution of included studies over time demonstrates that despite the conduct of several previous reviews of different types of literature surrounding this question, primary studies continue to the published as these reviews have yet to provide a definite answer.

Table \@ref(tab:incl-studies-character) shows the characteristics [**Include column here that says whether it was included in a systematic review - see below**]

As part of our our forward snowballing exercise (where articles citing an included study are cited), we recorded whether a study included in our review had been included in any previous evidence synthesis attempt in an attempt to qualify the added value of this analysis. Additionally, if an included article was subsequently cited by a review, all studies in that review were screened for inclusion for the sake of completeness. This analysis was performed by extracting the citing articles from Google Scholar on [DATE] and screening them manually. The DOI of articles extracted from this analysis are included in the appendix, as the Google Scholar search functionality is not readily reproducible.*

<!-- For the snowballing exercise - make sure you don't include studies published after review date: ~ June 2019 -->

As a summary of the duplication of work in this area, we looked at how many reviews a single included study had previously been included in.

<!--- TODO Need example table here, showing relevant cells --->

<!-- # ```{r agreementtableinter, echo = FALSE} -->

<!-- # ``` -->

<!-- # ```{r agreementtableintra, echo = FALSE} -->

<!-- # ``` -->

For the inter-rater reliability, percentage agreement was 97.3% (AC1 = XXXX, Table \@ref(tab:agreement-table-intra)), while for the intra-rater reliability, agreement was 98.6% (AC1 = XXXX, Table \@ref(tab:agreement-table-intra)).

The discrepancy between the percent agreement and the associated value of AC is expected, due to the heavy imbalance in this sample towards exclusion.[@feinstein1990]

Those records which were excluded in the initial screening, but were included either by the same reviewer on their second viewing (n=4), or by the second reviewer (n=29), were investigated. This discrepancy between the two reviewers was explained in all cases by differing interpretations of the inclusion criteria, specifically around the definition of cognitive decline versus mild cognitive impairment, and the definition of eligible lipids fractions.

<!--- Cross check this with the actual numbers included --->

<!----------------------------------------------------------------------------->



<!-- TODO This needs to use the PRISMA2020 package -->

```{r prisma-flow-setup, include = FALSE}
```

(ref:prisma-flow-cap) **PRISMA Flowchart:** Overview of how records moved through the systematic review process

(ref:prisma-flow-scap) PRISMA Flow Diagram

```{r prisma-flow-fig, echo = FALSE, results="asis", fig.cap='(ref:prisma-flow-cap)', out.width='100%', fig.scap='(ref:prisma-flow-scap)'}
knitr::include_graphics(file.path("figures/sys-rev/prismaflow.png"))
```

<!----------------------------------------------------------------------------->

Following de-duplication, the titles and abstracts of `r hold()` records were assessed for eligibility. `r hold()` were deemed potentially eligible and the full text records for these were requested and screened. \@ref(fig:prisma-flow-fig)

<!----------------------------------------------------------------------------->

### Characteristics of included studies

XXXX studies met the criteria for inclusion in the review.




As shown in


Include here:

-   PRISMA Flowchart

-   Summary of types of study

-   Summary of locations

-   Summary of diagnostic criteria used

-   Summary of risk of bias

-   Long table, horizontally. 


A common theme across the studies was a lack of data on vascular dementia. This is particularly interesting as lipids and statins are primarily a vascular disease. There is the potential that they studies encountered similar difficulties in address the unexpected results observed in the CPRD analysis in Chapter \@ref(cprd-analysis-heading). For vascular dementia, few studies exmined this outcome, primarily because of the absence (until recently) of

One item of particular interest in the absence of studies examining Mendelian randomisation studies is the absence of any effect following the adjustment for Apoe4. As covered in the discussion, ApoE4 genotype is the mjor risk factor for Alzheimer's disease.

In addition, there was quite a spread in terms fo the effect estimates used to th. Previous systematic review

### Converting risk ratios to odds ratios

Include formulae and informed assumptions 


### Hazard ratios vs risk/odds ratios

Not interchangeable in the context of systematic review. If outcome is rare, odds and risk ratios approximate each other, but the hazard ratio is measuring something completely different, by taking into account time-to-event in each treatment group. 

Several existing reviews do not distinguish between the two types of effect measures and include all existing studies in a single meta-analysis to produce an overall effect. 

The likely effect of this is that the overall effect measures are biased towards . . . . ?

Can you predict the direction of effect from a 

Best practice methods for dealing with disparate effect measures in included studies is to not perform a meta-analysis 

<!-- See the third row of the table here; https://training.cochrane.org/handbook/current/chapter-12#section-12-1 -->

Some evidence of manipulation of effect estimates in previous reviews,(e.g. Chou, Sci Reports - at least one study disagrees with) but not documented in review text.

### Risk of bias {#risk-of-bias-subheading}

### Sources of heterogeneity

Detail that some of these are exploratory, in particular the effect of different scales on the association between the groups. **[CROSS]**

### Publication bias {#sys-rev-pub-bias}

-   Check if there are protocols available for any of the published reports (unlikely for non-randomised controlled trials), and whether there were
-   Vascular dementia has substantially less published reports. Many (reference Smeeth et al 2010 here) simply group into AD and non-AD making comparison between published studies difficult

One particularly interesting meta-bias potentially applicable in this review is the definition of code lists across

Many studies did not report the codelists used to run the analysis, 



### Triangualtion across evidence sources

One key question for which multiple distinct sources of evidence were available were those looking at Laz

<!-- TODO Have a look at this -->


A key limitation for other types of dementia, in particularly vascular, is that there has yet to be a GWAS identifying relevant SNPS that could then be used in a MR study with SNPS for lipids to estimates the causal effect of lipids on vascular dementia [**Is this true?**] This rules out the use. In addition, there was limited literature available - as discussed in CHapter \@ref(sys-rev)

Consideration of the potential impact of the magnitude and direct of residual confounders/bias is not a major stretch from what is already happening in the assessment of the quality of evidence (GRADE) framework. Within GRADE, the overall quality of evidence can be upgraded when there is deemed to be unmeasured or residual confounding variables which reduce the. For example, if the propensity to treatment is related to comorbidity burden, but those on treatment still have better outcomes then those on control, it is likely that the true effect of the intervention is being underestimated. [@guyatt2011]

Without our framework and as part of the risk of bias assessments reported in \@ref(risk-of-bias-subheading), we test test

### Added evidental value of including preprints {}

<!-- TODO Will have to get actual numbers for this section -->

There were several relevant preprints captured by the search run using the tool presented in Chapter \@ref(sys-rev-tools-heading) (see Appendix \@ref(appendix-medrxivr-code)). While many of the preprints included were subsequently published by the time of submission of this thesis, at the time of the search they were only available 

Good example is [https://doi.org/10.1101/168674](https://doi.org/10.1101/168674), which was subsequently published in Nature Comms but in 2020, following peer-review (found no impact of LDL-c on AD following removal of APOE4).


Of interest, 

<!-- TODO Triple check whether any of the studies have yet to be published. Chose a given date and go from there -->

This likely reflects a bias in the type of study that is submitted, but indicates the additional evidential value of include preprints in the search strategy. It also means that searches are current - if the aim of systematic reviews is to

While the total number of relevant studies found using this method was

<!-- Can I explore the amount of information added by the preprints quantiatively using the weights in the meta-analysis -->

<!-- Add bit on comparison of preprints and published papers, demonstrating the preprints are a good reflection of -->

On further investigation, the argument that . 

It is not so much that they provide a source of unpublished grey-literature, more so that they provide a "future" look at . 

However, if the aim is to find the current state of the art in the topic area at the time of searching, inclusion of preprints is a requirement. 

While concerns have been raised that 

Previous meta-studies have found high concordance between the main interpretations and other study design details in preprint-journal article pairs,[@shi2021] though quality of reporting was slightly better in formally published articles.[@carneiro2020]

A common criticism of preprinted articles is that they have not formally been peer-reviewed. __[CITATION NEEDED]__ However, I believe this criticism is less warranted in the context of inclusion of preprints in a systematic review, given a formal assessment of risk of bias, which provides a structured approach to assessing the internal validity of a study. 

> Anyone who can't assess a study to the level of peer review shouldn't do a systematic review

References included in the review were also searched for on PubPeer, a platform which enables post-publication peer review of published journal articles.[@hunter2012]

The added value of peer-review to papers that is questionable and difficult to quantify for a number of reasons, including the fact that most peer reviews are closed

While [__some__] preprint have subsequently been published, this does not cause any major issues. 

## Discussion

Major limitation is that several included studies used data from electronic health databases, which come with serious concerns regarding validity [@hsieh2019] __[ALSO CITE WILKINSON AND MCGUINNESS HERE]__.

In addition, the a


###

### Comments on the process

As part of the reflective element of this thesis, 

Systematic reviews should not be performed as part of a thesis, without suitable support and resourcing guaranteed. Assumption that everyone does a systematic review (without risk of bias assessments, inclusion of all literature, searching for other reviews) is foolish.

Average time to complete a s

However, new developments such as automated screening would allow for a reduced need for personnel to work on these things. <!-- TODO CITATION NEEDED - Cite Triccios new paper here -->


Great learning experience in terms of managing a team, but due to a rotating schedule of people, much harder to do that expected.

A final point on the progress of the review is that, due to the need for dual screening and data extraction, a number of external researchers became involved in this review. I found the people-management aspect particularly challenging and could definitely have improved the process through better communication of deadlines, but it has provided good experience of leading a review team.



### Inclusion of preprints

As highlighted in Section \@ref(diverse-sources-preprints), this thesis sought to syntheze evidence across different publication statuses, predominantly preprinted. Using the tool described in Chapter \@ref(), two preprint serves related to health and biomedical sciences were search as part of this review. 

The added evidental value of including these preprints was highlighted in Section 

On further investigation, X of these preprints were subsequently published since the search was performed in 2019. While this provides some additional evidence that the quality of the research desribed by included preprints was sufficiently high to pass peer-review, it does not invalidate the approach, as the future publication status was unavaialble at the time the search was conducted.



### Open data sharing {#sys-rev-open-data}

<!--- How many of the included studies accurately reported the diagnostic criteria for EHR? Cross link with section in CPRD analysis --->

<!--- Comment here on issues involved in extracting data --->


### Original contribution to research

I believe there are three aspects where this review is distinct from those reviews already available in the published literature (as identified by __[CITATION NEEDED]__):

-   *Comprehensiveness:* While several reviews of this research topic exist,[@chu2018b; @yang2020; @muangpaisan2010; @poly2020b] the overlap between the list of studies included in each is not 100%. As part of this review, I have not only performed a original search of primary literature databases, but have also screened the reference lists of comparable reviews to ensure no study has been omitted.
-   *Structured risk of bias assessment:* The majority of the highly cited reviews on this topic either do not formally consider the risk of bias in the observational studies they include or do not use an appropriate domain-based assessment tool (e.g. ROBINS-I/E). This is important area in which this thesis can add value, as based on the risk-of-bias assessments I have performed to date, several primary studies are at high risk of bias and this should be reflected in the findings of any review on this topic.
-   *Inclusion of preprints:* Unlike other available reviews and enabled by the tool described in Chapter 3, this review systematically searched preprinted health-related manuscripts as a source of grey literature. As part of this chapter, I plan to examine the extent of the additional information provided to the review by the inclusion of preprints.

### Comparison with other reviews

Of note, as part of the review, we identified several previous systematic reviews of this topic.[CITE] However, this review is the first to use established domain based assessments tools (for example, the RoB 2 tool for randomized controlled trials)[@sterne2019] to assess the risk of bias in included studies, and explore the heterogeneity of results across different levels of risk of bias levels.

Some previous reviews did assess risk of bias, but used non-domain based assessment tools. Newcastle-Ottowa scale,

**The duplication of work across reviews (including, ironically, by this review) is substantial. In Section XXXX, I demonstrated that the each primary study included in this review was also included in other review so this topic, but that not all studies were included in all reviews. However, by creating a comprehensive review that attempted to draw together all available evidence from across the full range of study types.**

__Exclusion of the PROSPER trial__ Will need to have a good bit here on the exclusion of this RCT, particularly as it is included in the Cochrane review on the topic. Cross-reference with the meta-bias section in the early section, on the impact of only included studies that sought to make a definitive diagnosis, rather than using change in cognitive assessment scales.



<!-- Will hopefully be able to cite MSc review here to give details of other reviews -->

### Reviewing Mendelian randomisations studies

While

This may be because Mendelian randomisation studies have yet to reach a critical mass in terms of requiring a systematic review. 

Recent updates, such as a guide to reading and interpreting. However, this guide includes reporting items in their quality checklist - reporting quality while important, is unrelated to internal validity.

Problems with overlapping samples in reviews of Mendelian randomisation studies in that we may be double counting participants - for

Methods for reviews of Mendelian randomisation studies are not well developed to account for the consideration above.

### Strenghts and limitations

#### Inclusion of Mendelian randomisation studies

As a form of analysis that lends itself to multiple comparison, there is a strong

Through snowballing and other measures, we identified at least one Mendelian randomisation studi [@larsson2017b]

Is it a big issue that the studies have the same underlying datasets? <!--- see discussion from Matt Lee thesis, in email --->


Potentially missing other Mendelian randomisation studies, as we identified some through our snowball searching that were not captured by the search strategy. An example is Larsson et al. 2017[@larsson2017a], where the study examined the association between lipid fractions with Azlheimer's disease.

Potentially the use of methods such as snowballing (forwards and backwards citation chasing) should attenuate the bias introduced by this method. 

However, in future reviews, a dedicated search for "risk factors" and "dementia" and "


#### Inclusion of preprints

Comment on the fact that several of the studies identified through the search of preprints, were subsequently published, but much later than the
cut-off point for the search (e.g. they would not have been captured by the search for published papers)

<!--- Findings from this chapter are used throughout the later chapters of this thesis. In particular, Chapter 6 uses statistical modelling to exploring the impact of BCG vaccination on TB outcomes in greater detail, Chapter 7 explores the impact of the change in BCG vaccination policy on TB incidence rates using the incidence rate estimates from this chapter, Chapter 8 uses the understanding of the ETS gained from this chapter to parameterise a dynamic TB transmission model, and Chapter 9 uses the insights gained into the date variables in the ETS to fit a dynamic TB transmission model. From Sam abbotts--->

### Use in other chapters

Findings from this review are used though out the subsequent chapters. 

For example, 

## References
