---
bibliography: bibliography/references.bib
csl: bibliography/nature.csl
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
  bookdown::word_document2: 
      toc: false
      toc_depth: 3
      reference_docx: templates/word-styles-reference-01.docx
      number_sections: false 
  bookdown::html_document2: default
documentclass: book
---
```{block type='savequote', include=knitr::is_latex_output(),quote_author='(ref:sys-rev-quote)', echo = FALSE}
"It is surely a great criticism of our profession that we have not organised a critical summary by speciality or sub-speciality, up-dated periodically, of all relevant RCTS."  
```

(ref:sys-rev-quote) --- Archibald Cochrane, 2000 [@cochrane1979]

# Systematic review of all available evidence {#sys-rev-heading}

\minitoc <!-- this will include a mini table of contents-->

```{r, echo = FALSE, warning=FALSE, message=FALSE}
source("R/doc_options.R")
source("R/helper.R")
knitr::read_chunk("R/03-Code-Systematic-Review.R")
doc_type <- knitr::opts_knit$get('rmarkdown.pandoc.to') # Info on knitting format
```

<!-- ## To Do -->

<!-- -   TODO Look at whether searching preprints made any difference to my results -->

<!-- TODO Extra studies to include: -->
<!-- * Smeet et al. reference in the CPRD analysis -->
<!-- * Larsson - 10.1136/bmj.j5375 [@larsson2017b] -->
<!-- * Statins and the risk of dementia Jick -->

## Lay summary

Systematic reviews are a type of research that aim to use all existing evidence to provide the best answer to an important research question. They do this by finding and combining the results from many related primary research studies. Reviews involve multiple steps including: searching of existing studies; assessment of the studies against predefined inclusion criteria; collection of data from each study; assessment of each study's methods. 

This chapter presents a systematic review of primary studies that have examined the relationship between the levels of blood lipids (such as cholesterol and triglycerides), and treatments that change these levels, and dementia.

There were 127 primary studies <!-- TODO check this -->that contained information on this relationship. I found that statins reduce the risk of Alzheimer's disease, but had no effect of vascular dementia. Lipids were not associated with any outcome. The methods used in some of the primary studies meant that I was less confident in the accuracy of their results.

The use of the results of this review in subsequent chapters is dicussed.

&nbsp;

<!----------------------------------------------------------------------------->
## Introduction {#sys-rev-intro}

In this chapter, I describe a comprehensive systematic review of the relationship between blood lipid levels, and treatments that modify them, and the subsequent risk of dementia and related outcomes. This analysis sought to address two specific aims.

Firstly, as discussed in the Introduction to this thesis (Section \@ref(evidence-association)), several diverse forms of evidence on the relationship of lipids and dementia exist. These include randomised controlled trials, observational studies of different analytical design (cohort, case-control, etc), and Mendelian randomisation studies. However, based on a scoping review of existing literature, no previous evidence synthesis exercise has attempted to examine the association of lipids/statins with dementia outcomes across these distinct evidence types. Collating these diverse evidence sources is important, as if the observed association between lipids and dementia is constant across them, it increases our confidence in the association. As such, the primary aim of this analysis was to systematically review all available literature, regardless of study design.

Secondly, I explicitly sought to include health-related preprint servers as a potential evidence source in this review, as they are infrequently considered by evidence syntheists but may contain important unpublished studies. As an adjunct analysis to the systematic review presented in this chapter, I sought to quantify the additional evidential value of including preprints. This inclusion of preprint serves makes use of the preprint search tool presented in Chapter \@ref(sys-rev-tools-heading).

The results of this review are used to guide the primary analysis presented in Chapter \@ref(cprd-analysis-heading), in addition to forming a key evidence source used in the triangulation exercise presented in Chapter \@ref(discussion-heading).

&nbsp;
<!----------------------------------------------------------------------------->

## Methods

### Protocol

A pre-specified protocol for this analysis was registered using the Open Science Framework, and is available for inspection.[@mcguinnessluke2020] Deviations from this protocol are detailed in the relevant sections.

<!----------------------------------------------------------------------------->
&nbsp;

### Contributions

In line with best-practice guidance, secondary reviewers were used to check the accuracy of screening, data extraction and risk-of-bias assessment processes. Due to the scale of the project, this systematic review was performed in conjunction with a team of secondary reviewer. These included Alexandra MacAleenan, Athena Sheppard, Matthew Lee and Lena Schmidt. In addition, Sarah Dawson, and information specialist, provided input to the design of the search strategy.

<!-- TODO Double check this list at the end of the project. -->

<!----------------------------------------------------------------------------->
&nbsp;

### Search strategy

I systematically search electronic bibliographic databases to identify potentially relevant entries (hereafter referred to as "records"). The search strategy used in each database was developed in an iterative manner using a combination of free text and controlled vocabulary (MeSH/EMTREE)[@lefebvre2019searching] terms to identify studies which have examined the relationship between blood lipids levels and dementia, incorporating input from an information specialist. The strategy included terms related to lipids, lipid modifying treatments, and dementia and its sub-types, and was designed for MEDLINE before being adapted for use in the other bibliography databases listed. An outline of the general strategy is presented in the Table \@ref(tab:searchOverview-table) below and the full search strategies for each database are presented in Appendix \@ref(appendix-search-strategy). <!-- TODO Need to actually attach each search strategy. Should be able to loop through search strategy results. -->

&nbsp;

<!----------------------------------------------------------------------------->
(ref:searchOverview-caption) Summary of systematic search by topic. The full search strategy including all terms and the number of hits per term is included in Appendix \@ref(appendix-search-strategy).

(ref:searchOverview-scaption) searchOverview

```{r searchOverview-table, message=FALSE, results="asis", echo = FALSE}
```
<!----------------------------------------------------------------------------->

&nbsp;

When searching the bibliographic databases, study design filters were employed to try and reduce the screening load. To ensure that the study design filters are not excluding potentially relevant records, a random sample of 500 records identified by the main search but excluded by the filters (defined as "8 NOT 12" in Table \@ref(tab:searchOverview-table)) was screened.

<!-- TODO need to comment on this feed-back process, or remove -->

The following databases were searched from inception onwards: Medline, EMBASE, Psychinfo, Cochrane Central Register of Controlled Trials (CENTRAL), and Web of Science Core Collection. As the contents of the Web of Science Core Collection can vary by institution, the specific database searched via this platform are listed in Appendix \@ref(appendix-wos-databases). I also search clinical trial registries, for example ClinicalTrials.gov, to identify relevant randomized controlled trials.

In addition, I searched the bioRxiv and medRxiv preprint repositories using the tool developed in Chapter \@ref(sys-rev-tools-heading) to identify potentially relevant preprinted studies.

Grey literature was also searched via ProQuest, OpenGrey and Web of Science Conference Proceedings Citation Index, while theses were accessed using the Open Access Theses and Dissertations portal. In addition, the abstracts list of relevant conferences (e.g. the proceedings of the Alzheimer's Association International Conference, published in the journal Alzheimer's & Dementia) were searched by hand. <!-- TODO how were these searched? Date and other range --> Finally, the reference lists of included studies were searched by hand while studies citing included studies was examined using Google Scholar (forward and reverse citation searching or "snowballing"<!-- TODO CITATION NEEDED -->).

<!-- TODO Look at citationchaser for this, but have a cut-off publication date of July 2019 - better than searching by hand/Google Scholar -->

&nbsp;
<!----------------------------------------------------------------------------->

### Study selection

Records were imported into Endnote and de-duplicated using the method outlined in Bramer et al. (2016).[@bramer2016] In summary, this method uses multiple stages to identify potential duplicates, beginning with automatic deletion of records matching on multiple fields ("Author" + "Year" + "Title" + "Journal"), followed by manual review of less similar articles (e.g. those matched based on the "Title" field alone).

Following deduplication of records, screening (both title/abstract and full-text) was performed using a combination of Endnote, a citation management tool,[@hupe2019] and Rayyan, a web-based screening application.[@ouzzani2016] Title and abstract screening to remove obviously irrelevant records was performed primarily by me, with a random sample of excluded records being screened in duplicate to ensure consistency with the inclusion criteria.

Similarly, I completed all full-text screening, with a random ~10% being screened in duplicate by a second reviewer. In addition, any records identified I identified as being difficult to assess against the inclusion criteria were screened in duplicate. Reasons for exclusion at this stage were recorded. Disagreements occurring during either stage of the screening process were resolved through discussion with a senior colleague. A PRIMSA flow diagram was produced to document how records moved through the review.[@page2021]

The criteria used to assess eligibility are presented in the subsequent sections.

<!----------------------------------------------------------------------------->
&nbsp;

#### Inclusion criteria

I sought to include studies that examine the relationship between blood lipid levels (or any specific lipid fraction, including total cholesterol, HDL, LDL, and triglycerides) and risk of incident dementia/MCI. <!-- TODO Do I actually want to include MCI? --> Eligible study designs include randomized controlled trials and non-randomized observational studies of lipid modifying treatments, longitudinal studies examining the effect of increased/decreased blood lipid levels, and genetic instrumental variable (Mendelian randomization) studies examining the effect of genetically increased/decreased blood lipid levels.

Participants were free (or assumed to be free) of dementia/MCI <!-- TODO Do I actually want to include MCI? -->  at baseline. Studies of any duration were included to allow for exploration of the effect of length of follow-up on the effect estimate using meta-regression. No limits were placed on the sample size of included studies.

Eligible studies defined dementia according to recognised criteria, for example the National Institute of Neurological Disorders and Stroke Association-Internationale pour la Recherche en l'Enseignement en Neurosciences (NINDS-AIREN), International Classification of Diseases (ICD), or Diagnostic and Statistical Manual of Mental Disorders (DSM) criteria. <!-- TODO CITATION NEEDED --> <!-- TODO do i actually want to include MCI? --> For MCI, eligible studies are those that attempted state a definition for diagnoses of MCI (e.g. an adapted version of the Petersen criteria [@petersen1999]) and create ordinal groups of patients (e.g. no dementia or dementia/MCI/dementia) based on this definition.

No limitations were imposed on publication status, publication date, venue or language, although sufficiently detailed reports of the studies to be able to examine their methods were required for inclusion.

<!----------------------------------------------------------------------------->
&nbsp;

#### Exclusion criteria

Case-control studies, cross-sectional studies, qualitative studies, case reports/series and narrative reviews were excluded. Studies which present no evidence of attempting to exclude prevalent cases from their analyses were also excluded. Studies that measure change in continuous cognitive measures (e.g. MoCA score) without attempt to map these scores to ordinal groups (e.g. no dementia/MCI/dementia) were excluded. Conference abstracts with no corresponding full-text publication were examined, and I contacted authors to obtain information on the study's status. Studies that are reported in insufficient detail (e.g. only in conference abstracts, new, letters, editorials and opinion) were excluded. Previous systematic reviews were not eligible for inclusion, but their reference lists were screened to identify any potentially relevant articles. 
<!-- TODO Expand on rationale for exclusion of cross-sectional studies. Dementia is a disease severly at risk of differential recall, leading to misclassification bias. By using prospective studies, the potential for this bias is reduced. -->

Studies with outcomes not directly related to the clinical syndrome of dementia (e.g., neuroimaging), studies implementing a "multi-domain intervention" where the lipid-regulating agent is included in each arms (e.g. for example, a study examining exercise + statins vs statins alone, but a study examining exercise + statins vs exercise alone would be included), and studies where there was no screening for dementia at baseline except if the sample was initially assessed in mid-life (i.e. below the age of 50) were excluded. Finally, studies using a dietary intervention, for example omega-3 fatty acid enriched diet, were excluded as it is difficult to disentangle the effect of other elements contained within the diet. Note, this is distinct from studies which delivered a simple tablet-based omega-3 intervention, which would have been eligible for inclusion.

<!----------------------------------------------------------------------------->
&nbsp;

### Validation of screening process

Inter- and intra-rater reliability during the screening stage were assessed for a 10% sub-sample of records at the title and abstract screening stage. Intra-rater reliability involved a single reviewer applying the inclusion criteria to the same set of records while blinded to their previous decisions (i.e. assessment of conistency), while inter-rater reliability involved two reviewers independently screening the same set of records (i.e. assessment of accuracy).

Rater reliability was assessed using Gwet's agreement coefficient (AC1).[@gwet2008] This measure of inter-rater reliability was chosen over other methods of assessing inter-rater reliability such as percent agreement (number of agreements divided by total number of assessments), as it accounts for chance agreement between reviewers but does not suffer from bias due to severely imbalanced marginal totals in the same way that Cohen's $kappa$ value does. [@cohen1960: @gwet2008; @wongpakaran2013] Given the small number of included studies in this review as a proportion of the total number screened, this is an important characteristic.

Gwet's AC1 is defined as:

$$AC1 = \frac{observed\;agreement-chance\;agreement}{1-chance\;agreement}$$ 

In reference to a two-by-two table with cells A, B, C and D, it is calculated using the following:

\begin{equation}
  AC1 = \frac{\frac{A+D}{N}-e(\gamma)}{1-e(\gamma)}
  (\#eq:AC1-main)
\end{equation}

where $e(\gamma)$ is the chance agreement between raters, given as $2q(1-q)$, where 

\begin{equation}
  q = \frac{(A+C)+(A+B)}{2N}
  (\#eq:AC1-supp)
\end{equation}

<!-- TODO Need to be sure of how to calculate. -->

How to interpret agreement co-efficients is widely debated, and while arbitary cut-off values may mislead,[@brennan1992] they provide a useful rubric by which to assess inter-rater agreement. Here, I used guidelines based on a stricter interpretation of the Cohen's $kappa$ coefficient,[@mchugh2012] presented in Table \@ref(tab:gwet-table).

&nbsp;

<!----------------------------------------------------------------------------->
(ref:gwet-caption) Suggested ranges to aid in interpretation of Gwet's AC1 inter-rater reliability metric

(ref:gwet-scaption) Ranges for Gwet's AC1

```{r gwet-table, message=FALSE, results="asis", echo = FALSE}
```
<!----------------------------------------------------------------------------->

&nbsp;

If this assessment demonstrated issues with the screening process, a larger proportion of records would have been dual-screened. 

&nbsp;
<!----------------------------------------------------------------------------->

### Data extraction

Data extraction was performed using a piloted data extraction form. Extracted items included: article metadata (year of publication, author list, journal), study characteristics (location, cohort name, exposure, outcomes, outcome criteria used), patient characteristics (age, sex, baseline cognition scores, baseline education scores), and results (exposure-outcome pairing, effect measure, effect estimate, error estimate, p-value). All data was extracted the data in the first instance and was checked for accuracy by a second member of the review team.

#### Harmonisation of cholesterol measures

Harmonization of cholesterol measures across studies was performed, as different studies used different methods to quantify exposure, including comparing differing risks in the highest vs lowest quartiles of a lipid, using a binary classification of patients into a hypercholesterolaemia or not, categorizing lipid levels into high, middle, and low groups according to study-defined criteria, and simply treating the exposure as a continuous variable. <!-- TODO This might not be as accurate now given the focus on dose response --> <!-- TODO Cross reference with the data extraction issues here - i.e. not providing qratile cut-off values, not poviding numbers per group, etc. --> Where necessary, lipid levels reported in _mmol/L_ were converted in _mg/dL_ using the following formula:

\begin{equation} 
  mg/dL = mmol/L \times{} Z
  (\#eq:lipidConversion)
\end{equation} 

where $Z = 38.67$ for total cholesterol, LDL-c and HDL-c, and $Z = 88.57$ for triglycerides. For widely-used categorises of lipids levels on the _mg/dL_ scale, see Table \@ref(tab:lipidLevels-table) in Section \@ref(intro-lipid-fractions). 

#### Study-fication

As part of the data extraction process, multiple resords resulting from the analysis of the same data were included and grouped into single units, hereafter called studies. This is likely in the advent of multiple papers reporting results on the same cohort, but say, at different time points. Study-fication builds out the most comprehensive accounts of the studies and results from as many published articles were applicable. 

This was particularly relevant to preprints and published papers reporting the same study, which were not considered duplicate records. Instead, they were considered as different reports of the same study. This is due to the potential for the published version to offer some information that the preprint did not, and vice versa.

<!----------------------------------------------------------------------------->
&nbsp;

#### Following up with authors {#contacting-authors}

Where data points required either for the analysis or risk-of-bias assessment but were not reported, the primary authors of the study were contacted. This approach was taken due to the potentially large impact of following up with authors on the results of the review,[@reynders2019] 

This was particularly important for the dose response meta-analysis, where the number of participants and the cut-offs per category were often not reported.

<!-- TODO Use the example of getting the cut-off points from the  -->

<!-- TODO move to results, and keep just a short bit here -->

<!----------------------------------------------------------------------------->
&nbsp;

#### Converting between different exposure measures

<!-- TODO Include formulae and informed assumptions  -->

The range of effect measures presented by studies (odds ratios, risk ratios, hazard ratios, etc) are not directly interchangeable in the context of systematic review. If the outcome is rare, odds and risk ratios approximate each other.<!-- TODO CITATION NEEDED --> However, hazard ratios provide a very different interpretation, taking into account time-to-event in each treatment group. 

Several existing reviews do not distinguish between the types of effect measures and include all existing studies in a single meta-analysis to produce an overall effect. In adddition, there is some evidence of manipulation of effect estimates in previous reviews,(e.g. Chou, Sci Reports - at least one study disagrees with) but this is not accurately documented in the review text.

<!-- TODO Need to really fix this -->


<!-- The likely effect of this is that the overall effect measures are biased towards . . . . ? -->

<!-- Can you predict the direction of effect from a  -->

<!-- Best practice methods for dealing with disparate effect measures in included studies is to not perform a meta-analysis  -->

<!-- See the third row of the table here; https://training.cochrane.org/handbook/current/chapter-12#section-12-1 -->

&nbsp;

<!----------------------------------------------------------------------------->

### Risk of bias assessment {#risk-of-bias}

A key use of the review presented in this chapter is to identify different sources of evidence at risk of a diverse range of biases, and to contrast and compare findings across them (see Section \@ref(triangulation-overview) for an overview of triangulation and Section \@ref(intro-triangulation) for the results of this qualitative analysis). To enable this triangulation exercise, a detailed and structured risk of bias assessment formed an important part of this review.

There has been a recent movement within the evidence synthesis community from examining _methodological quality_ to assessing _risk of bias_,[@mcguinness2018; @sterne2016] and thus directly evaluating the internal validity of a study. Internal validity is defined here as the absence of systematic error (or bias) in a study, which may influence its results.[@campbell1957; @juni2001] 

This move was prompted by a unclear definition of "methodological quality" which could include facets such as, and challenges in the comparison of results from different tools. As part of this, the community also moved from checklist or score based tools towards domain-based methods, in which different potential sources of bias in a study are assessed in order.

Additionally, results should be assessed at the result (defined as a a specific outcome at a specific timepoint) rather than the study level. For example, a study may report on the efficacy of an intervention at 6 months and two years follow-up. missing outcome data that is not an issues at 6 months may introduce bias at 2 year follow-up. In this case, assigning a bias judgement to the study as a whole hides this differential biases for each result.

In this review, domain-based tools were used to assess the risk of bias for each result in each included study. The study design-specific tools are introduced and discussed in more detail in the following sections. In addition, the tool also aim to capture the potential direction of bias for each result. Possible responses included: "Favours experimental", "Favours comparator", "Towards null", "Away from null", and "Unpredictable".

<!----------------------------------------------------------------------------->
&nbsp;

#### Randomised controlled trials

Risk of bias assessment in randomised controlled trials was performed using the domain-based risk-of-bias assessment tool appropriate to the study design. Randomized controlled trials were assessed using the RoB2 tool.[@sterne2019]

The tool assess the risk of bias across five domains: Bias arising from the randomization process, Bias due to deviations from intended intervention, Bias due to missing outcome data, Bias in measurement of the outcome, Bias in selection of the reported result. Acceptable judgements include: low risk of bias, some concerns, high risk of bias. Each of the 5 domains contains a series of signalling questions or prompts, which guide the user through the tool. Once a domain-level judgement for each domain has been assigned, an overall judgement, using the same three levels of risk of bias, is assigned to the result.

<!----------------------------------------------------------------------------->
&nbsp;

#### Non-randomised studies of interventions/exposures

For non-randomised studies of interventions (NRSI), I used the ROBINS-I (Risk Of Bias In Non-randomised Studies - of Interventions) tool.[@sterne2016] This tool assess the risk of bias across seven domains: Bias due to confounding, Bias due to selection of participants, Bias in classification of interventions, Bias due to deviations from intended interventions, Bias due to missing data, Bias in measurement of outcomes, and Bias in selection of the reported result. Similar to RoB 2, it has a number of prompting questions per domain, with acceptable judgements including “Low risk”, “Moderate risk”, “Serious risk” and “Critical risk”. Ideally, observationals studies should be assessed in reference to an idealised randomised controlled trial. Further, the rare overall judgement of "Low" is considered equivalent to a randomised controlled trial.

For non-randomised studies of exposures (NRSE), I opted to use a preliminary version of the ROBINS-E tool.[@morganr2020] The tool is still in development, but while the signalling questions for each domain are yet to be confirmed, the overarching risk-of-bias domains have been finalised. The motivation for this using this tool above other existing published tools such as the Newcastle-Ottowa scale (NOS).[@wells2000] was two-fold. In the first instance, as mentioned in the introduction to this section, using a domain-based tool has distinct advantages over better-developed checklist-type tools such as the NOS. <!-- TODO CITATION NEEDED --> In addition, using a domain-based tool for non-randomised studies of exposures enabled better comparison with risk-of-bias assessments performed for the other study designs.

<!----------------------------------------------------------------------------->
&nbsp;

#### Mendelian randomisation studies

At present, no formalised risk-of-bias assessment tool for Mendelian randomization studies is available. Assessment of the risk of bias in Mendelian randomisation studies was informed by the approach used in a previous systematic review of Mendelian randomisation,[@mamluk2020] as identified by a review of risk of bias assessments in systematic reviews of MR studies (advance results from this review were obtained from contact with the authors.). <!-- TODO CITATION NEEDED --> A copy of this tool is available in Appendix \@ref(appendix-mr-rob). 

<!----------------------------------------------------------------------------->
&nbsp;

#### Risk of bias due to missing evidence

A recent shift towards the assessment of missing evidence due to selective non-reporting - as distinct from the selective reporting of a single result from multiple planned - is demonstrated via the forthcoming RoB-ME (Risk of Bias due to Missing Evidence in a synthesis) tool. <!-- TODO Need more here describing missing evidence --> The tool is in development stages, and as part of this review, I piloted the tool, and provided feedback to the developers. <!-- TODO CITATION NEEDED --> 

This additional appraisal marks a departure from the registered protocol, as there was initially no intention to try and examine the risk of bias due to missing evidence. This is because the tool did not exist when the protocol was originally registered.

<!----------------------------------------------------------------------------->
&nbsp;

### Analysis methods

An inital qualiatative synthesis of evidence was performed, summarising the data extracted from studies across 

Where individual studies were deemed comparable, they were incoroporated into a quantitative analysis or "meta-analysis". <!-- TODO CITATION NEEDED --> Meta-analysis provides a summary or pooled effect estimate across studies.

Of note, studies were not combined across different study designs (i.e. RCTs were not combined in a meta-analysis with results from observational studies). The results from each individual analytical approach were summarised, but are compare and contrasted more fully in the triangulation exercise presented in Chapter \@ref(discussion-heading)

&nbsp;
<!----------------------------------------------------------------------------->

#### Standard meta-analysis

<!-- Where both the outcome and the exposure were dichotmous, or the -->

Both a fixed-effect and random-effects meta-analysis model was employed to combine the different included studies. <!-- TODO Need more information on the difference between the two approaches, and their formula here, and description of different methods and quarrels regarding distinction fixed effect vs random effects meta-analyses. --> The fixed-effect method was implemented as:

\begin{equation}
  \theta_i = \mu + u_i
  (\#eq:meta-analysis-random)
\end{equation}

\begin{equation}
  weighted\ average = \frac{\sum Y_i (1/SE_i^2)}{\sum(1/SE_i^2)}
  (\#eq:meta-analysis-fixed)
\end{equation}

<!-- where ... -->

<!-- TODO Need equation for random effects too. See Sean's thesis here. -->

#### Dose-response analyses

<!-- TODO Explore how to map from simple diagnosis of hyperchol to dose response. Seems that does response needs >3 categories -->
<!-- Will also need description of dose-response meta-analysis here, with reference to forthcoming book Julian sent on.  -->

Several of the included studies presented data on multiple categories of lipid levels, but provided an overall effect estimate based on a comparison of only two of these categories (e.g. for example, highest vs lowest quartile). <!-- TODO Question: if I look at highest vs lowest, and also at top dose vs lowest dose, am I double counting results? --> While this allows for easy interpretation of the resulting effect estimate, it ignores any potential non-linear relationships between the exposure and outcome, in addition to discarding useful information contain in the interim groups.

Where possible, data for all exposure groups was extracted. <!-- TODO ??? --> Studies were excluded from this analysis if the number of categories was less than three, if the exposure cut-off points for for each category were not presented (e.g. if the study reports splitting participants into quartiles and comparing the highest vs lowest without giving the quartile bands in mmol/L).

<!-- Taken from another paper: "A potential non-linear association was evaluated using a restricted cubic spline model with three knots at the 10th, 50th and 90th percentile of frequency of the exposure [@orsini2012meta]" -->
<!-- TODO Do I initially want to fit a linear relationship across categories and  -->
A <!-- TODO confirm type of spline --> spline model was fitted to allow for a non-linear relationship, for example a U or J-shaped relationship, where low and high levels of an exposure can have an effect versus the "normal" reference dose. Reference doses were defined _a priori_ as the cut-off of the "Normal"/"Optimal" categories for each fractions, as detailed in Table \@ref(tab:lipidLevels-table). Under this approach, the reference dose was defined as 200 mg/dL for total cholesterol, 100 mg/dL for LDL-c, 40 mg/dL for HDL-c, and 150 mg/dL for triglycerides.

Due to the requirements for the dose response analysis, studies were excluded from this secondary analysis if they did not provide the require information: cut-off points. Where this was not reported in the study, I contacted the corresponding author to attempt to obtain the required information (see \@ref(contacting-authors)). 

When the highest category was open ended (e.g LDL-c $\geqslant$ 200 mg/dL), I calculated category midpoint by assuming the width of the higest category was the same as the one immediately below it. Similarly, when, the lowest category was open-ended (e.g LDL-c $\leqslant$ 100 mg/dL), I set the lower boundary for this category to zero (though this is unlikely to occur natually, it was difficult to define).

&nbsp;
<!----------------------------------------------------------------------------->

#### Sensitivity analyses

I conducted a leave-one-out analysis in order to explore the impact of any given study on the results. <!-- TODO CITATION NEEDED --> In addition, where available, I performed meta-regression to assess whether the results varied by age or sex.

<!-- TODO Need more here -->

&nbsp;
<!----------------------------------------------------------------------------->

#### Visualisation of results

The use of evidence maps is becoming increasingly common to explore the distibution of research cohorts included in a systematic review.<!-- TODO CITATION NEEDED --> As such, each individual cohort contibuting to the evidence base was quantified and visualised on a world map.

Further, given the importance of visualising the potential biases of a result alongside the result itself, a new visualisation tool was designed to allow for "paired" forest plots (as recommended by the ROB2 publication). <!-- TODO CITATION NEEDED --> This tool was developed as an adjunct to this thesis to aid in creating standard risk-of-bias figures,[@mcguinness2020robvisPaper] and the "paired" forest plot functionality grew out of a collaboration with other reserachers to design a modular method for creating these plots.[@zotero-14999] <!-- Apparently collaboration is a big thing they want to see --> A summary of this tool is contained in Appendix \@ref(appendix-robvis), and all forest plots presented in this analysis were created using this tool.

&nbsp;
<!----------------------------------------------------------------------------->

#### Assessment of added value of including preprints

Preprints are a valuable evidence source (see Introduction, Section \@ref()) but their inclusion in a systematic reivew. As part of a study within this larger review, I also to explore the additional evidental value of including preprints in the meta-analysis, assessed using the fixed effect weight from a standard meta-analysis.

Additionally, I followed preprints up over time to investigate whether all identified preprints included in the review were subsequently published (in which case preprints provide a snapshot into the future, and a systematic review update would capture these reports) or alternatively, if some preprints were not published, then preprints provide a distinct evidence source. 

&nbsp;
<!----------------------------------------------------------------------------->

## Results 

### Initial search and validation of search filters

`r num_to_text(23447,T)` records were identified through database searches.

<!--- TODO Cross check this with the actual numbers included --->

Of the 500 random records screened to ensure the accuracy of the study design filters, no eligible records were identified. Many of those excluded by the filters were basic science studies, commentaries or educational articles. <!-- TODO CITATION NEEDED -->

&nbsp;
<!----------------------------------------------------------------------------->

### Screening results

Following de-duplication, the titles and abstracts of `r num_to_text(16109)` records were assessed for eligibility. `r num_to_text(387)` were deemed potentially eligible, and the full text records for these were accessed and screened. \@ref(fig:prisma-flow-fig) <!-- TODO Check these figures -->

The PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-analyses) flow diagram[@page2021] <!-- TODO CITATION NEEDED - cite PRIMSA flow paper here, if published --> presented in Figure \@ref(fig:prisma-flow-fig), illustrates the movement of articles through the review. 

To highlight the contribution of preprint archives to the review, the flow diagram delineates between those records captured through databases searches (presented on the right of the diagram) and those captured by the search tool described in the previous chapter (presented in grey on the left of the diagram).

<!----------------------------------------------------------------------------->

<!-- TODO PRISMA must include reasons for exclusion - extract from  -->

```{r prisma-flow-setup, include = FALSE}
```

\blandscape{}

&nbsp;

(ref:prisma-flow-cap) PRISMA Flow diagram illustrating how records moved through the systematic review process. The different contributions of databases and preprint servers to the review are indicated.

(ref:prisma-flow-scap) PRISMA flow diagram

```{r prisma-flow-fig, echo = FALSE, results="asis", fig.pos="H", fig.cap='(ref:prisma-flow-cap)', out.width='100%', fig.scap='(ref:prisma-flow-scap)'}
knitr::include_graphics(file.path("figures/sys-rev/prismaflow.png"))
```

\elandscape{}

<!----------------------------------------------------------------------------->

<!-- TODO Show bar chart by date of publication? -->

<!-- For the snowballing exercise - make sure you don't include studies published after review date: ~ June 2019 -->

```{r agree-setup, include = FALSE}
```

The values of $AC1$ were interpreted against the categories presented in Table \@ref(tab:gwet-table). For the inter-rater reliability, agreement was "almost perfect" ($AC1$ = `r agreeInter_coeff[1]`, $kappa$ = `r agreeInter_coeff[2]`, Table \@ref(tab:agreeInter-table)). Similarly for intra-rater reliability, agreement was "almost perfect" ($AC1$ = `r agreeIntra_coeff[1]`, $kappa$ = `r agreeIntra_coeff[2]`, Table \@ref(tab:agreeIntra-table)). The discrepancy between the $AC1$ and $kappa$ coefficients illustrates the sensitivity of $kappa$ to imbalanced marginals, caused in this sample by a heavy imbalance  towards exclusion.[@feinstein1990]

&nbsp;

<!----------------------------------------------------------------------------->
(ref:agreeInter-caption) Inter-rater agreement on a subset of records, indicating high accuracy. 

<!-- TODO Include AC1 result in caption above -->

(ref:agreeInter-scaption) Inter-rater agreement

```{r agreeInter-table, message=FALSE, results="asis", echo = FALSE}
```
<!----------------------------------------------------------------------------->

<!----------------------------------------------------------------------------->
(ref:agreeIntra-caption) Intra-rater agreement on subset of records, indicating high consistency.

(ref:agreeIntra-scaption) Inter-rater agreement

```{r agreeIntra-table, message=FALSE, results="asis", echo = FALSE}
```
<!----------------------------------------------------------------------------->

&nbsp;

Those records which were excluded in the initial screening, but were included by the second reviewer (n=`r discrepancy_Inter`, Table \@ref(tab: agreeInter-table)) were investigated. This discrepancy between the two reviewers was explained in all cases by differing interpretations of the inclusion criteria, specifically around the definition of cognitive decline versus mild cognitive impairment, and the definition of eligible lipids fractions. 

&nbsp;
<!----------------------------------------------------------------------------->

### Characteristics of included studies

Following full-text screening, __XXXX__ studies met the criteria for inclusion in the review. Table \@ref(tab:studyCharacteristics-table) presents a summary of the characteristics of each study.


&nbsp;

\blandscape{}
<!----------------------------------------------------------------------------->
(ref:studyCharacteristics-caption) studyCharacteristics

(ref:studyCharacteristics-scaption) studyCharacteristics

```{r studyCharacteristics-table, message=FALSE, results="asis", echo = FALSE}
```
<!----------------------------------------------------------------------------->
\elandscape{}

The majority of studies were non-randomised, with only one included randomised controlled trials (the Heart Protection Study) and a relative small number of Mendelian randomisation studies. Of the non-randomised studies, many were assessments of the relationship with statis use.

Of the studies examining lipid levels, by far the most common fraction assessed was total cholesterol. The association with triglycerides levels was not consider in many papers.
<!-- TODO Check this is actually true-->

The vast majority of studies examined either all-cause dementia or Alzheimer's disease, with only a small proportion examining vascular dementia.

Many studies using electronic health records as their data source did not accurately report the diagnostic criteria used to identify cases, and of those that did, none sought to validate the accuracy of their defined code lists.

<!-- TODO Check this, and do I actually want to say this, as I don't validate my codelist in the CPRD analysis? -->

Many of the studies were conducted in electronic health record databases, which have a number of advantages over smaller more traditional cohort studies.

<!-- TODO Could extract before and after adjusted for ApoE4 to illustrate -->

<!--- TODO Figures and tables to include here: 

-   PRISMA Flowchart (done)

-   Summary of types of study (half done)

-   Summary of locations (half done)

-   Summary of diagnostic criteria used (done)

-   Summary of risk of bias (not done)

--->

<!-- TODO Also need to extract the codes used in each study -->


As shown in Figure \@ref(fig:cohortLocations), the majority of study cohorts were based in the developed world.

<!----------------------------------------------------------------------------->
```{r cohortLocationsSetup, include = FALSE}

```

(ref:cohortLocations-cap) Geographical distribution of study cohorts

(ref:cohortLocations-scap) Geographical distribution of study cohorts

```{r cohortLocations, echo = FALSE, results="asis", fig.pos = "H", fig.cap='(ref:cohortLocations-cap)', out.width='100%', fig.scap='(ref:cohortLocations-scap)'}
knitr::include_graphics(file.path("figures/sys-rev/cohortLocations.png"))
```
<!----------------------------------------------------------------------------->




<!----------------------------------------------------------------------------->
&nbsp;

### Risk of bias {#risk-of-bias-subheading}

Many of the included studies were at high-risk of bias. 

The one randomised controllled trial was deemed to be a low risk of bias, 

&nbsp;
<!----------------------------------------------------------------------------->

### Meta-analysis of statin use and hypercholesteroleamia

<!-- Straight-forward random effects meta-analysis of binary exposures, using study defined measures of each. -->

Statin use was associated with a reduce risk of all-cause dementia (`r estimate(0.76,0.66,0.88)`, Figure \@ref(fig:fpStatins)).

&nbsp;
<!----------------------------------------------------------------------------->
```{r primaryFigures, include = FALSE}
```

(ref:fpStatins-cap) __Forest plot showing effect of statins:__

(ref:fpStatins-scap) Forest plot showing effect of statins

```{r fpStatins, echo = FALSE, results="asis", fig.pos = "H", fig.cap='(ref:fpStatins-cap)', out.width='100%', fig.scap='(ref:fpStatins-scap)'}
knitr::include_graphics(file.path("figures/sys-rev/forester_statins_any.png"))
```
<!----------------------------------------------------------------------------->
&nbsp;

### Dose response meta-analysis of lipid levels {#dose-response-results}

<!-- TODO Cite excluded studies and list exclusion reasons  -->

Several studies were excluded from the dose-response meta-analysis, as the number of cases/controls per dose group could not be calculated and the corresponding author for the study did not respond to clarification requests. The results from the dose response analysis can be seen in Figure \@ref(fig:dose-response).

<!-- TODO Include image -->

&nbsp;
<!----------------------------------------------------------------------------->

<!-- ### Sources of heterogeneity -->

<!-- Detail that some of these are exploratory, in particular the effect of different scales on the association between the groups. **[CROSS]** -->

<!-- Important here to examine effect of risk of bias (due to immortal time in observational studies). -->

<!-- Also important to examine the effect of: -->

<!-- * age at baseline -->
<!-- * sex (percentage) -->
<!-- * education -->
<!-- * baseline cognitive scores (how to group) -->

<!-- ### Sensitivity analyses -->

&nbsp;
<!----------------------------------------------------------------------------->

### Publication bias {#sys-rev-pub-bias}

<!-- -   Check if there are protocols available for any of the published reports (unlikely for non-randomised controlled trials), and whether there were -->
<!-- -   Vascular dementia has substantially less published reports. Many (reference Smeeth et al 2010 here) simply group into AD and non-AD making comparison between published studies difficult -->

There was little evidence of publication bias across the evidence base (Figure \@ref(fig:funnelStatinsAny)).

<!-- One particularly interesting meta-bias potentially applicable in this review is the definition of code lists across -->

<!-- Many studies using large observational electronic health records (n=?) did not report the code-lists used define the outcome events in their analysis. Depsite attempts to obtain this additional information through contact with a -->

&nbsp;
<!----------------------------------------------------------------------------->
(ref:funnelStatinsAny-cap) Funnel plot of results examining the relationship between statins and any dementia

(ref:funnelStatinsAny-scap) Funnel plot of results examining the relationship between statins and any dementia
```{r funnelStatinsAny, echo = FALSE, results="asis", fig.pos = "H", fig.cap='(ref:funnelStatinsAny-cap)', out.width='100%', fig.scap='(ref:funnelStatinsAny-scap)'}
knitr::include_graphics(file.path("figures/sys-rev/funnel_statins_any.png"))
```
<!----------------------------------------------------------------------------->
&nbsp;

### Added evidental value of including preprints {#sys-rev-including-preprints}

<!-- TODO Will have to get actual numbers for this section -->

As show in Figure \@ref(prisma-flow-fig), the number of results returned by the preprint searching, was not substantial (bioRxiv = 256, medRxiv = 0). <!-- TODO check -->

<!-- QUESTION Can I explore the amount of information added by the preprints quantiatively using the weights in the meta-analysis -->

&nbsp;
<!----------------------------------------------------------------------------->

### Risk of bias due to missing evidence

A key issue identified relating to bias due to missing data was the potential for the search to miss relevant Mendelian randomisation studies which had examined the association between Alzhiemer's disease and multiple risk factors. This is discussed further in Section \@ref(rev-discussion-MR).

<!----------------------------------------------------------------------------->
&nbsp;

## Discussion

### Summary of findings

This review has presented a summary of the available evidence on the association between lipids, and treatments that modify lipids such as statins, and the subsequent risk of dementia.

The distribution of evidence between analytical designs is to be expected. Randomised controlled trials of dementia are particularly challenging, as the long follow-up, necessary due to the long latent period of the condition, makes trials logistically challenging and financial expensive. Similarly, Mendelian randomisation is a relatively new study design, and so only appears in the literature in recent years, driven by the growth of dementia GWASs (Figure \@ref(fig:typeByYear)).

<!----------------------------------------------------------------------------->
(ref:typeByYear-cap) __Study designs by year of publication__ - 

(ref:typeByYear-scap) Study designs by year of publication

```{r typeByYear, echo = FALSE, results="asis", fig.pos = "H", fig.cap='(ref:typeByYear-cap)', out.width='100%', fig.scap='(ref:typeByYear-scap)'}
knitr::include_graphics(file.path("figures/sys-rev/type_by_year.png"))
```
<!----------------------------------------------------------------------------->

A common theme across the evidence base was a lack of data on the association of vascular dementia. This is particularly interesting given that lipids and statins are primarily related to vascular disease. There is the potential that studies encountered similar difficulties in address the unexpected results observed in the CPRD analysis in Chapter \@ref(cprd-analysis-heading), likely due to confounding by indication, and so may suffer from the "file-drawer effect". <!-- TODO CITATION NEEDED --> For vascular dementia, few Mendelian randomisation studies examined this outcome, primarily because of the absence (until recently) of genome wide association studies, which are used in two sample summary Mendelian randomisation studies.

One item of particular interest is the attenuation of any effects observed by Mendelian randomisation studies following the adjustment for/exclusion of genetic variation in the Apoe4 gene region. As covered in the discussion, ApoE4 genotype is the major risk factor for Alzheimer's disease, but is also involved in cholesterol transport.

<!--- 
QUESTION
This does introduce some conflicts with other studies where a change score has been dichotomised in order to have a binary dementia or not dementia outcome. - No these studies should not be included, as not recognised criteria.
-->

<!-- __Exclusion of the PROSPER trial__ Will need to have a good bit here on the exclusion of this RCT, particularly as it is included in the Cochrane review on the topic. Cross-reference with the meta-bias section in the early section, on the impact of only included studies that sought to make a definitive diagnosis, rather than using change in cognitive assessment scales. -->

This review did not include the commonly cited PROSPER study, which examined the effect of pravastatin on CVD risk and is regularly held up as  providing no evidence for an effect of the statin on dementia outcomes. While widely cited and included in the Cochrane review on this topic,[@mcguinness2016] only reported the mean change in a range of cognitive measures (MMSE, Stroop test, Picture-Word Learning test, etc.) over follow-up rather than incident. While this is a useful indicator of general cognitive decline, it is not equivalent to a dementia diagnosis using a , as cognitive tests should feed into a broader diagnostic pathway (see section \@ref(diagnostic-criteria)). As such, this trial did not met the inclusion criteria for this review.

<!----------------------------------------------------------------------------->
&nbsp;

### Previous reviews {#rev-previous-reviews}

<!-- TODO This section will be informed by the review published as a preprint -->

__This section will be completed once Georgia has completed her analysis, and can also be cross-references with the meta-meta analysis published is Brain Sciences recently.__

While conducting this review, I identified several previous systematic reviews of this topic.[@chu2018; @yang2020; @muangpaisan2010; @poly2020;@kuzma2018a] However, this review is the first to use established domain based assessments tools (for example, the RoB 2 tool for randomized controlled trials)[@sterne2019] to assess the risk of bias in included studies, and explore the heterogeneity of results across different levels of risk of bias levels. Some previous reviews did assess risk of bias, but used non-domain based assessment tools, such as the Newcastle-Ottowa scale. <!-- TODO CITATION NEEDED -->

However, despite these differences in methodology, the duplication of work across reviews (including this review) is substantial. In retrospect, an alternative approach to conducting a further systematic review from scratch could have been employed. Known as an umbrella review, or review-of-reviews, these studies use other systematic reviews rather than primary studies as the unit of analysis. This approach would have enable more efficient identification of relevant primary studies, to which the methods which sets this review apart from other published reviews could have been applied.

<!-- IDEA Put upSet plot here - though this will likely now be in Georgia's dissertation -->

<!-- An upset plot shows the total size of each set (in this case, the set of included studies in each review) in the bottom left hand bar plot. For example, the XXXX review includes XXXX studies. The alternative is to have a barplot showing the total cumulative number of records over the years, using our set as the master, with the bars coloured by the number of records included in 1/2/3/4 reviews. The information below the bar axis could then show the number of reviews published in that year, or alternative, you could have an upsidedown bar to show cumulative number of reviews on this topic. -->

<!----------------------------------------------------------------------------->
&nbsp;

<!-- ### Triangulation across evidence sources -->

<!-- TODO Have a look at this - potentially move to triangulation chapter -->

<!-- One key question for which multiple distinct sources of evidence were available were those looking at Laz -->

<!-- Consideration of the potential impact of the magnitude and direct of residual confounders/bias is not a major stretch from what is already happening in the assessment of the quality of evidence (GRADE) framework. Within GRADE, the overall quality of evidence can be upgraded when there is deemed to be unmeasured or residual confounding variables which reduce the. For example, if the propensity to treatment is related to comorbidity burden, but those on treatment still have better outcomes then those on control, it is likely that the true effect of the intervention is being underestimated. [@guyatt2011] -->

<!-- Without our framework and as part of the risk of bias assessments reported in \@ref(risk-of-bias-subheading), I attempted to records the direction of the bias, so that it could feed into the triangulation. -->


<!----------------------------------------------------------------------------->
&nbsp;

### Inclusion of preprints

As highlighted in Section \@ref(diverse-sources-preprints), this review explicitly sought to synthesize evidence across different publication statuses (preprinted vs. published). Using the tool described in Chapter \@ref(sys-rev-tools-intro), two preprint serves related to health and biomedical sciences were search as part of this review (see Appendix \@ref(appendix-medrxivr-code) for the code used to search the repositories). There were several relevant preprints captured by the search. The added evidential value of including these preprints was highlighted in Section \ref(sys-rev-including-preprints). 

The small number of studies return by the searches (or the absence of any hits in the medRxiv database - see Figure \@ref(fig:test)) is due to the timing of the preprint searches. The searches for this review were performed in mid-July 2019, but the first medRxiv preprint was registered on 25th June 2019. As such, at the point it was searched, the medRxiv database contained only a small number of records.

While none of the preprints contributed uniquely to the review, this is again. Consider the example of one of the Mendelian randomisation analyses of the effect of LDl-c on Alzheimer's disease, which found no impact of LDL-c on AD following removal of APOE4. The manuscript was initially published in bioRxiv in July 2017[@zhu2017] which was subsequently published in Nature Communications in January 2018, following peer-review.[@zhu2018] While this study was captured by both the preprint and published searches in this review, had the searches been run within this window, the preprint would have contributed unique data to the review. This illustrates that while it may not have aided this review, if the aim is to find the current state of the art in the topic area at the time of searching, inclusion of preprints is a necessity.

Of note, since the start of my review, inclusion of preprints in systematic reviews has now become widespread due to the role of preprint servers, in particular medRxiv, as a key evidence dissemination venue during the early stages of the COVID-19 pandemic. However, how well this adoption of preprints will transfer to other topics, where the speed of research does not put the same focus on preprinted articles, is currently unknown.


<!----------------------------------------------------------------------------->
```{r preprintGrowthSetup, include=FALSE}
```

(ref:preprintGrowth-cap) __Growth of preprint repositories over time__ - Given the relative sizes of the preprint repositories at the time the searches for this review were conducted (bioRxiv n= `r n_at_search["bio"]`, medRxiv n = `r n_at_search["med"]`), the relative number of hits returned by each is expected.

(ref:preprintGrowth-scap) Growth of preprint repositories over time

```{r preprintGrowth, echo = FALSE, results="asis", fig.pos = "H", fig.cap='(ref:preprintGrowth-cap)', out.width='100%', fig.scap='(ref:preprintGrowth-scap)'}
knitr::include_graphics(file.path("figures/sys-rev/preprint_growth.png"))
```
<!----------------------------------------------------------------------------->
<!-- TODO Include figure here showing growth of preprint database size, and cite studies detailing increasing acceptance as a valid form of evidence.  -->

<!-- This does speak to the over-representation of certain study designs in the preprint literature  -->

<!-- Regardless, some studies were identified -->

<!----------------------------------------------------------------------------->
&nbsp;

### Reviewing Mendelian randomisations studies {#rev-discussion-MR}

Mendelian randomisation is a powerful analytical technique, using natural variation in participants genomes to (assuming the assumptions of the method are valid), though it's inclusion as an acceptable study design in this review was complicated by a number of factors. 

Firstly, as this study design is relatively new, particularly when compared to randomised trials or cohort studies, the process and tools for systematically assessing them are not as well developed. A key example of this is in the absence of validated search filters for Mendelian randomisations studies. This limitation is further complicated by the varying terminology used to describe the method, particularly in the early years of it's application.

Additionally, there is currently no widely used risk-of-bias assessment tool for Mendelian randomisation studies. A recent commentary provided a checklist  interpreting Mendelian randomisation studies, this guide includes reporting items in their quality checklist. While reporting quality is important, it is a separate consideration to internal validity, as discussed in Section \@ref(). Similarly, a previous review of Mendelian randomisation studies used the Q-Genie tool, which was validated to assess the quality of genetic association studies in meta-analysis.[@sohani2015] While this tool addresses the studies used, it does not access the additional methodological considerations of the analysis of the Mendelian randomisation analysis itself. <!-- TODO Read this tool. --> For this review, I utilised the best available author-devised tool, sourced on a recent review of systematic reviews of Mendelian randomisation studies. 

<!-- TODO Need to add some text to the ROB section of this chapter on distinction between reporting and quality -->

<!-- Problems with overlapping samples in reviews of Mendelian randomisation studies in that may be double counting participants -->
<!-- TODO Check discussion on this point from Matt Lee thesis, in email -->
<!-- TODO Email George with query on dual counting, following review of email with Julian -->

As a further stumbling block, Mendelian randomisation, particularly when using a two-sample summary data design, is a form of analysis that lends itself to multiple exposure-outcome comparisons. This is particularly relevant to the consideration of bias due to missing evidence. As an example, through snowballing and other measures, I identified at least one relevant Mendelian randomisation study that had not been identified by the search strategy.[@larsson2017b] On review of this paper, the search would not have been expected to find it given the absence of any lipid-related keywords in the title and abstract. The study examined the association between lipid fractions with Alzheimer's disease as one of many risk factors for the condition. Studies such as this can introduce bias into a systematic review, as it is commonly only those risk factors that show a statistically significant result that are reported in the abstract and so are captured by the search. This may bias systematic reviews, including this one, as the analysis of multiple risk factors against a single outcome within a single publication becomes more common. These studies are described as "unknown unknown's" in the context of the RoB-ME tool, and are particularly challenging (as opposed to an analysis that was insufficiently reported to be included in the statistical analysis, or the "known unknown's").

Useful future work to improve the methodology for inclusion of Mendelian randomisation studies in systematic reviews should involve the development of a validated search filter for this study design.[@waffenschmidt2020; @wagner2020] Alternatively, in better-resourced reviews, a dedicated search for "risk factors" and "dementia" and "Mendelian randomisation", followed by manual review of studies that look across multiple risk factors, would be advisable. This was not feasible in the context of this review, given the large number of records to be screened, even when using study design filters (n=`r comma(16109)`). Additionally, the value of methods that supporting the traditional bibliographic database search, such as snowballing (forwards and backwards citation chasing) and communication with relevant topic experts should not be underestimated. Finally, development of a risk-of-bias assessment tool by a panel of methodologists and analysts would be of substanital benefit. 

<!-- TODO Ask Matt Page whether he has done any work on this? Studies looking at many risk factors don't report null results in abstract and so do no -->

&nbsp;
<!----------------------------------------------------------------------------->

### Comments on the process

As part of the reflective element of this thesis, I collated my experiences on performing a systematic review.

&nbsp;

#### Protocol registration

While the protocol was registered on the Open Science Framework largely to allow for the sharing of associated documents such as the proposed search strategy, anecdotal evidence from colleagues and collaborators suggested that the findability of the protocol was limited. In hindsight, I should also have cross-posted the protocol on PROSPERO, the international prospective register of systematic reviews,[@booth2011] (which does not allow for the uploading of related files).

&nbsp;

<!-- #### Workload -->

<!-- Systematic reviews should not be performed as part of a thesis, without suitable support and resourcing guaranteed. Assumption that everyone does a systematic review (without risk of bias assessments, inclusion of all literature, searching for other reviews) is foolish. -->

<!-- Average time to complete a s -->

<!-- However, new developments such as automated screening would allow for a reduced need for personnel to work on these things. <!-- TODO CITATION NEEDED - Cite Triccios new paper here -->

&nbsp;

#### People management

This review provide an excellent opportunity to gain experience in managing a team of researchers. However, due to the need for dual screening and data extraction over a long period of time, a number of external researchers became involved in this review. I found the people-management aspect particularly challenging, and in retrospect could definitely have improved the process through better communication of expectations and deadlines.

&nbsp;
<!----------------------------------------------------------------------------->

<!-- ### Validation against recent umbrella review -->

<!-- In summer 2020, an umbrella review (also known as a review-of-reviews) of the impact of lipids levels of Alzheimer's disease risk -->


<!-- The study included XXX reviews and YYY primary studies, and I used this list of primary studies as an extra validation step to assess the accuracy of my work. -->

<!-- Umbrella reviews follow a systematic review approach, but their unit of interest is other systematic reviews rather than primary studies. In fields where there is a lot of existing reviews, they -->

<!-- All studies identified by reviews included in te -->

<!-- In hindsight, this approach ma -->


&nbsp;

<!----------------------------------------------------------------------------->

#### Open data sharing {#sys-rev-open-data}

As discussed in Section \@ref(dose-response-results), many primary studies did not report important elements, and so these could not be extracted. This limiation was compounded by the expected low response rate to requests for further information from primary authors (although, in hindsight, the form of contact used (email) has been shown to be less successful in eliciting responses from authors when compared with telephoning[@danko2019]).

While contacting authors is worthwhile, as it can substantially change the conclusion of a systematic review[@meursingereynders2019] and is not too costly to systematic reviewers,[@cooper2019] a far preferable option is that the authors of primary studies readily deposit all relevant study data at the point of publication. 

Based on my experience of extracting data for this review, I co-wrote an guidance article to aid primary prevention scientists in preparing and sharing their data so that it can easily be incorporated into a evidence synthesis exercise, using a trial of mindfulness interventions as an case study.[@hennessy2021]

Similarly, a substantial amount of time and effort has gone into making the data obtained by this review openly available to other researchers. <!-- TODO Cite the Zenodo repo here! -->

&nbsp;
<!----------------------------------------------------------------------------->

### Strengths and limitations

<!-- TODO Cross reference with other discussion sections to ensure material isn't being repeated. -->

#### Strengths

I believe there are four aspects where this review is distinct from those reviews already available in the published literature (as identified by <!-- TODO CITATION NEEDED -->):

-   *Comprehensiveness:* While several reviews of this research topic exist,[@chu2018; @yang2020; @muangpaisan2010; @poly2020] the overlap between the list of studies included in each is not 100%. As part of this review, I have not only performed a original search of primary literature databases, but have also screened the reference lists of comparable reviews to ensure no study has been omitted. <!-- TODO Though this could --> In addition

-   *Structured risk of bias assessment:* The majority of the highly cited reviews on this topic either do not formally consider the risk of bias in the observational studies they include or do not use an appropriate domain-based assessment tool (e.g. ROBINS-I/E). This is important area in which this thesis can add value, as based on the risk-of-bias assessments I have performed to date, several primary studies are at high risk of bias and this should be reflected in the findings of any review on this topic.

-   *Inclusion of preprints:* Unlike other available reviews and enabled by the tool described in Chapter \@ref(sys-rev-tools-heading), this review systematically searched preprinted health-related manuscripts as a source of grey literature. As part of this chapter, I plan to examine the extent of the additional information provided to the review by the inclusion of preprints.

<!-- TODO Comment on the fact that several of the studies identified through the search of preprints, were subsequently published, but much later than the cut-off point for the search (e.g. they would not have been captured by the search for published papers) -->

- *Contribution to methods work:* A large part of this review was the associated work on improving research synthesis methods. This work is detailed as relevant throughout the Chapter, often referring to additional work detailed in the . In addition this review was used to pilot an upcoming risk of bias tool

<!----------------------------------------------------------------------------->
&nbsp;

#### Limitations

The primary limitation of this review is that several included studies used data from EHR databases, which come with serious concerns regarding validity [@hsieh2019] [@mcguinness2019validity; @wilkinson2018] Relatedly, several  studies which made use of electronic health record database did not report the specific code lists used, potentially introducing substantial heterogeneity between effect estimates. An empirical example of the effect of differing EHR code list is presented as part of the analysis in Chapter 4 (see Section \@ref(comparing-codelists)).

In addition, the fact that only a sample of records were dual screened at the title/abstract and full-text stages is a potential limitation, as there is a chance that some eligible records could have been excluded. However,  evidence from assessments of inter- and intra-rater reliability indicate that is is not a major concern.

One particular limitiation with regards to the risk of bias assessment is the fact that the ROBINS-E assessments were performed without the tool being finalised. This meant that there were no signalling questions to guide the domain-level risk of bias assessment, which may have influenced the accuracy with which domain-level judgements were assigned. However, there is no published empirical evidence supporting the need for signalling questions, and assessment of inter-rater reliability across the different tools did not indicate a specific problem with the ROBINS-E assessments. In fact, low agreement was common across the tools, though this is expected based on the available literature. <!-- TODO CITATION NEEDED -->

One further limitation is the fact that the risk of bias due to missing evidence assessment, combined with some empirical evidence that some studies were missed by the search but contained relevant studies is a definite limitation of this review (see Section \@Ref(rev-discussion-MR) above for a fuller discussion of this issue with respect to Mendelian randomisations studies). Unfortunately, this is probably a common limitation across all reviews, based on the way in which increased sensitivity must be balanced with a reasonable workload.

<!----------------------------------------------------------------------------->
&nbsp;

## Conclusions

In this chapter I have presented a comprehensive systematic review of the different sources of evidence available which examined the relationship between lipid levels and dementia use. 

This work built on the tool introduced in the preceeding chapter (Chapter \ref(sys-rev-tools-heading)), and findings from this review are used though out the subsequent chapters: in Chapter \@ref(cprd-analysis-heading),  summary of the evidence guided the choice of analysis approach, ensuring that the new analysis was at risk of a different source of bias; while in Chapter \@ref(ipd-heading), prospective cohorts identified by the review were contacted in an attempt to obtain individual participant data; finally, the cumulative effect measures calculated here are used as a key source of evidence for the triangulation exercise presented in Chapter \ref(discussion-heading).

<!-- IDEA Link here to IPD, talking about how the meta-regression on key variables suggested the use of sex and age as key variables-->

\newpage 

## References

