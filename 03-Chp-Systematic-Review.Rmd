---
bibliography: bibliography/references.bib
csl: bibliography/nature.csl
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
  bookdown::word_document2: 
      toc: false
      toc_depth: 3
      reference_docx: templates/word-styles-reference-01.docx
      number_sections: false 
  bookdown::html_document2: default
documentclass: book
---
```{block type='savequote', include=knitr::is_latex_output(),quote_author='(ref:sys-rev-quote)', echo = FALSE}
"It is surely a great criticism of our profession that we have not organised a critical summary by speciality or sub-speciality, up-dated periodically, of all relevant RCTS."  
```

(ref:sys-rev-quote) --- Archibald Cochrane, 2000 [@cochrane1979]

# Systematic review of all available evidence {#sys-rev-heading}

\minitoc <!-- this will include a mini table of contents-->

```{r, echo = FALSE, warning=FALSE, message=FALSE}
source("R/doc_options.R")
source("R/helper.R")
knitr::read_chunk("R/03-Code-Systematic-Review.R")
doc_type <- knitr::opts_knit$get('rmarkdown.pandoc.to') # Info on knitting format
```

<!-- ## To Do -->

<!-- -   TODO Examine how many times a primary study was included in a review -->
<!-- -   TODO Look at whether searching preprints made any difference to my results -->
<!-- -  TODO Could apply ROBIS to previous reviews to see how they stack-up? (This may be done as part of Georgia's summer project) -->

<!-- TODO Extra studies to include: -->

<!-- * Smeet et al. reference in the CPRD analysis -->
<!-- * Larsson - 10.1136/bmj.j5375 [@larsson2017b] -->
<!-- * Statins and the risk of dementia Jick -->

## Lay summary

The importance of

I found that

The results of this review are used to guide the primary analysis presented in Chapter \@ref(cprd-analysis-heading), in addition to forming a key evidence source used in the triangulation exercise presented in Chapter \@ref(discussion-heading).

&nbsp;

<!----------------------------------------------------------------------------->
## Introduction {#sys-rev-intro}

The aim of this chapter is to systematically review all available literature on the association between blood levels of total cholesterol and it's constituent parts (HDL-c,LDL-c and triglycerides) on the subsequent risk of dementia.

Based on the review of existing literature, no previous evidence synthesis exercise attempted to examine all available literature simultaneously. 

Only by looking across all available evidence can the true, given the often limited use of one type of evidence (RCTs are hard to do, obs studies are biased).

Literature con

Systematic reviews aim to transparently and reproducibly identift

<!-- TODO Need to seriously improve this -->

<!----------------------------------------------------------------------------->

&nbsp;

## Methods

### Protocol

A pre-specified protocol for this analysis was registered using the Open Science Framework, and is available for inspection.[@mcguinnessluke2020] 

### Contributions

This systematic review was performed in conjunction with a team of secondary reviewer, all of whom . These included Alexandra MacAleenana, Athena Sheppard, Matthew Lee and Lena Schmidt.

<!-- TODO Double check this list at the end of the project. -->

### Search strategy and and study selection

I systematically search electronic bibliographic databases to identify potentially relevant records. The search strategy used in each database was developed in an iterative manner using a combination of free text and controlled vocabulary (MeSH/EMTREE)[@lefebvre2019searching] terms to identify studies which have examined the relationship between blood lipids levels and dementia, incorporating input from an information specialist. The strategy will include terms related to lipids, lipid modifying treatments, and dementia and its sub-types, and was designed for MEDLINE before being adapted for use in the other bibliography databases listed. An outline of the general strategy is presented in the Table \@ref(tab:searchOverview-table) below and the full draft search strategies for each database are presented in Appendix \@ref(appendix-search-strategy). <!-- TODO Need to actually attach each search strategy. Should be able to loop through search strategy results. -->

&nbsp;

<!----------------------------------------------------------------------------->
(ref:searchOverview-caption) Summary of systematic search by topic. The full search strategy including all terms and the number of hits per term is included in Appendix \@ref(appendix-search-strategy).

(ref:searchOverview-scaption) searchOverview

```{r searchOverview-table, message=FALSE, results="asis", echo = FALSE}
```
<!----------------------------------------------------------------------------->

&nbsp;

To ensure that the study design filters are not excluding potentially relevant records, a random sample of 500 records identified by the main search but excluded by the filters (defined as "8 NOT 14" in Table \@ref(tab:searchOverview-table)) was screened. If any potentially relevant studies are identified, their titles and abstracts were searched for key terms that can be incorporated into the filters to improve search sensitivity.

<!-- TODO need to comment on this feed-back process, or remove -->

The following databases were searched from inception onwards: Medline, EMBASE, Psychinfo, Cochrane Central Register of Controlled Trials (CENTRAL), and Web of Science Core Collection (for the specific databases available thi via the University). We will also search clinical trial registries, for example ClinicalTrials.gov, to identify relevant randomized controlled trials. As the contents of the Web of Science Core COllection can vary by institution, the specific database searched via this platform are listed in Appendix \@ref(appendix-wos-databases).

The abstracts list of relevant conferences (e.g. the proceedings of the Alzheimer's Association International Conference, published in the journal Alzheimer's & Dementia) were searched by hand. <!-- TODO how were these searched? Date and other range -->

 Grey literature will also be searched via ProQuest, OpenGrey and Web of Science Conference Proceedings Citation Index, while theses were accessed using the Open Access Theses and Dissertations portal. We will also search bioRxiv and medRxiv, preprint repositories using a tool built as part of this thesis, to identify potentially relevant studies. Finally, the reference lists of included studies were searched by hand while studies citing included studies will be examined using Google Scholar (forward and reverse citation searching).

<!-- TODO Look at citationchaser for this, but have a cut-off publication date of July 2019 - better than searching by hand/Google Scholar -->

<!-- Include table with overview of search strategy here: summarise topics and how they were combined -->

#### Study selection

Records were imported into Endnote and deduplicated using the method outlined in Bramer et al. (2016).[@bramer2016] In summary, this method uses multiple stages to identify potential duplicates, beginning with automatic deletion of records matching on multiple fields ("Author" + "Year" + "Title" + "Journal"), followed by manual review of less similar articles (e.g. those matched based on the "Title" field alone).

Screening (both title/abstract and full-text) was performed using a combination of Endnote, a citation management tool,[@hupe2019] and Rayyan, a web-based screening application.[@ouzzani2016] Title and abstract screening to remove obviously irrelevant records was performed primarily by the primary author, with a random selection of excluded records being screened in duplicate to ensure consistency with the inclusion criteria.

Full-text screening will also be completed in full by the primary author. A second reviewer screened a random sample (approx 10%) of included and excluded records, in addition to any records identified by the first reviewer as being difficult to assess against the inclusion criteria. Reasons for exclusion at this stage were recorded. Disagreements occurring during either stage of the screening process were resolved through discussion with a senior colleague. A PRIMSA flow diagram will be produced to document how records moved through the review.[@page2021]

#### Inclusion criteria

We will seek studies that examine the relationship between blood lipid levels (or any specific lipid fraction, including total cholesterol, HDL, LDL, and triglycerides) and risk of incident dementia/MCI. Eligible study designs include randomized controlled trials and non-randomized observational studies of lipid modifying treatments, longitudinal studies examining the effect of increased/decreased blood lipid levels, and genetic instrumental variable (Mendelian randomization) studies examining the effect of genetically increased/decreased blood lipid levels.

Participants were free (or assumed to be free) of dementia/MCI at baseline. Studies of any duration were included to allow for exploration of the effect of length of follow-up on the effect estimate using meta-regression. No limits will be placed on the sample size of included studies.

Eligible studies defined dementia according to recognised criteria, for example the National Institute of Neurological Disorders and Stroke Association-Internationale pour la Recherche en l'Enseignement en Neurosciences (NINDS-AIREN), International Classification of Diseases (ICD), or Diagnostic and Statistical Manual of Mental Disorders (DSM) criteria. <!-- TODO do i actually want to include MCI? -->
 For MCI, eligible studies are those that attempted state a definition for diagnoses of MCI (e.g. an adapted version of the Petersen criteria [@petersen1999]) and create ordinal groups of patients (e.g. no dementia or dementia/MCI/dementia) based on this definition.

No limitations will be imposed on publication status, publication date, venue or language, although sufficiently detailed reports of the studies to be able to examine their methods were required for inclusion.

#### Exclusion criteria

Case-control studies, cross-sectional studies, qualitative studies, case reports/series and narrative reviews will be excluded. Studies which present no evidence of attempting to exclude prevalent cases from their analyses will also be excluded. Studies that measure change in continuous cognitive measures (e.g. MoCA score) without attempt to map these scores to ordinal groups (e.g. no dementia/MCI/dementia) will be excluded. Conference abstracts with no corresponding full-text publication will be examined, and we will contact authors to obtain information on the study's status. Studies that are reported in insufficient detail (e.g. only in conference abstracts, new, letters, editorials and opinion) will be excluded. Previous systematic reviews will not be eligible, but their reference lists will be screened to identify any potentially relevant articles. Studies with outcomes not directly related to the clinical syndrome of dementia (e.g., neuroimaging), studies implementing a "multi-domain intervention" where the lipid-regulating agent is included in each arms (e.g. for example, a study examining exercise + statins vs statins alone, but a study examining exercise + statins vs exercise alone would be included), and studies where there was no screening for dementia at baseline except if the sample was initially assessed in mid-life (i.e. below the age of 50) will be excluded.

Excluded studies performing autopsy unless it was done under accepted criteria Exclude studies using a dietary intervention, for example omega-3 fatty acid enriched diet, as it hard to disentangle the effect of other elements contained within te diet, vs simple tablet based supplements of

<!-- TODO Expand on rationale for exclusion of cross-sectional studies. Dementia is a disease severly at risk of differential recall, leading to misclassification bias. By using prospective studies, the potential for this bias is reduced. -->

<!----------------------------------------------------------------------------->
&nbsp;

#### Assessment of inter-rater reliability

Inter- and intra-rater reliability was assessed for a 10% sub-sample of records at the title and abstract screening stage. Intra-rater reliability involved a single reviewer applying the inclusion criteria to the same set of records while blinded to their previous decisions, while inter-rater reliability involved two reviewers independently screening the same set of records.

If this approach demonstrates a significant level of erroneous exclusion by the primary author a larger proportion will be dual-screened. 

Rater reliability was assessed using Gwet's agreement coefficient (AC1).[@gwet2008] This measure of inter-rater reliability was chosen over other methods of assessing inter-rater reliability such as percent agreement (number of agreements divided by total number of assessments), as it accounts for chance agreement between reviewers but does not suffer from bias due to severely imbalanced marginal totals in the same way that Cohen's $kappa$ value does. [@cohen1960: @gwet2008; @wongpakaran2013] Given the small number of included studies in this review, this is an important characteristic.

Gwet's AC1 is defined as:

$$AC1 = \frac{observed\;agreement-chance\;agreement}{1-chance\;agreement}$$ 

In reference to a two-by-two table with cells A, B, C and D, it is calculated using the following:


\begin{equation}
  AC1 = \frac{\frac{A+D}{N}-e(\gamma)}{1-e(\gamma)}
  (\#eq:AC1-main)
\end{equation}

where $e(\gamma)$ is the chance agreement between raters, given as $2q(1-q)$, where 

\begin{equation}
  q = \frac{(A+C)+(A+B)}{2N}
  (\#eq:AC1-supp)
\end{equation}

<!-- TODO Need to be sure of how to calculate. -->

How to interpret agreement co-efficients is widely debated, and while arbitary cut-off values may mislead,[@brennan1992] they provide a useful rubric by which to assess inter-rater agreement. Here, I used guidelines based on a stricter interpretation of the Cohen's $kappa$ coefficient,[@mchugh2012] presented in Table \@ref(tab:gwet-table).

&nbsp;

<!----------------------------------------------------------------------------->
(ref:gwet-caption) Suggested ranges to aid in interpretation of Gwet's AC1 inter-rater reliability metric

(ref:gwet-scaption) Ranges for Gwet's AC1

```{r gwet-table, message=FALSE, results="asis", echo = FALSE}
```
<!----------------------------------------------------------------------------->

&nbsp;

### Data extraction

Harmonization of cholesterol measures across studies was performed, as different studies used different methods to quantify exposure, including comparing differing risks in the highest vs lowest quartiles of a lipid, using a binary classification of patients into a hypercholesterolaemia or not, categorizing lipid levels into high, middle, and low groups according to study-defined criteria, and simply treating the exposure as a continuous variable.


Following guidance for a dose response meta-analysis, I picked midpoint for all studies where bands were provided (extremely common in the case of lipids)

This analysis was unusual, as multiple methods of integration and analysis were required with data spanning many forms.


A piloted data extraction for was used. I extracted the data in the first instance and subset was checked by a colleague for accuracy.

<!-- TODO Cross reference with the data extraction issues here - i.e. not providing qratile cut-off values, not poviding numbers per group, etc. -->

Where necessary, lipid levels reported in _mmol/L_ were converted in _mg/dL_ using the following formula:

\begin{equation} 
  mg/dL = mmol/L \times{} Z
  (\#eq:lipidConversion)
\end{equation} 

where $Z = 38.67$ for total cholesterol, LDL-c and HDL-c, and $Z = 88.57$ for triglycerides. For widely-used categorises of lipids levels on the _mg/dL_ scale, see Table \@ref(tab:lipidLevels-table) in Section \@ref(intro-lipid-fractions). 

<!----------------------------------------------------------------------------->

#### Studification

Multiple publications resulting from the analysis of the same data were included and grouped ().

In the advent of multiple papers reporting results on the same cohort, but say, at different time points, I used a process of studifiction to build out the most comprehensive accounts of the studies and results from as many published articles were applicable. 

This was particularly relevant to preprints and published papers reporting the same study, which were not considered duplicate records. Instead, they were considered as different reports of the same study. This is due to the potential for the published version to offer some information that the preprint did not, and vice versa.

<!----------------------------------------------------------------------------->

#### Following up with authors {#contacting-authors}

Where data points required either for the analysis or risk-of-bias assessment but were not reported, the primary authors of the study were contacted. This approach was taken due to the potentially large impact of following up with authors on the results of the review,[@reynders2019] 

This was particular important for the dose response meta-analysis, we


Use the example of getting the cut-off points from the 

<!-- TODO move to results, and keep just a short bit here -->

<!----------------------------------------------------------------------------->
&nbsp;

#### Converting between different

Include formulae and informed assumptions 

Not interchangeable in the context of systematic review. If outcome is rare, odds and risk ratios approximate each other, but the hazard ratio is measuring something completely different, by taking into account time-to-event in each treatment group. 

Several existing reviews do not distinguish between the two types of effect measures and include all existing studies in a single meta-analysis to produce an overall effect. 

The likely effect of this is that the overall effect measures are biased towards . . . . ?

Can you predict the direction of effect from a 

Best practice methods for dealing with disparate effect measures in included studies is to not perform a meta-analysis 

<!-- See the third row of the table here; https://training.cochrane.org/handbook/current/chapter-12#section-12-1 -->

Some evidence of manipulation of effect estimates in previous reviews,(e.g. Chou, Sci Reports - at least one study disagrees with) but not documented in review text.

WHile the analysis of change scores is feasible, as they were explicityl

&nbsp;

<!----------------------------------------------------------------------------->

### Risk of bias assessment {#risk-of-bias}

A key use of the review presented in this chapter is to identify different sources of evidence at risk of a diverse range of biases, and to contrast and compare findings across them (see Section \@ref(intro-triangulation) and Section \@ref(triangulation-overview) for an overview of triangulation). To enable this triangulation exercise, a detailed and structured risk of bias assessment formed an important part of this review.

There has been a recent movement within the evidence synthesis community from examining _methodological quality_ to assessing _risk of bias_,[@mcguinness2018; @sterne2016] and thus directly evaluating the internal validity of a study. Internal validity is defined here as the absence of systematic error (or bias) in a study, which may influence its results.[@campbell1957; @juni2001] 

This move was prompted by a unclear definition of "methodological quality" which could include facets such as, and challenges in the comparison of results from different tools.

As part of this, the community also moved from checklist or score based tools towards domain-based methods, in which different potential sources of bias in a study are assessed in order.

Additionally, results should be assessed at the result (defined as a a specific outcome at a specific timepoint) rather than the study level. For example, a study may report on the efficacy of an intervention at 6 months and two years follow-up. missing outcome data that is not an issues at 6 months may introduce bias at 2 year follow-up. In this case, assigning a bias judgement to the study as a whole hides this differential biases for each result.

In this review, domain-based tools were used to assess the risk of bias for each result in each included study. The study design-specific tools are introduced and discussed in more detail in the following sections.

In addition, the tool also aim to capture the potential direction of bias for each given domain. An optional question at the end of each domain, e.g. for Domain 1 of the RoB2 tool: "What is the predicted direction of bias arising from the randomization process?", possible responses included

* Not applicable 
* Favours experimental 
* Favours comparator
* Towards null 
* Away from null 
* Unpredictable

<!----------------------------------------------------------------------------->
&nbsp;

#### Randomised controlled trials

Risk of bias assessment in randomised controlled trials was performed using the domain-based risk-of-bias assessment tool appropriate to the study design. Randomized controlled trials were assessed using the RoB2 tool,[@sterne2019], 
The tool assess the risk of bias across five domains: Bias arising from the randomization process, Bias due to deviations from intended intervention, Bias due to missing outcome data, Bias in measurement of the outcome, Bias in selection of the reported result. Acceptable judgements include: low risk of bias, some concerns, high risk of bias. Each of the 5 domains contains a series of signalling questions or prompts, which guide the user through the tool.

One a domain-level judgement for each domain has been assigned, an overall judgement, using the same three levels of risk of bias, is assigned to 

<!----------------------------------------------------------------------------->
&nbsp;

#### Non-randomised studies of interventions/exposures

For non-randomised studies of interventions (NRSI), I used the ROBINS-I (Risk of bias in non-randomised studies - of interventions) <!-- TODO Capitalise appropriately --> tool.[@sterne2016] This tool assess the risk of bias across seven domains: Bias due to confounding, Bias due to selection of participants, Bias in classification of interventions, Bias due to deviations from intended interventions, Bias due to missing data, Bias in measurement of outcomes, and Bias in selection of the reported result. Similar to the RoB 2, it has a number of prompting questions per domain, with judgements including. Ideally, obs studies assessed in reference to an idealised randomised controlled trial. Further, the rare overall judgement of Low should be considered equivalent to a randomised controlled trial.

For non-randomised studies of exposures (NRSE), I opted to use a preliminary version of the ROBINS-E tool, despite it still being in development.[@morganr2020] While the signalling questions for each domain are yet to be confirmed, the overarching domains have been finalised.

Despite these limitations, I chose this approach in place of other existing published tools such as the Newcastle-Ottowa scale (NOS).[@wells2000] The motivation for this decision was two-fold. In the first instance, as mentioned in the introduction to this section, using a domain-based tool has distinct advantages over better-developed checklist-type tools such as the NOS. In addition, using a domain-based tool enabled better comparison with risk-of-bias assessments performed for the other study designs.

<!----------------------------------------------------------------------------->

&nbsp;

#### Mendelian randomisation studies

<!-- TODO This next bit is not all true -->

At present, no risk-of-bias assessment tool for Mendelian randomization studies is available. Assessment of the risk of bias in Mendelian randomisation studies was informed by the approach used in a previous systematic review of Mendelian randomisation,[@mamluk2020] as identified by a review of risk of bias assessments in systematic reviews of MR studies. A copy of this tool is available in Appendix \@ref(appendix-mr-rob). Advance results from this review of bias assessment in MR studies were obtained from contact with the authors.

&nbsp;

#### Risk of bias due to missing evidence

A recent shift towards the assessment of missing evidence due to selective non-reporting (as distinct from the selective reporting of a single result from multiple planned - handled in Domain 5 of the RoB 2 tool for example).

As part of this review, I piloted the RoB-ME (Risk of Bias due to Missing Evidence in a synthesis) tool. <!-- TODO CITATION NEEDED --> Results were visualised


<!----------------------------------------------------------------------------->

### Analysis methods

Studies were not combined across different study designs (i.e. RCTs were not combined in a meta-analysis with results from observational studies). 

#### Standard meta-analysis

Meta-analysis is an important method of . . . 

Both a fixed-effect and random-effects meta-analysis model was.

For 

Need formula here, and description of different methods and quarrels regarding distinction fixed effect vs random effects meta-analyses.

This analytical approach was used for 

$u$

\begin{equation}
  \theta_i = \mu + u_i
  (\#eq:meta-analysis-random)
\end{equation}

\begin{equation}
  weighted\ average = \frac{\sum Y_i (1/SE_i^2)}{\sum(1/SE_i^2)}
  (\#eq:meta-analysis-fixed)
\end{equation}

where ...

<!-- TODO Need equation for random effects too. See Sean's thesis here. -->

This analytical approach was used for cases where the exposure was dichotomous (e.g. statin use or not, hypercholesterolemia or not).

#### Dose-response analyses

<!-- TODO Explore how to map from simple diagnosis of hyperchol to dose response. Seems that does response needs >3 categories -->
<!-- Will also need description of dose-response meta-analysis here, with reference to forthcoming book Julian sent on.  -->

Several of the included studies presented data on multiple categories of lipid levels, but provided an overall effect estimate based on a comparison of only two of these categories (e.g. for example, highest vs lowest quartile). <!-- TODO Question: if I look at highest vs lowest, and also at top dose vs lowest dose, am I double counting results? -->
 while this allows for easy interpretation of the resulting effect estimate, it ignores any potential non-linear relationships between the exposure and outcome, in addition to discarding useful information contain in the interim groups.

Where possible, data for all exposure groups was extracted, and this <!-- TODO ??? -->
. Studies were excluded from this analysis if the number of categories was less than three, if the exposure cut-off points for for each category were not presented (e.g. if the study reports splitting participants into quartiles and comparing the highest vs lowest without giving the quartile bands in mmol/L).

<!-- Taken from another paper: "A potential non-linear association was evaluated using a restricted cubic spline model with three knots at the 10th, 50th and 90th percentile of frequency of the exposure [@orsini2012meta]" -->


<!-- TODO Do I initially want to fit a linear relationship across categories and  -->
A XXXX spline model was fitted to allow for a non-linear relationship, for example a U or J-shaped relationship, where low and high levels of an exposure can have an effect versus the "normal" reference dose. Reference doses were defined _a priori_ as the cut-off of the "Normal"/"Optimal" categories for each fractions, as detailed in Table \@ref(tab:lipidLevels-table). Under this approach, the reference dose was defined as 200 mg/dL for total cholesterol, 100 mg/dL for LDL-c, 40 mg/dL for HDL-c, and 150 mg/dL for triglycerides.

Due to the requirements for the dose response analysis, studies were excluded from this secondary analysis if they did not provide the require information: cut-off points. Where this was not reported in the study, I contacted the corresponding author to attempt to obtain the required information (see \@ref(contacting-authors)). 


When the highest category was open ended (e.g LDL-c $\geqslant$ 200 mg/dL), I calculated category midpoint by assuming the widht of the higest category was the same as the one immediately below it. Similarly, when, the lowest category was open-ended (e.g LDL-c $\leqslant$ 100 mg/dL), I set the lower boundary for this category to zero (though this is unlikely to occur natually, it was difficult to define).


I assumed the width of the category to be the same as the adjacent category.
When the lowest category was open ended, I set the lower boundary to zero.

#### Sensitivity analyses

I conducted a leave one out analysis in order to explore the impact of any given study on the results. <!-- TODO CITATION NEEDED -->

In addition, 



&nbsp;
<!----------------------------------------------------------------------------->

#### Visualisation of results

Given the importance of visualising the potential biases alongside the results of any given study, a new visualisation tool was designed to allow for "paired" forest plots (as recommended by the ROB2 publication)

This tool grew out of a collaboration with other reserachers to design a modular method for creating these plots <!-- Apparently collaboration is a big thing they want to see -->

A summary of this tool is contained in Appendix \@ref(appendix-robvis), and all forest plots presented in this analysis were created using

Results from the risk-of-bias assessment were visualized using a paired forest/risk-of-bias blobbogram, created using the `robvis` tool. This tool was developed as an adjunct to this thesis to aid in creating standard risk-of-bias figures.[@mcguinness2020robvisPaper] A summary of the tool is provided in Appendix \@ref(appendix-robvis).



Also, the use of evidence maps is becoming increasingly common to explore the distibution of research cohorts included in a 

As such, each individual cohort contibuting to the evidence base was quantified and visualised on a world map.



&nbsp;
<!----------------------------------------------------------------------------->

### Assessment of added value of including preprint

Preprints are a valuable evidence source (see Introduction) but their inclusion in a systematic reivew
s
Additionally, I followed preprints up over time to investigate whether all identified preprints included in the review were subsequently published (in which case preprints give you a look into the future, and a systematic review update would capture these reports) or alternatively, if some preprints were not published, then preprints provide a different. 

I also sought to explore the additional evidental value of including preprints in the meta-analysis, assessed using the fixed effect weight from a standard meta-analysis.

<!-- TODO List here that one of the key reasons for building the tool was to . Reference back to  -->


<!----------------------------------------------------------------------------->

### Patient and public involvement

Patient and public members were intended to be involved at this poit



<!----------------------------------------------------------------------------->

### Deviations from the protocol

A dose response meta-analysis was not planned, marking a departure from the registered protocol. <!-- TODO Check this is accurate -->

Similarly, there was initially no intention to try and examine the risk of bias due to missing evidence.


## Results

<!----------------------------------------------------------------------------->

### Search and screening results

Following the search

`r num_to_text(23447,T)` records were identified through database searches. Following depulication, `r num_to_text(16109)` unique records remained. 

<!--- Cross check this with the actual numbers included --->

No eligible records were identified in the screening of the 500 records excluded by the search. Many of these were basic science studies <!-- TODO text -->


<!----------------------------------------------------------------------------->

```{r prisma-flow-setup, include = FALSE}
```

\blandscape{}

&nbsp;

(ref:prisma-flow-cap) PRISMA Flow diagram illustrating how records moved through the systematic review process. The different contributions of databases and preprint servers to the review are indicated.

(ref:prisma-flow-scap) PRISMA flow diagram

```{r prisma-flow-fig, echo = FALSE, results="asis", fig.pos="H", fig.cap='(ref:prisma-flow-cap)', out.width='100%', fig.scap='(ref:prisma-flow-scap)'}
knitr::include_graphics(file.path("figures/sys-rev/prismaflow.png"))
```

\elandscape{}

<!----------------------------------------------------------------------------->

Following de-duplication, the titles and abstracts of `r num_to_text(16109)` records were assessed for eligibility. `r num_to_text(387)` were deemed potentially eligible and the full text records for these were requested and screened. \@ref(fig:prisma-flow-fig) <!-- TODO Check these figures -->

<!----------------------------------------------------------------------------->

A PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-analyses) flow diagram,[@page2021] <!-- TODO CITATION NEEDED - cite PRIMSA flow paper here, if published --> shown in Figure \@ref(prisma-flow-fig), illustrates the movement of articles through the review. To highlight the contribution of preprint archives to the review, the flow diagram delineates between those records captured through databases searches (presented on the right of the diagram) and those captured by the search tool described in the previous chapter (presented in grey on the left of the diagram).

Following screening, XXX studies were included.

The distribution of included studies over time demonstrates that despite the conduct of several previous reviews of different types of literature surrounding this question, primary studies continue to the published as these reviews have yet to provide a definite answer.

&nbsp;       

As part of our our forward snowballing exercise (where articles citing an included study are cited), I recorded whether a study included in our review had been included in any previous evidence synthesis attempt in an attempt to qualify the added value of this analysis. Additionally, if an included article was subsequently cited by a review, all studies in that review were screened for inclusion for the sake of completeness. This analysis was performed by extracting the citing articles from Google Scholar on [DATE] and screening them manually. The DOI of articles extracted from this analysis are included in the appendix, as the Google Scholar search functionality is not readily reproducible.*

<!-- TODO Actually do this! -->

<!-- For the snowballing exercise - make sure you don't include studies published after review date: ~ June 2019 -->

As a summary of the duplication of work in this area, Ilooked at how many reviews a single included study had previously been included in.

<!--- TODO Need example table here, showing relevant cells --->

&nbsp;

<!----------------------------------------------------------------------------->
(ref:agreeInter-caption) Inter-rater agreement on a subset of ~10% of records, illustrating high accuracy. 

<!-- TODO Include AC1 result in caption above -->

(ref:agreeInter-scaption) Inter-rater agreement

```{r agreeInter-table, message=FALSE, results="asis", echo = FALSE}
```
<!----------------------------------------------------------------------------->

<!----------------------------------------------------------------------------->
(ref:agreeIntra-caption) Intra-rater agreement on subset of records, indicating high consistency.

(ref:agreeIntra-scaption) Inter-rater agreement

```{r agreeIntra-table, message=FALSE, results="asis", echo = FALSE}
```
<!----------------------------------------------------------------------------->

&nbsp;

For the inter-rater reliability, agreement was very high ($AC1$ = `r agreeInter_coeff[1]`, $kappa$ = `r agreeInter_coeff[2]`, Table \@ref(tab:agreeInter-table)). Agreement was similary high for intra-rater reliability ($AC1$ = `r agreeIntra_coeff[1]`, $kappa$ = `r agreeIntra_coeff[2]`, Table \@ref(tab:agreeIntra-table)). The discrepancy between the $AC1$ and $kappa$ coefficients illustrates the sensitivity of $kappa$ to imbalanced marginals, caused in this sample by a heavy imbalance  towards exclusion.[@feinstein1990]

Those records which were excluded in the initial screening, but were included by the second reviewer (n=`r discrepancy_Inter`) were investigated. This discrepancy between the two reviewers was explained in all cases by differing interpretations of the inclusion criteria, specifically around the definition of cognitive decline versus mild cognitive impairment, and the definition of eligible lipids fractions. 

Additionally, records I excluded on my second viewing (n=`r discrepancy_Intra`) were investigated. All were . . . 

&nbsp;

<!----------------------------------------------------------------------------->

### Characteristics of included studies

XXXX studies met the criteria for inclusion in the review.

Table \@ref(tab:studyCharacteristics-table) shows the characteristics 


&nbsp;


\blandscape{}

<!----------------------------------------------------------------------------->
(ref:studyCharacteristics-caption) studyCharacteristics

(ref:studyCharacteristics-scaption) studyCharacteristics

```{r studyCharacteristics-table, message=FALSE, results="asis", echo = FALSE}
```
<!----------------------------------------------------------------------------->

\elandscape{}

As shown in


<!--- TODO Figures and tables to include here: 

-   PRISMA Flowchart (done)

-   Summary of types of study (half done)

-   Summary of locations (half done)

-   Summary of diagnostic criteria used (done)

-   Summary of risk of bias (not done)

--->

<!----------------------------------------------------------------------------->

```{r cohortLocationsSetup, include = FALSE}

```

(ref:cohortLocations-cap) Geographical distribution of study cohorts

(ref:cohortLocations-scap) Geographical distribution of study cohorts

```{r cohortLocations, echo = FALSE, results="asis", fig.pos = "H", fig.cap='(ref:cohortLocations-cap)', out.width='100%', fig.scap='(ref:cohortLocations-scap)'}
knitr::include_graphics(file.path("figures/sys-rev/cohortLocations.png"))
```
<!----------------------------------------------------------------------------->

Many of the included studies were conducted in European countries (Figure \@ref(fig:cohortLocations).





A common theme across the studies was a lack of data on vascular dementia. This is particularly interesting as lipids and statins are primarily a vascular disease. There is the potential that they studies encountered similar difficulties in address the unexpected results observed in the CPRD analysis in Chapter \@ref(cprd-analysis-heading). For vascular dementia, few studies examined this outcome, primarily because of the absence (until recently) of

<!-- IDEA Could include a second evidence synthesis map here to summarise Table 1, highlighting the lack of vascular dementia outcomes and the massive amount of observational data -->  

One item of particular interest in the absence of studies examining Mendelian randomisation studies is the absence of any effect following the adjustment for Apoe4. As covered in the discussion, ApoE4 genotype is the mjor risk factor for Alzheimer's disease.

<!-- TODO Could extract before and after adjusted for ApoE4 to illustrate -->


In addition, there was quite a spread in terms fo the effect estimates used to th. Previous systematic review



<!----------------------------------------------------------------------------->
&nbsp;


### Risk of bias {#risk-of-bias-subheading}

&nbsp;

<!----------------------------------------------------------------------------->

### Meta-analysis of statin use and hypercholesteroleamia

Straight-forward random effects meta-analysis of binary exposures


```{r primaryFigures, include = FALSE}

```


<!----------------------------------------------------------------------------->
(ref:fpStatins-cap) __Forest plot showing effect of statins:__

(ref:fpStatins-scap) Forest plot showing effect of statins

```{r fpStatins, echo = FALSE, results="asis", fig.pos = "H", fig.cap='(ref:fpStatins-cap)', out.width='100%', fig.scap='(ref:fpStatins-scap)'}
knitr::include_graphics(file.path("figures/sys-rev/forester_statins_any.png"))
```
<!----------------------------------------------------------------------------->

&nbsp;

<!----------------------------------------------------------------------------->


### Dose response meta-analysis of lipid levels

<!-- TODO Cite excluded studies and list exclusion reasons  -->

Several studies were excluded from the dose-response meta-analysis, as the number of cases/controls be


<!-- TODO Include image -->


&nbsp;

<!----------------------------------------------------------------------------->

### Sources of heterogeneity

Detail that some of these are exploratory, in particular the effect of different scales on the association between the groups. **[CROSS]**


Important here to examine effect of risk of bias (due to immortal time in observational studies).

Also important to examine the effect of:

* age at baseline
* sex (percentage)
* education
* baseline cognitive scores (how to group)


&nbsp;

<!----------------------------------------------------------------------------->


### Publication bias {#sys-rev-pub-bias}

-   Check if there are protocols available for any of the published reports (unlikely for non-randomised controlled trials), and whether there were
-   Vascular dementia has substantially less published reports. Many (reference Smeeth et al 2010 here) simply group into AD and non-AD making comparison between published studies difficult

One particularly interesting meta-bias potentially applicable in this review is the definition of code lists across

Many studies did not report the codelists used to run the analysis, 

&nbsp;

<!----------------------------------------------------------------------------->

### Triangualtion across evidence sources

<!-- TODO Have a look at this - potentially move to triangulation chapter -->

One key question for which multiple distinct sources of evidence were available were those looking at Laz

A key limitation for other types of dementia, in particularly vascular, is that there has yet to be a GWAS identifying relevant SNPS that could then be used in a MR study with SNPS for lipids to estimates the causal effect of lipids on vascular dementia [**Is this true?**] This rules out the use. In addition, there was limited literature available - as discussed in CHapter \@ref(sys-rev)

Consideration of the potential impact of the magnitude and direct of residual confounders/bias is not a major stretch from what is already happening in the assessment of the quality of evidence (GRADE) framework. Within GRADE, the overall quality of evidence can be upgraded when there is deemed to be unmeasured or residual confounding variables which reduce the. For example, if the propensity to treatment is related to comorbidity burden, but those on treatment still have better outcomes then those on control, it is likely that the true effect of the intervention is being underestimated. [@guyatt2011]

Without our framework and as part of the risk of bias assessments reported in \@ref(risk-of-bias-subheading), I attempted to records the direction of the bias, so that it could feed into the triangulation.


&nbsp;

<!----------------------------------------------------------------------------->

### Added evidental value of including preprints {sys-rev-including-preprints}

<!-- TODO Will have to get actual numbers for this section -->

There were several relevant preprints captured by the search run using the tool presented in Chapter \@ref(sys-rev-tools-heading) (see Appendix \@ref(appendix-medrxivr-code)). While many of the preprints included were subsequently published by the time of submission of this thesis, at the time of the search they were only available 

Good example is [https://doi.org/10.1101/168674](https://doi.org/10.1101/168674), which was subsequently published in Nature Comms but in 2020, following peer-review (found no impact of LDL-c on AD following removal of APOE4).


Of interest, 

<!-- TODO Triple check whether any of the studies have yet to be published. Chose a given date and go from there -->

This likely reflects a bias in the type of study that is submitted, but indicates the additional evidential value of include preprints in the search strategy. It also means that searches are current - if the aim of systematic reviews is to

While the total number of relevant studies found using this method was

<!-- Can I explore the amount of information added by the preprints quantiatively using the weights in the meta-analysis -->

<!-- Add bit on comparison of preprints and published papers, demonstrating the preprints are a good reflection of -->

On further investigation, the argument that . 

It is not so much that they provide a source of unpublished grey-literature, more so that they provide a "future" look at . 

However, if the aim is to find the current state of the art in the topic area at the time of searching, inclusion of preprints is a requirement. 

While concerns have been raised that 

Previous meta-studies have found high concordance between the main interpretations and other study design details in preprint-journal article pairs,[@shi2021] though quality of reporting was slightly better in formally published articles.[@carneiro2020]

A common criticism of preprinted articles is that they have not formally been peer-reviewed. __[CITATION NEEDED]__ <!-- TODO See Chapter 1 for reference for this quote. -->
 However, I believe this criticism is less warranted in the context of inclusion of preprints in a systematic review, given a formal assessment of risk of bias, which provides a structured approach to assessing the internal validity of a study. 

> Anyone who can't assess a study to the level of peer review shouldn't do a systematic review

References included in the review were also searched for on PubPeer, a platform which enables post-publication peer review of published journal articles.[@hunter2012]

The added value of peer-review to papers that is questionable and difficult to quantify for a number of reasons, including the fact that most peer reviews are closed

While `r hold()` preprint have subsequently been published, this does not cause any major issues.


### Risk of bias due to missing evidence


<!----------------------------------------------------------------------------->
&nbsp;

## Discussion

Major limitation is that several included studies used data from electronic health databases, which come with serious concerns regarding validity [@hsieh2019] [@mcguinness2019validity; @wilkinson2018]

In addition, the a

&nbsp;

<!----------------------------------------------------------------------------->

### Summary of findings

<!----------------------------------------------------------------------------->
&nbsp;


### Previous reviews

<!-- TODO Not sure this is the best place for this -->

**Will need to include something in the methods section on this**

As part of this analysis, Iexamined the overlap between different published reviews on this topic.

Several primary studies were captured in multiple systematic reviews. However, some studies were not captured by any previous review,

<!-- IDEA Put upSet plot here - though this will likely now be in Georgia's dissertation -->

An upset plot shows the total size of each set (in this case, the set of included studies in each review) in the bottom left hand bar plot. For example, the XXXX review includes XXXX studies.

Where a line joins two points in the matrix, the main barchart shows the number of records shared by these reviews. So for example, XXXX records were captured both by the XXXX review and the XXXX review.

Where the matrix has just a single point highlighted, this shows the records unique to that review.

The reviews are ordered by date, to attempt to show the increasing overlap and total number of studies

Our review is presented as the last point on this plot, and captured XXXX records

The alternative is to have a barplot showing the total cumulative number of records over the years, using our set as the master, with the bars coloured by the number of records included in 1/2/3/4 reviews. The information below the bar axis could then show the number of reviews published in that year, or alternative, you could have an upsidedown bar to show cumulative number of reviews on this topic.

<!--- Take a stand here! --->

Of note, this review did not include the commonly cited PROSPER study, which examined the effect of pravastatin on CVD risk and putatively provides no evidence for an effect of the statin on dementia outcomes. While widely cited and included in the Cochrane review on this topic, it has not been included here, for the reason that it only reported the change from cognitive score (as measured using the [CHECK] scale) over a mean follow-up. While this is a useful indicator of general cognitive decline, it is not equivalent to a dementia outcome, as cogntive scales should feed into a broader diagnostic pathway - See section \@ref(diagnostic-criteria)

This does introduce some conflicts with other studies where a change score has been dichotomised in order to have a binary dementia or not dementia outcome.

Does it also cause issues in terms of self-report?


<!-- TODO This section will be informed by the review published as a preprint -->

Of note, as part of the review, Iidentified several previous systematic reviews of this topic.[CITE] However, this review is the first to use established domain based assessments tools (for example, the RoB 2 tool for randomized controlled trials)[@sterne2019] to assess the risk of bias in included studies, and explore the heterogeneity of results across different levels of risk of bias levels.

Some previous reviews did assess risk of bias, but used non-domain based assessment tools. Newcastle-Ottowa scale,

**The duplication of work across reviews (including, ironically, by this review) is substantial. In Section XXXX, I demonstrated that the each primary study included in this review was also included in other review so this topic, but that not all studies were included in all reviews. However, by creating a comprehensive review that attempted to draw together all available evidence from across the full range of study types.**

__Exclusion of the PROSPER trial__ Will need to have a good bit here on the exclusion of this RCT, particularly as it is included in the Cochrane review on the topic. Cross-reference with the meta-bias section in the early section, on the impact of only included studies that sought to make a definitive diagnosis, rather than using change in cognitive assessment scales.

<!-- TODO Will hopefully be able to cite MSc review here to give details of other reviews -->

In retrospective, an alternative approach to the one taken could have been employed, performing an review of reviews, also known as an umbrella review. In these studies, the unit of analysis are systematic reviews rather than the 

However, a comparative analysis between indicates 


&nbsp;

<!----------------------------------------------------------------------------->

### Comments on the process

As part of the reflective element of this thesis, I collated my experiences on performing a systematic review.

Systematic reviews should not be performed as part of a thesis, without suitable support and resourcing guaranteed. Assumption that everyone does a systematic review (without risk of bias assessments, inclusion of all literature, searching for other reviews) is foolish.

Average time to complete a s

However, new developments such as automated screening would allow for a reduced need for personnel to work on these things. <!-- TODO CITATION NEEDED - Cite Triccios new paper here -->


Great learning experience in terms of managing a team, but due to a rotating schedule of people, much harder to do that expected.

A final point on the progress of the review is that, due to the need for dual screening and data extraction, a number of external researchers became involved in this review. I found the people-management aspect particularly challenging and could definitely have improved the process through better communication of deadlines, but it has provided good experience of leading a review team.

&nbsp;

<!----------------------------------------------------------------------------->

### Inclusion of preprints

As highlighted in Section \@ref(diverse-sources-preprints), this thesis explicitly sought to synthesize evidence across different publication statuses, predominantly preprinted vs published. Using the tool described in Chapter \@ref(sys-rev-tools-intro), two preprint serves related to health and biomedical sciences were search as part of this review. 

The added evidential value of including these preprints was highlighted in Section \ref()

On further investigation, X of these preprints were subsequently published since the search was performed in 2019. While this provides some additional evidence that the quality of the research described by included preprints was sufficiently high to pass peer-review, it does not invalidate the approach, as the future publication status was unavailable at the time the search was conducted.


&nbsp;

<!----------------------------------------------------------------------------->

### Open data sharing {#sys-rev-open-data}

As evidenced by the issues involved in obtaining 


Building in part on the research, I co-wrote an guidance piece for pre

Similarly, a substanital amount of time and effort has gone into making the data obtained by this review openly available to 

<!--- How many of the included studies accurately reported the diagnostic criteria for EHR? Cross link with section in CPRD analysis --->

<!--- Comment here on issues involved in extracting data --->


&nbsp;

<!----------------------------------------------------------------------------->

### Strenghts and limitations

I believe there are four aspects where this review is distinct from those reviews already available in the published literature (as identified by <!-- TODO CITATION NEEDED -->):

-   *Comprehensiveness:* While several reviews of this research topic exist,[@chu2018; @yang2020; @muangpaisan2010; @poly2020] the overlap between the list of studies included in each is not 100%. As part of this review, I have not only performed a original search of primary literature databases, but have also screened the reference lists of comparable reviews to ensure no study has been omitted.

-   *Structured risk of bias assessment:* The majority of the highly cited reviews on this topic either do not formally consider the risk of bias in the observational studies they include or do not use an appropriate domain-based assessment tool (e.g. ROBINS-I/E). This is important area in which this thesis can add value, as based on the risk-of-bias assessments I have performed to date, several primary studies are at high risk of bias and this should be reflected in the findings of any review on this topic.

-   *Inclusion of preprints:* Unlike other available reviews and enabled by the tool described in Chapter 3, this review systematically searched preprinted health-related manuscripts as a source of grey literature. As part of this chapter, I plan to examine the extent of the additional information provided to the review by the inclusion of preprints.

- *Contribution to methods work:* A large part of this review was the associated work on improving research synthesis methods. This work is detailed as relevant throughout the Chapter, often referring to additional work detailed in the . IN addition this review was used to pilot an upcoming risk of bias tool

&nbsp;

#### Limitations




&nbsp;

<!----------------------------------------------------------------------------->

### Reviewing Mendelian randomisations studies

<!-- Three things to cover: No good tools for dealing with MR studies yet (particularly search filters, as terminology is hard + no risk of bias tool); Is double counting of studies an issue (maybe speak to George, particularly if you can find an example)  -->

<!-- TODO Email George with query on dual counting, following review of email with Julian -->

While

This may be because Mendelian randomisation studies have yet to reach a critical mass in terms of requiring a systematic review. 

Recent updates, such as a guide to reading and interpreting Mendelian .

However, this guide includes reporting items in their quality checklist - reporting quality while important, is unrelated to internal validity.

Problems with overlapping samples in reviews of Mendelian randomisation studies in that may be double counting participants

Double-counting of other -

Methods for reviews of Mendelian randomisation studies are not well developed to account for the consideration above.

In short, given the recent developments in the production of studies and results from Mendelian randomisation 

This is particularly relevant to missing evidence, as new methods in MR studies allow for the anlysis of many exposures/outcomes concurrently. In this scenario, missing evidence is likely to be an issue as onyl those of particular interest to the authors/those reaching statistical significance will be reported in the paper 

This is highlighted by the identification of studies relevant to the review though coincidental reading and citation searhcing, that were not identified by the search as they reported many results and only the statistically significant ones were included in searchable fields

This is particularly troubling given the comprehensiveness of the search (~17,000 records screened). In retrospect, a mitigating approach may have been to have an additional search for topics related to risk factors, dementia and MR studies, and screened t

<!-- TODO Best option probably to highlight that while these studies were included, it was not easy given a lack of resources for MR studies -->

As a form of analysis that lends itself to multiple comparison, there is a strong

Through snowballing and other measures, I identified at least one Mendelian randomisation study that had not been identified by the search strategy. [@larsson2017b] On review of this paper, the search would not have been expected to find it, given the absence of any lipid-related keywords in the title and abstract.

The study examined the association between lipid fractions with Azlheimer's disease, as one of many risk factors for AzD. Studies such as this can introduce bias, as it is commonly only those risk factors that show a statistically significant result that are reported in the abstract and so are captured by systematic reviews of the topic. This may bias reviews as the analysising multiple risk factors at a time becomes more common. 

These studies are described as "unknown unknown's" in the context of the RoB-ME tool, and are particularly challenging (as opposed to an analysis that was insufficiently reported to be included in the statistical analysis, or the "known unknown's"),

<!-- TODO Ask Matt Page whether he has done any work on this? Studies looking at many risk factors don't report null results in abstract and so do no -->


Is it a big issue that the studies have the same underlying datasets? <!-- TODO Check discussion on this point from Matt Lee thesis, in email -->

Potentially the use of methods such as snowballing (forwards and backwards citation chasing) and communication with relevant topic experts should cpature similar studies. 
However, in future better-resourced reviews, a dedicated search for "risk factors" and "dementia" and "Mendelian randomisation", followed by manual review of studies that look across multiple risk factors, would be advisable.



&nbsp;

<!----------------------------------------------------------------------------->

### Strengths and limitations


&nbsp;

<!----------------------------------------------------------------------------->




#### Inclusion of preprints

Comment on the fact that several of the studies identified through the search of preprints, were subsequently published, but much later than the cut-off point for the search (e.g. they would not have been captured by the search for published papers)


&nbsp;

<!----------------------------------------------------------------------------->

### Use in other chapters

<!--- IDEA Findings from this chapter are used throughout the later chapters of this thesis. In particular, Chapter 6 uses statistical modelling to exploring the impact of BCG vaccination on TB outcomes in greater detail, Chapter 7 explores the impact of the change in BCG vaccination policy on TB incidence rates using the incidence rate estimates from this chapter, Chapter 8 uses the understanding of the ETS gained from this chapter to parameterise a dynamic TB transmission model, and Chapter 9 uses the insights gained into the date variables in the ETS to fit a dynamic TB transmission model. From Sam abbotts--->

Findings from this review are used though out the subsequent chapters. 

For example, 


## Conclusions

In this chapter I have 

This work built on the tool introduced in the preceeding chapter, and the results from the systematic review are used in further chapters. 

In Chapter \@ref(cprd-analysis-heading), 
A summary of the evidence guided the 

In Chapter \@ref(ipd-heading), 

<!-- IDEA Link here to IPD, talking about how the meta-regression on key variables suggested the use of -->


A

\newpage 

## References

